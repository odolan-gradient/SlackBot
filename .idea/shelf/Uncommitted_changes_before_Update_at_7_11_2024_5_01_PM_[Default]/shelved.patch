Index: Logger.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import datetime\r\nimport json\r\nimport time\r\nimport xml.etree.ElementTree as ET\r\nfrom collections import defaultdict\r\nfrom collections import deque\r\nfrom datetime import date, timedelta, datetime\r\nfrom os import path\r\n\r\nimport requests\r\nfrom dateutil import tz\r\nfrom google.cloud import bigquery\r\nfrom requests.adapters import HTTPAdapter, Retry\r\n\r\nfrom CropCoefficient import CropCoefficient\r\nfrom CwsiProcessor import CwsiProcessor\r\nfrom DBWriter import DBWriter\r\nfrom Field import Field\r\nfrom Grower import Grower\r\nfrom Notifications import Notification_SensorError, Notification_TechnicianWarning\r\nfrom Soils import Soil\r\nfrom Technician import Technician\r\nfrom Thresholds import Thresholds\r\n\r\nDIRECTORY_YEAR = \"2024\"\r\nDXD_DIRECTORY = \"H:\\\\Shared drives\\\\Stomato\\\\\" + DIRECTORY_YEAR + \"\\\\Dxd Files\\\\\"\r\n\r\n\r\n####################################################################\r\n# Base Class for a logger in a field                               #\r\n####################################################################\r\ndef read_last_time(dxd_file: str):\r\n    \"\"\"\r\n    Read and return the last time information was pulled from a dxd file.\r\n\r\n    The Last Time information was pulled indicates when the last time the API was used to pull information\r\n\r\n    :param dxd_file:\r\n        File we want to get the last time from\r\n    :return:\r\n        if found return:\r\n            last_time - datetime obj\r\n            last_timestamp - timestamp\r\n        else 0\r\n    \"\"\"\r\n    txt = \"no data found\"\r\n    with open(dxd_file) as file:\r\n        data = json.load(file)\r\n        if \"created\" in data:\r\n            last_time_ts = data['device']['timeseries'][-1]['configuration']['values'][-1][0]\r\n            return last_time_ts\r\n\r\n    return 0\r\n\r\n\r\ndef change_mrid(logger_ids: list, mr_id_to_set: int = 0):\r\n    \"\"\"\r\n\r\n    :param logger_ids:\r\n    :param mr_id_to_set:\r\n    \"\"\"\r\n    try:\r\n\r\n        for logger_id in logger_ids:\r\n            file_path = path.join(DXD_DIRECTORY, logger_id + '.dxd')\r\n\r\n            print(f'Changing dxd file MRID {file_path}...')\r\n\r\n            if not path.isfile(file_path):\r\n                raise FileNotFoundError(f\"The file '{file_path}' does not exist.\")\r\n\r\n            with open(file_path, 'r', encoding=\"utf8\") as fd:\r\n                parsed_json = json.load(fd)\r\n\r\n            if mr_id_to_set > -1:\r\n                print(f'Modifying file MRID with the set back MRID: {mr_id_to_set}')\r\n                if \"created\" in parsed_json:\r\n                    # Modify the MRID to the new value\r\n                    parsed_json['device']['timeseries'][-1]['configuration']['values'][-1][1] = mr_id_to_set\r\n\r\n            # Write the modified JSON back to the file\r\n            with open(file_path, 'w', encoding=\"utf8\") as fd:\r\n                json.dump(parsed_json, fd, indent=4, sort_keys=True, default=str)\r\n\r\n            print('Successfully changed MRID')\r\n\r\n    except Exception as error:\r\n        print(f'ERROR in changing dxd MRID file for json data for {logger_id}')\r\n        print(error)\r\n\r\n\r\nclass Logger(object):\r\n    \"\"\"\r\n    Class to hold information for 1 logger installed in the field.\r\n\r\n    This class will encompass everything a logger needs to do to update its information. That includes\r\n        downloading the new dxds, converting them, processing the information, and then writing it out\r\n        to its Google Sheet.\r\n\r\n    Attributes:\r\n        id: String to hold the logger's ID\r\n        password: String to hold the logger's password\r\n        prev_day_gallons: Number to hold the previous day gallons. This value is needed and used to calculate what\r\n            the next day's gallons are\r\n        prev_day_switch: Number to hold the previous day switch minutes. This value is needed and used to calculate\r\n            what the next day's switch minutes are\r\n        daily_switch: List to hold values for daily switch\r\n        ports: Dictionary to hold the different port values\r\n        cwsi_processor: CwsiProcessor Object used to process the information from the logger\r\n    \"\"\"\r\n\r\n    def __init__(\r\n            self,\r\n            id: str,\r\n            password: str,\r\n            name: str,\r\n            crop_type: str,\r\n            soil_type: str,\r\n            gpm: float,\r\n            irrigation_set_acres: float,\r\n            logger_direction: str,\r\n            install_date: datetime.date,\r\n            lat: str = '',\r\n            long: str = '',\r\n            grower: Grower = None,\r\n            field: Field = None,\r\n            planting_date: datetime.date = None,\r\n            rnd: bool = False,\r\n            active: bool = True,\r\n            nickname: str = ''\r\n    ) -> object:\r\n        \"\"\"\r\n\r\n        :param install_date:\r\n        :param id:\r\n        :param password:\r\n        :param name:\r\n        :param crop_type:\r\n        :param field_capacity:\r\n        :param wilting_point:\r\n        :param gpm:\r\n        :param irrigation_set_acres:\r\n        :param grower:\r\n        :param field:\r\n        :param planting_date:\r\n        :param rnd:\r\n        \"\"\"\r\n\r\n        self.id = id\r\n        self.password = password\r\n        self.name = name\r\n        self.nickname = nickname\r\n        self.grower = grower\r\n        self.field = field\r\n        self.cwsi_processor = CwsiProcessor()\r\n        self.dbwriter = DBWriter()\r\n        self.prev_day_switch = 0\r\n        self.prev_day_gallons = 0\r\n        self.updated = False\r\n        self.crashed = False\r\n        self.crop_coefficient = CropCoefficient()\r\n        if self.id[0] == 'z':\r\n            self.model = 'z6'\r\n        else:\r\n            self.model = 'unknown'\r\n        if isinstance(planting_date, str):\r\n            ptdate = datetime.strptime(planting_date, '%m/%d/%Y').date()\r\n            self.planting_date = ptdate\r\n        else:\r\n            self.planting_date = planting_date\r\n        self.crop_type = crop_type\r\n        self.soil = Soil(soil_type)\r\n        if gpm is not None:\r\n            self.gpm = float(gpm)\r\n        if irrigation_set_acres is not None:\r\n            self.irrigation_set_acres = float(irrigation_set_acres)\r\n        self.prev_mrid = 0\r\n        self.rnd = rnd\r\n        self.active = active\r\n        self.lat = lat\r\n        self.long = long\r\n        self.logger_direction = logger_direction\r\n        self.gdd_total = 0\r\n        self.crop_stage = 'NA'\r\n        self.ir_active = False\r\n        self.ports = {}\r\n        self.install_date = install_date\r\n        self.broken = False\r\n        self.uninstall_date = None\r\n        self.consecutive_ir_values = deque()\r\n        self.irrigation_ledger = {}\r\n\r\n    def __repr__(self):\r\n        return f'Logger Name: {self.name}, Nickname: {self.nickname}, Active: {self.active}, Crop Type: {self.crop_type}, Soil: {self.soil.soil_type}, IR Active: {self.ir_active}'\r\n\r\n    def to_string(self):\r\n        \"\"\"\r\n        Function used to print out output to screen.\r\n\r\n        Print out the Logger sheet name, id, password, sheet url,\r\n            sheet id, prev day gallons, prev day switch\r\n        :return:\r\n        \"\"\"\r\n        name_str = f'Name: {str(self.name)}'\r\n        grower_str = f'Grower: {str(self.grower.name)}'\r\n        id_str = f'ID: {str(self.id)}'\r\n        active_str = f'Active: {str(self.active)}'\r\n        if not hasattr(self, 'crop_type'):\r\n            if hasattr(self, 'cropType'):\r\n                self.crop_type = self.cropType\r\n        crop_str = f'Crop: {str(self.crop_type)}'\r\n        prev_day_switch_str = f'Prev Day Switch: {str(self.prev_day_switch)}'\r\n        if not hasattr(self, 'gdd_total'):\r\n            self.gdd_total = 'No GDD'\r\n        gdd_str = f'GDD Total: {str(self.gdd_total)}'\r\n        gpm_str = f'GPM: {str(self.gpm)}'\r\n        soil_type_str = f'Soil Type: {str(self.soil.soil_type)}'\r\n        updated_str = f'Updated: {str(self.updated)}'\r\n        if not hasattr(self, 'crop_stage'):\r\n            self.crop_stage = 'No Crop Stage'\r\n        if not hasattr(self, 'irrigation_set_acres'):\r\n            if hasattr(self, 'acres'):\r\n                self.irrigation_set_acres = self.acres\r\n        if not hasattr(self, 'crashed'):\r\n            self.crashed = False\r\n        if not hasattr(self, 'rnd'):\r\n            self.rnd = False\r\n        if not hasattr(self, 'ir_active'):\r\n            self.ir_active = None\r\n        install_date_str = f'Install Date: {str(self.install_date)}'\r\n        formatted_consecutive_psi = [round(tup[0], 2) for tup in self.consecutive_ir_values]\r\n        formatted_consecutive_sdd = [round(tup[1], 2) for tup in self.consecutive_ir_values]\r\n        formatted_consecutive_dates = [f\"{d.month}-{d.day}-{d.year}\" for _, _, d in self.consecutive_ir_values]\r\n        # formatted_consecutive_psi = [t[0] for t in self.consecutive_ir_values]\r\n        consect_psi_str = f'Consec PSI: {formatted_consecutive_psi}'\r\n        consect_dates_str = f'Consec Dates: {formatted_consecutive_dates}'\r\n\r\n        print('....................................................................')\r\n        print(f'\\t{name_str:30} | Nickname: {str(self.nickname)}')\r\n        print(f'\\t{grower_str:30} | Field: {str(self.field.name)}')\r\n        print(f'\\t{id_str:30} | Password: {str(self.password)}')\r\n        print(f'\\t{active_str:30} | Broken: {str(self.broken)}')\r\n        print(f'\\t{install_date_str:30} | Planting Date: {str(self.planting_date)}')\r\n        print(f'\\t{crop_str:30} | R&D: {str(self.rnd)}')\r\n        print(f'\\t{prev_day_switch_str:30} | IR Active: {self.ir_active}')\r\n        print(f'\\t{consect_psi_str:30} | Consect SDD: {formatted_consecutive_sdd}')\r\n        print(f'\\t{consect_dates_str:30}')\r\n        print(f'\\t{gdd_str:30} | Crop Stage: {self.crop_stage}')\r\n        print(f'\\t{gpm_str:30} | Acres: {str(self.irrigation_set_acres)}')\r\n        print(f'\\t{soil_type_str:30} | FC: {str(self.soil.field_capacity)}   WP: {str(self.soil.wilting_point)}')\r\n        print(f'\\t{updated_str:30} | Crashed: {str(self.crashed)}')\r\n        print('....................................................................')\r\n\r\n    def get_logger_data(\r\n            self,\r\n            specific_mrid: int = None,\r\n            subtract_from_mrid: int = 0,\r\n            mr_id_location: str = DXD_DIRECTORY,\r\n            dxd_save_location: str = DXD_DIRECTORY,\r\n            file_name: str = None,\r\n    ) -> bool:\r\n        \"\"\"\r\n        Download dxd files from decagon API containing all the logger information and store them.\r\n        Call on the corresponding API for z6 loggers. Using zentra API v1.0\r\n\r\n        Access information for the API is hard coded - email, user_password, url\r\n        Files are stored in the OutputFolder with the Logger ID as the name. For example:\r\n            5G0B0420.dxd\r\n        :return:\r\n        \"\"\"\r\n        response_success = False\r\n        # file_path = ''\r\n        mr_id_file_path = ''\r\n        dxd_save_location_file_path = ''\r\n        if file_name is None:\r\n            file_name = self.id + '.dxd'\r\n        else:\r\n            file_name = file_name + '.dxd'\r\n        if path.exists(mr_id_location):\r\n            mr_id_file_path = mr_id_location + file_name\r\n        if path.exists(dxd_save_location):\r\n            dxd_save_location_file_path = dxd_save_location + file_name\r\n\r\n        # if path.exists(DXD_DIRECTORY):\r\n        #     file_path = DXD_DIRECTORY + file_name\r\n\r\n        mr_id = 0\r\n        if self.crashed:\r\n            print(\"\\tCrashed so running with previous MRID: {0}\".format(self.prev_mrid))\r\n            mr_id = self.prev_mrid\r\n            # Resetting the crashed boolean\r\n            self.crashed = False\r\n            mr_id_to_set_back = 0\r\n        else:\r\n            file_mrid = self.get_mrid(mr_id_file_path)\r\n            print(f'\\tDXD mrid = {file_mrid}')\r\n            mr_id_to_set_back = file_mrid\r\n            if specific_mrid is not None:\r\n                # We passed in a specific_mrid\r\n                mr_id = specific_mrid\r\n                print(\r\n                    f\"\\t-->> Running from specific MRID: {str(specific_mrid)}\"\r\n                )\r\n            elif subtract_from_mrid > 0:\r\n                # We passed in a subtract_from_mrid\r\n                mr_id = file_mrid - subtract_from_mrid\r\n                print(\r\n                    f\"\\t-->> Subtracting from MRID: {str(file_mrid)} - {str(subtract_from_mrid)} = {str(mr_id)}\"\r\n                )\r\n            else:\r\n                # No special pass in's, just default run\r\n                mr_id = file_mrid\r\n                mr_id_to_set_back = 0\r\n                self.prev_mrid = mr_id\r\n\r\n        if mr_id < 0:\r\n            print(f'\\tMRID is negative {mr_id}, so setting to 0')\r\n            mr_id = 0\r\n        response = self.zentra_api_call(mr_id)\r\n\r\n        if response.ok:\r\n            response_success = True\r\n            self.write_dxd_file(dxd_save_location_file_path, response, mr_id_to_set_back)\r\n        else:\r\n            print('\\tResponse not OK')\r\n            response_success = False\r\n        return response_success\r\n\r\n    def write_dxd_file(self, file_path, response, mr_id_to_set_back: int = 0):\r\n        try:\r\n            write_dxd_start_time = time.time()\r\n\r\n            print('\\tWriting dxd file...')\r\n\r\n            parsed_json = json.loads(response.content)\r\n            if mr_id_to_set_back > 0:\r\n                print(f'\\tMRID was adjusted by special pass in (subtract or specific mrid)...')\r\n                print(f'\\tModifying file MRID with the set back MRID: {mr_id_to_set_back}')\r\n                if \"created\" in parsed_json:\r\n                    # Modify the MRID to the new value\r\n                    parsed_json['device']['timeseries'][-1]['configuration']['values'][-1][1] = mr_id_to_set_back\r\n            with open(file_path, 'w', encoding=\"utf8\") as fd:\r\n                json.dump(parsed_json, fd, indent=4, sort_keys=True, default=str)\r\n\r\n            write_dxd_end_time = time.time()\r\n            print(\"----------FINISHED----------\")\r\n            write_dxd_elapsed_time_seconds = write_dxd_end_time - write_dxd_start_time\r\n\r\n            write_dxd_elapsed_time_hours = int(write_dxd_elapsed_time_seconds // 3600)\r\n            write_dxd_elapsed_time_minutes = int((write_dxd_elapsed_time_seconds % 3600) // 60)\r\n            write_dxd_elapsed_time_seconds = int(write_dxd_elapsed_time_seconds % 60)\r\n            print(f\"\\tWrite DXD execution time: {write_dxd_elapsed_time_hours}:\"\r\n                  + f\"{write_dxd_elapsed_time_minutes}:\"\r\n                  + f\"{write_dxd_elapsed_time_seconds} (hours:minutes:seconds)\")\r\n            print()\r\n        except Exception as error:\r\n            print('\\tERROR in writing dxd file for json data for z6')\r\n            self.updated = False\r\n            print(error)\r\n\r\n    def zentra_api_call(self, mr_id):\r\n        print('\\tCalling Zentra API...')\r\n        url = 'https://zentracloud.com/api/v1/readings'\r\n        params = {'user': 'jgarrido@morningstarco.com', 'user_password': 'Mexico1012', 'sn': self.id,\r\n                  'device_password': self.password, 'start_mrid': mr_id}\r\n        # OLD METHOD BUILDING WHOLE STRING BY HAND\r\n        # email_lead = '?user='\r\n        # email = 'jgarrido@morningstarco.com'\r\n        # user_password_lead = '&user_password='\r\n        # user_password = 'Mexico1012'\r\n        # id_lead = '&sn='\r\n        # id = self.id\r\n        # password_lead = '&device_password='\r\n        # password = self.password\r\n        # mrid_lead = '&start_mrid='\r\n        # built_url = url + email_lead + email + \\\r\n        #             user_password_lead + user_password + \\\r\n        #             id_lead + id + \\\r\n        #             password_lead + password\r\n        # if mr_id > 0:\r\n        #     built_url = built_url + mrid_lead + str(mr_id)\r\n        try:\r\n            retry_strategy = Retry(\r\n                total=7,\r\n                backoff_factor=2,\r\n                status_forcelist=[429, 443, 500, 502, 503, 504]\r\n            )\r\n            adapter = HTTPAdapter(max_retries=retry_strategy)\r\n            http = requests.Session()\r\n            http.mount(\"https://\", adapter)\r\n            http.mount(\"http://\", adapter)\r\n\r\n            print(f\"\\tMRID for API Call: {mr_id}\")\r\n\r\n            response = http.get(url, params=params)\r\n            print(\"\\t-->> Request URL: \" + str(response.url))\r\n\r\n            # OLD METHOD WITHOUT RETRY OR BACKOFF\r\n            # response = requests.get(url, timeout=120)\r\n\r\n            if response.ok:\r\n                print(f'\\tSuccessful METER API')\r\n\r\n        except Exception as error:\r\n            print('\\tERROR in making z6 data request to METER')\r\n            self.updated = False\r\n            print(error)\r\n        except requests.exceptions.Timeout:\r\n            print('\\tTimeout in making z6 data request to METER')\r\n            self.updated = False\r\n        except requests.exceptions.TooManyRedirects:\r\n            print('\\tToo many redirects in making z6 data request to METER')\r\n            self.updated = False\r\n        except requests.exceptions.RequestException as e:\r\n            print('\\tERROR in making z6 data request to METER')\r\n            self.updated = False\r\n            print(e)\r\n        return response\r\n\r\n    def get_mrid(self, file_path):\r\n        if self.is_file(file_path):\r\n            mr_id = self.read_mrid(file_path)\r\n        else:\r\n            mr_id = 0\r\n        return mr_id\r\n\r\n    def is_file(self, file: str) -> bool:\r\n        \"\"\"\r\n        Check if a file is a file or not.\r\n\r\n        :param file:\r\n        :return:\r\n            True if it is a file\r\n            None if it is not\r\n        \"\"\"\r\n        try:\r\n            with open(file):\r\n                pass\r\n            return True\r\n        except IOError as e:\r\n            print(\"Unable to open file %s\" % file)\r\n            return None\r\n\r\n    def read_mrid(self, file_path: str):\r\n        \"\"\"\r\n        Read and return the MRID from a dxd file.\r\n\r\n        MRID is a number that indicates how current the information in the dxd is. Everytime a download occurs,\r\n        the MRID is set to a newer number. Each new download just requires the MRID + 1 to get the latest info\r\n        since the last download.\r\n\r\n        :param file_path:\r\n            File we want to get the MRID from\r\n        :return:\r\n            MRID if found, 0 if not\r\n        \"\"\"\r\n        with open(file_path) as file:\r\n            data = json.load(file)\r\n            if \"created\" in data:\r\n                mrid = data['device']['timeseries'][-1]['configuration']['values'][-1][1]\r\n                self.prev_mrid = mrid\r\n                return mrid\r\n        return 0\r\n\r\n    def read_ereset(self, dxd_file):\r\n        \"\"\"\r\n        Read and return the eReset from a dxd file.\r\n\r\n        eReset is a number that indicates how many times the logger has been reset. This is useful information\r\n        because each time a logger is reset, the gallons and switch counters get reset to 0 as well which can affect\r\n        the total tallies.\r\n\r\n        :param dxd_file:\r\n            File we want to get the eReset from\r\n        :return:\r\n            eReset if found, 0 if not\r\n        \"\"\"\r\n        txt = \"no data found\"\r\n        doc = ET.parse(dxd_file)\r\n        root = doc.getroot()\r\n        for element in root.iter():\r\n            if 'Status' in element.tag:\r\n                ereset = int(element.get('eReset'))\r\n                return ereset\r\n        return 0\r\n\r\n    def read_battery_level(self, dxd_file: str):\r\n        \"\"\"\r\n        Read and return the Battery level from a dxd file.\r\n\r\n        Battery is a number that indicates how much battery is left in the logger batteries.\r\n\r\n        :param dxd_file:\r\n            File we want to get the Battery level from\r\n        :return:\r\n            Battery level if found, None if not\r\n        \"\"\"\r\n\r\n        with open(dxd_file) as file:\r\n            data = json.load(file)\r\n            if \"created\" in data:\r\n                battery_level = data['device']['timeseries'][-1]['configuration']['values'][-1][-2][0]['value']\r\n                return battery_level\r\n        return None\r\n\r\n    def read_dxd(self, dxd_save_location: str = DXD_DIRECTORY, file_name: str = None, ):\r\n        \"\"\"\r\n        Read the dxd file and check the response.\r\n\r\n        :return:\r\n            Dictionary with raw dates and raw values\r\n        \"\"\"\r\n        result = None\r\n        if file_name is None:\r\n            file_name = self.id + '.dxd'\r\n        else:\r\n            file_name = file_name + '.dxd'\r\n\r\n        if path.exists(dxd_save_location):\r\n            file_path = dxd_save_location + file_name\r\n\r\n        raw_dxd = \"no data found\"\r\n        with open(file_path) as file:\r\n            raw_dxd = json.load(file)\r\n        if raw_dxd == \"no data found\":\r\n            return None\r\n        return raw_dxd\r\n\r\n    def get_all_ports_information(self, raw_dxd: dict, specific_year: int = datetime.now().year) -> dict:\r\n        \"\"\"\r\n        Read the dxd file and returns the information in all ports.\r\n\r\n        :param specific_year:\r\n        :param raw_dxd:\r\n            Dxd file to read\r\n        :return:\r\n            Dictionary holding the raw values in each port\r\n        \"\"\"\r\n        raw_dxd_dict = None\r\n        converted_results = {\"dates\": [], \"canopy temperature\": [], \"ambient temperature\": [], \"rh\": [], \"vpd\": [],\r\n                             \"vwc_1\": [], \"vwc_2\": [], \"vwc_3\": [], \"vwc_1_ec\": [], \"vwc_2_ec\": [], \"vwc_3_ec\": [],\r\n                             \"daily gallons\": [], \"daily switch\": []}\r\n        data_from_previous_years = False\r\n        min_number_of_values_for_timeseries = 5\r\n\r\n        if \"device\" in raw_dxd:\r\n            all_data_series = []\r\n            for ind, timeseries in enumerate(raw_dxd['device']['timeseries']):\r\n                # Check that the timeseries has at least 5 values\r\n                if len(timeseries['configuration']['values']) >= min_number_of_values_for_timeseries:\r\n                    # Grab start and end of timeseries\r\n                    timeseries_start_date_timestamp = timeseries['configuration']['values'][0][0]\r\n                    timeseries_end_date_timestamp = timeseries['configuration']['values'][-1][0]\r\n\r\n                    # Special check because some timeseries seem to get 1 random value at the very end from an\r\n                    # early year. When this happens, we just grab the date for the 2nd to last value instead of\r\n                    # the last\r\n                    if timeseries_end_date_timestamp < timeseries_start_date_timestamp:\r\n                        # start = self.convert_timestamp_to_local_datetime(timeseries_start_date_timestamp)\r\n                        # end = self.convert_timestamp_to_local_datetime(timeseries_end_date_timestamp)\r\n                        #\r\n                        # print('Error: END DATE < START DATE')\r\n                        # print(self.field.name)\r\n                        # print(self.name)\r\n                        # print(f'{end} < {start}')\r\n                        # print()\r\n                        timeseries_end_date_timestamp = timeseries['configuration']['values'][-2][0]\r\n\r\n                    # Convert those start and end timestamps to datetimes\r\n                    timeseries_start_date_datetime = self.convert_timestamp_to_local_datetime(\r\n                        timeseries_start_date_timestamp)\r\n                    timeseries_start_date_datetime_year = timeseries_start_date_datetime.year\r\n\r\n                    timeseries_end_date_datetime = self.convert_timestamp_to_local_datetime(\r\n                        timeseries_end_date_timestamp)\r\n                    timeseries_end_date_datetime_year = timeseries_end_date_datetime.year\r\n\r\n                    if timeseries_start_date_datetime_year <= specific_year <= timeseries_end_date_datetime_year:\r\n                        # Grab the port configuration of this particular timeseries\r\n                        ports = self.get_ports(timeseries['configuration']['sensors'])\r\n\r\n                        # Check if ports.keys() has every one of the required ports\r\n                        required_ports = [1, 2, 3, 4, 5, 6]\r\n                        has_all_ports = True\r\n\r\n                        for port_num in required_ports:\r\n                            if port_num not in ports.keys():\r\n                                has_all_ports = False\r\n                                break\r\n\r\n                        # If it has all required ports and each port is set to the correct sensor:\r\n                        if (has_all_ports and\r\n                            ports[1] in ['Infra Red'] and\r\n                            ports[2] in ['VP4', 'Atmos 14'] and\r\n                            ports[3] in ['GS1', 'Terros 10', 'GS3', 'Terros 12'] and\r\n                            ports[4] in ['GS1', 'Terros 10', 'GS3', 'Terros 12'] and\r\n                            ports[5] in ['GS1', 'Terros 10', 'GS3', 'Terros 12'] and\r\n                            ports[6] in ['Switch']):\r\n\r\n                            # If we passed all our checks, this is a valid timeseries we care about so append it along with\r\n                            # its ports to all_data_series as a tuple (timeseries['configuration], ports)\r\n                            all_data_series.append((timeseries['configuration'], ports))\r\n                        else:\r\n                            print(f\"Error not a standard configuration: {ports}\")\r\n\r\n            # For each tuple (chapter, ports) in all the chapters and ports that have data we care about\r\n            for data_series, ports in all_data_series:\r\n                # print(ports)\r\n                data_index_offset, ir_port, vp4_port, vwc1_port, vwc2_port, vwc3_port, switch_port = self.get_sensor_port_indexes(ports)\r\n                sensor_data_indexes = self.get_sensor_individual_data_indexes()\r\n\r\n                # Grab data\r\n                for data_point in data_series['values']:\r\n                    data_from_previous_years = False\r\n\r\n                    timestamp = (data_point[0])\r\n\r\n                    # Bandaid fix for Meter randomly giving us data from 1970 and crashing our local timestamp conversion. wtf\r\n                    if timestamp > 0:\r\n                        datapoint_datetime = self.convert_timestamp_to_local_datetime(timestamp)\r\n\r\n                        # Check and only add data that is from current year going forward\r\n                        d_year = datapoint_datetime.year\r\n                        if d_year == specific_year or (datetime.now().month == 1 and datetime.now().day == 1):\r\n\r\n                            # Check and only grab data from install date going forward\r\n                            # This is to avoid grabbing data from a logger that was moved to another field from the\r\n                            # previous field it was installed in\r\n                            if self.install_date <= datapoint_datetime.date():\r\n                                converted_results[\"dates\"].append(datapoint_datetime)\r\n\r\n                                # Grabbing IR data\r\n                                if ir_port is not None:\r\n                                    ir_temp_value = data_point[ir_port][sensor_data_indexes['ir temp']]['value']\r\n                                    if ir_temp_value == 'None' or ir_temp_value == '':\r\n                                        ir_temp_value = None\r\n                                    converted_results[\"canopy temperature\"].append(ir_temp_value)\r\n                                else:\r\n                                    converted_results[\"canopy temperature\"].append(None)\r\n\r\n                                # Grabbing VP4/Atmos 14 data\r\n                                if vp4_port is not None:\r\n                                    vp4_air_temp_value = data_point[vp4_port][sensor_data_indexes['vp4 air temp']]['value']\r\n                                    if vp4_air_temp_value == 'None' or vp4_air_temp_value == '':\r\n                                        vp4_air_temp_value = None\r\n                                    converted_results[\"ambient temperature\"].append(vp4_air_temp_value)\r\n\r\n                                    vp_rh_value = data_point[vp4_port][sensor_data_indexes['vp4 rh']]['value']\r\n                                    if vp_rh_value == 'None' or vp_rh_value == '':\r\n                                        vp_rh_value = None\r\n                                    converted_results[\"rh\"].append(vp_rh_value)\r\n\r\n                                    vp_vpd_value = data_point[vp4_port][sensor_data_indexes['vp4 vpd']]['value']\r\n                                    if vp_vpd_value == 'None' or vp_vpd_value == '':\r\n                                        vp_vpd_value = None\r\n                                    converted_results[\"vpd\"].append(vp_vpd_value)\r\n                                else:\r\n                                    converted_results[\"ambient temperature\"].append(None)\r\n                                    converted_results[\"rh\"].append(None)\r\n                                    converted_results[\"vpd\"].append(None)\r\n\r\n                                # Grabbing VWC 1 data\r\n                                if vwc1_port is not None:\r\n                                    vwc1_value = data_point[vwc1_port][sensor_data_indexes['vwc volumetric']]['value']\r\n                                    if vwc1_value == 'None' or vwc1_value == '':\r\n                                        vwc1_value = None\r\n                                    converted_results[\"vwc_1\"].append(vwc1_value)\r\n\r\n                                    if ports[vwc1_port - data_index_offset] in ['GS3', 'Terros 12']:\r\n                                        vwc1_ec_value = data_point[vwc1_port][sensor_data_indexes['vwc ec']]['value']\r\n                                        if vwc1_ec_value == 'None' or vwc1_ec_value == '':\r\n                                            vwc1_ec_value = None\r\n                                        converted_results[\"vwc_1_ec\"].append(vwc1_ec_value)\r\n                                    else:\r\n                                        converted_results[\"vwc_1_ec\"].append(None)\r\n                                else:\r\n                                    converted_results[\"vwc_1\"].append(None)\r\n\r\n                                # Grabbing VWC 2 data\r\n                                if vwc2_port is not None:\r\n                                    vwc2_value = data_point[vwc2_port][sensor_data_indexes['vwc volumetric']]['value']\r\n                                    if vwc2_value == 'None' or vwc2_value == '':\r\n                                        vwc2_value = None\r\n                                    converted_results[\"vwc_2\"].append(vwc2_value)\r\n\r\n                                    if ports[vwc2_port - data_index_offset] in ['GS3', 'Terros 12']:\r\n                                        vwc2_ec_value = data_point[vwc2_port][sensor_data_indexes['vwc ec']]['value']\r\n                                        if vwc2_ec_value == 'None' or vwc2_ec_value == '':\r\n                                            vwc2_ec_value = None\r\n                                        converted_results[\"vwc_2_ec\"].append(vwc2_ec_value)\r\n                                    else:\r\n                                        converted_results[\"vwc_2_ec\"].append(None)\r\n                                else:\r\n                                    converted_results[\"vwc_2\"].append(None)\r\n\r\n                                # Grabbing VWC 3 data\r\n                                if vwc3_port is not None:\r\n                                    vwc3_value = data_point[vwc3_port][sensor_data_indexes['vwc volumetric']]['value']\r\n                                    if vwc3_value == 'None' or vwc3_value == '':\r\n                                        vwc3_value = None\r\n                                    converted_results[\"vwc_3\"].append(vwc3_value)\r\n\r\n                                    if ports[vwc3_port - data_index_offset] in ['GS3', 'Terros 12']:\r\n                                        vwc3_ec_value = data_point[vwc3_port][sensor_data_indexes['vwc ec']]['value']\r\n                                        if vwc3_ec_value == 'None' or vwc3_ec_value == '':\r\n                                            vwc3_ec_value = None\r\n                                        converted_results[\"vwc_3_ec\"].append(vwc3_ec_value)\r\n                                    else:\r\n                                        converted_results[\"vwc_3_ec\"].append(None)\r\n                                else:\r\n                                    converted_results[\"vwc_3\"].append(None)\r\n\r\n                                # Grabbing Switch data\r\n                                if switch_port is not None:\r\n                                    switch_value = data_point[switch_port][sensor_data_indexes['switch minutes']]['value']\r\n                                    if switch_value == 'None' or switch_value == '':\r\n                                        switch_value = None\r\n                                    converted_results[\"daily switch\"].append(switch_value)\r\n                                else:\r\n                                    converted_results[\"daily switch\"].append(None)\r\n                            # else:\r\n                            #     print('Ignored some data from earlier in the year')\r\n                        else:\r\n                            data_from_previous_years = True\r\n\r\n            if data_from_previous_years:\r\n                print(\"Ignored some data from previous years\")\r\n        return converted_results\r\n\r\n    def get_all_ports_information_weather_stations(self, raw_dxd: dict,\r\n                                                   specific_year: int = datetime.now().year) -> dict:\r\n        \"\"\"\r\n        Read the dxd file and returns the information in all ports.\r\n\r\n        :param specific_year:\r\n        :param raw_dxd:\r\n            Dxd file to read\r\n        :return:\r\n            Dictionary holding the raw values in each port\r\n        \"\"\"\r\n        raw_dxd_dict = None\r\n        converted_results = {\"dates\": [], \"solar radiation\": [], \"precipitation\": [], \"lightning activity\": [],\r\n                             \"lightning distance\": [],\r\n                             \"wind direction\": [], \"wind speed\": [], \"gust speed\": [], \"ambient temperature\": [],\r\n                             \"relative humidity\": [], \"atmospheric pressure\": [],\r\n                             \"x axis level\": [], \"y axis level\": [], \"vpd\": []}\r\n        data_from_previous_years = False\r\n        min_number_of_values_for_timeseries = 5\r\n\r\n        if \"device\" in raw_dxd:\r\n            all_data_series = []\r\n            for ind, timeseries in enumerate(raw_dxd['device']['timeseries']):\r\n                # Check that the timeseries has at least 5 values\r\n                if len(timeseries['configuration']['values']) >= min_number_of_values_for_timeseries:\r\n                    # Grab start and end of timeseries\r\n                    timeseries_start_date_timestamp = timeseries['configuration']['values'][0][0]\r\n                    timeseries_end_date_timestamp = timeseries['configuration']['values'][-1][0]\r\n\r\n                    # Special check because some timeseries seem to get 1 random value at the very end from an\r\n                    # early year. When this happens, we just grab the date for the 2nd to last value instead of\r\n                    # the last\r\n                    if timeseries_end_date_timestamp < timeseries_start_date_timestamp:\r\n                        # start = self.convert_timestamp_to_local_datetime(timeseries_start_date_timestamp)\r\n                        # end = self.convert_timestamp_to_local_datetime(timeseries_end_date_timestamp)\r\n                        #\r\n                        # print('Error: END DATE < START DATE')\r\n                        # print(self.field.name)\r\n                        # print(self.name)\r\n                        # print(f'{end} < {start}')\r\n                        # print()\r\n                        timeseries_end_date_timestamp = timeseries['configuration']['values'][-2][0]\r\n\r\n                    # Convert those start and end timestamps to datetimes\r\n                    timeseries_start_date_datetime = self.convert_timestamp_to_local_datetime(\r\n                        timeseries_start_date_timestamp)\r\n                    timeseries_start_date_datetime_year = timeseries_start_date_datetime.year\r\n\r\n                    timeseries_end_date_datetime = self.convert_timestamp_to_local_datetime(\r\n                        timeseries_end_date_timestamp)\r\n                    timeseries_end_date_datetime_year = timeseries_end_date_datetime.year\r\n\r\n                    if timeseries_start_date_datetime_year <= specific_year <= timeseries_end_date_datetime_year:\r\n                        # Grab the port configuration of this particular timeseries\r\n                        ports = self.get_ports(timeseries['configuration']['sensors'])\r\n\r\n                        # Check if ports.keys() has every one of the required ports\r\n                        required_ports = [2]\r\n                        has_all_ports = True\r\n\r\n                        for port_num in required_ports:\r\n                            if port_num not in ports.keys():\r\n                                has_all_ports = False\r\n                                break\r\n\r\n                        # If it has all required ports and each port is set to the correct sensor:\r\n                        if has_all_ports and ports[2] in ['Atmos 41']:\r\n\r\n                            # If we passed all our checks, this is a valid timeseries we care about so append it along with\r\n                            # its ports to all_data_series as a tuple (timeseries['configuration], ports)\r\n                            all_data_series.append((timeseries['configuration'], ports))\r\n                        else:\r\n                            print(f\"Error not a standard configuration: {ports}\")\r\n\r\n            # For each tuple (chapter, ports) in all the chapters and ports that have data we care about\r\n            for data_series, ports in all_data_series:\r\n                # print(ports)\r\n                data_index_offset, atmos_41_port = self.get_sensor_port_indexes_weather_stations(ports)\r\n                sensor_data_indexes = self.get_sensor_individual_data_indexes_weather_stations()\r\n\r\n                # Grab data\r\n                for data_point in data_series['values']:\r\n\r\n                    data_from_previous_years = False\r\n\r\n                    timestamp = (data_point[0])\r\n\r\n                    # Bandaid fix for Meter randomly giving us data from 1970 and crashing our local timestamp conversion. wtf\r\n                    if timestamp > 0:\r\n                        datapoint_datetime = self.convert_timestamp_to_local_datetime(timestamp)\r\n\r\n                        # Check and only add data that is from current year going forward\r\n                        d_year = datapoint_datetime.year\r\n                        if d_year == specific_year or (datetime.now().month == 1 and datetime.now().day == 1):\r\n\r\n                            '''\r\n                            {\"dates\": [], \"solar radiation\": [], \"precipitation\": [], \"lightning activity\": [],\r\n                             \"lightning distance\": [],\r\n                             \"wind direction\": [], \"wind speed\": [], \"gust speed\": [], \"ambient temperature\": [],\r\n                             \"relative humidity\": [], \"atmospheric pressure\": [],\r\n                             \"x axis level\": [], \"y axis level\": [], \"vpd\": []}\r\n                            '''\r\n                            # Check and only grab data from install date going forward\r\n                            # This is to avoid grabbing data from a logger that was moved to another field from the\r\n                            # previous field it was installed in\r\n                            try:\r\n                                if self.install_date <= datapoint_datetime.date():\r\n                                    converted_results[\"dates\"].append(datapoint_datetime)\r\n\r\n                                    if atmos_41_port is not None:\r\n                                        solar_radiation_value = data_point[atmos_41_port][sensor_data_indexes[\r\n                                            'solar radiation']]['value']\r\n                                        precipitation_value = data_point[atmos_41_port][sensor_data_indexes[\r\n                                            'precipitation']]['value']\r\n                                        lightning_activity_value = data_point[atmos_41_port][sensor_data_indexes[\r\n                                            'lightning activity']]['value']\r\n                                        lightning_distance_value = data_point[atmos_41_port][sensor_data_indexes[\r\n                                            'lightning distance']]['value']\r\n                                        wind_direction_value = data_point[atmos_41_port][sensor_data_indexes[\r\n                                            'wind direction']]['value']\r\n                                        wind_speed_value = data_point[atmos_41_port][sensor_data_indexes[\r\n                                            'wind speed']]['value']\r\n                                        gust_speed_value = data_point[atmos_41_port][sensor_data_indexes[\r\n                                            'gust speed']]['value']\r\n                                        ambient_temperature_value = data_point[atmos_41_port][sensor_data_indexes[\r\n                                            'air temperature']][\r\n                                            'value']\r\n                                        relative_humidity_value = data_point[atmos_41_port][sensor_data_indexes[\r\n                                            'relative humidity']][\r\n                                            'value']\r\n                                        atmospheric_pressure_value = data_point[atmos_41_port][sensor_data_indexes[\r\n                                            'atmospheric pressure']][\r\n                                            'value']\r\n                                        x_axis_level_value = data_point[atmos_41_port][sensor_data_indexes[\r\n                                            'x axis level']]['value']\r\n                                        y_axis_level_value = data_point[atmos_41_port][sensor_data_indexes[\r\n                                            'y axis level']]['value']\r\n                                        vpd_value = data_point[atmos_41_port][sensor_data_indexes['vpd']]['value']\r\n\r\n                                        if solar_radiation_value == 'None' or solar_radiation_value == '':\r\n                                            solar_radiation_value = None\r\n                                        if precipitation_value == 'None' or precipitation_value == '':\r\n                                            precipitation_value = None\r\n                                        if lightning_activity_value == 'None' or lightning_activity_value == '':\r\n                                            lightning_activity_value = None\r\n                                        if lightning_distance_value == 'None' or lightning_distance_value == '':\r\n                                            lightning_distance_value = None\r\n                                        if wind_direction_value == 'None' or wind_direction_value == '':\r\n                                            wind_direction_value = None\r\n                                        if wind_speed_value == 'None' or wind_speed_value == '':\r\n                                            wind_speed_value = None\r\n                                        if gust_speed_value == 'None' or gust_speed_value == '':\r\n                                            gust_speed_value = None\r\n                                        if ambient_temperature_value == 'None' or ambient_temperature_value == '':\r\n                                            ambient_temperature_value = None\r\n                                        if relative_humidity_value == 'None' or relative_humidity_value == '':\r\n                                            relative_humidity_value = None\r\n                                        if atmospheric_pressure_value == 'None' or atmospheric_pressure_value == '':\r\n                                            atmospheric_pressure_value = None\r\n                                        if x_axis_level_value == 'None' or x_axis_level_value == '':\r\n                                            x_axis_level_value = None\r\n                                        if y_axis_level_value == 'None' or y_axis_level_value == '':\r\n                                            y_axis_level_value = None\r\n                                        if vpd_value == 'None' or vpd_value == '':\r\n                                            vpd_value = None\r\n\r\n                                        converted_results[\"solar radiation\"].append(solar_radiation_value)\r\n                                        converted_results[\"precipitation\"].append(precipitation_value)\r\n                                        converted_results[\"lightning activity\"].append(lightning_activity_value)\r\n                                        converted_results[\"lightning distance\"].append(lightning_distance_value)\r\n                                        converted_results[\"wind direction\"].append(wind_direction_value)\r\n                                        converted_results[\"wind speed\"].append(wind_speed_value)\r\n                                        converted_results[\"gust speed\"].append(gust_speed_value)\r\n                                        converted_results[\"ambient temperature\"].append(ambient_temperature_value)\r\n                                        converted_results[\"relative humidity\"].append(relative_humidity_value)\r\n                                        converted_results[\"atmospheric pressure\"].append(atmospheric_pressure_value)\r\n                                        converted_results[\"x axis level\"].append(x_axis_level_value)\r\n                                        converted_results[\"y axis level\"].append(y_axis_level_value)\r\n                                        converted_results[\"vpd\"].append(vpd_value)\r\n                                    else:\r\n                                        converted_results[\"solar radiation\"].append(None)\r\n                                        converted_results[\"precipitation\"].append(None)\r\n                                        converted_results[\"lightning activity\"].append(None)\r\n                                        converted_results[\"lightning distance\"].append(None)\r\n                                        converted_results[\"wind direction\"].append(None)\r\n                                        converted_results[\"wind speed\"].append(None)\r\n                                        converted_results[\"gust speed\"].append(None)\r\n                                        converted_results[\"ambient temperature\"].append(None)\r\n                                        converted_results[\"relative humidity\"].append(None)\r\n                                        converted_results[\"atmospheric pressure\"].append(None)\r\n                                        converted_results[\"x axis level\"].append(None)\r\n                                        converted_results[\"y axis level\"].append(None)\r\n                                        converted_results[\"vpd\"].append(None)\r\n                            except Exception as e:\r\n                                print(f\"Error: {e}\")\r\n                                print()\r\n                        else:\r\n                            data_from_previous_years = True\r\n\r\n            if data_from_previous_years:\r\n                print(\"Ignored some data from previous years\")\r\n        return converted_results\r\n\r\n    def get_sensor_port_indexes(self, ports):\r\n        ir_port = None\r\n        vp4_port = None\r\n        vwc1_port = None\r\n        vwc2_port = None\r\n        vwc3_port = None\r\n        switch_port = None\r\n        vwc_ports = []\r\n        # Offset to account for Zentra data return having a timestamp in index 0, mrid in index 1,\r\n        # and 1 extra value in index 2. Actual sensors don't start until index 3. Using an offset of 2\r\n        # since the port data starts at 1 already, not from 0. So 1 + 2 = 3 as the first sensor data index.\r\n        data_index_offset = 2\r\n        for key in ports:\r\n            value = ports[key]\r\n            if ir_port is None and value == 'Infra Red':\r\n                ir_port = key + data_index_offset\r\n            elif vp4_port is None and value in ['VP4', 'Atmos 14']:\r\n                vp4_port = key + data_index_offset\r\n            elif value in ['GS1', 'GS3', 'Terros 10', 'Terros 12']:\r\n                vwc_ports.append(key)\r\n            elif switch_port is None and value == 'Switch':\r\n                switch_port = key + data_index_offset\r\n\r\n        # VWC ports grabbed after in a sorted list to ensure we assign them in the correct order. The first port with\r\n        # a VWC sensor will be vwc1_port, the second will be vwc2_port, and so on until we have 3 ports for VWCs\r\n        for key in sorted(vwc_ports):\r\n            if vwc1_port is None:\r\n                vwc1_port = key + data_index_offset\r\n            elif vwc2_port is None:\r\n                vwc2_port = key + data_index_offset\r\n            elif vwc3_port is None:\r\n                vwc3_port = key + data_index_offset\r\n\r\n        return data_index_offset, ir_port, vp4_port, vwc1_port, vwc2_port, vwc3_port, switch_port\r\n\r\n    def get_sensor_port_indexes_weather_stations(self, ports):\r\n        atmos_41_port = None\r\n        # Offset to account for Zentra data return having a timestamp in index 0, mrid in index 1,\r\n        # and 1 extra value in index 2. Actual sensors don't start until index 3. Using an offset of 2\r\n        # since the port data starts at 1 already, not from 0. So 1 + 2 = 3 as the first sensor data index.\r\n        data_index_offset = 2\r\n        for ind, key in enumerate(ports):\r\n            value = ports[key]\r\n            if atmos_41_port is None and value == 'Atmos 41':\r\n                # Can't use key as the value to designate the atmos_41_port because the API doesn't respect that.\r\n                # It uses the index to position the values, not the key for some stupid reason\r\n                # atmos_41_port = key + data_index_offset\r\n                atmos_41_port = ind + 1 + data_index_offset\r\n\r\n        return data_index_offset, atmos_41_port\r\n\r\n    def get_ports(self, time_series, dxd_save_location: str = DXD_DIRECTORY) -> dict:\r\n        \"\"\"\r\n        Read the dxd file and return information on what sensor is connected to each port.\r\n\r\n        :return:\r\n            Dictionary with the port and what sensor is connected to each port\r\n                {'number': value, ...}\r\n        \"\"\"\r\n        ports = {}\r\n        for sensor in time_series:\r\n            sensor_type = self.sensor_name(sensor[\"sensor_number\"])\r\n            # ports[sensor[\"port\"]] = sensor[\"sensor_number\"]\r\n            ports[sensor[\"port\"]] = sensor_type\r\n\r\n        return ports\r\n\r\n    def remove_duplicate_data(self, raw_data: dict, ports: dict) -> dict:\r\n        print('Looking for duplicates-')\r\n        duplicates = []\r\n        duplicate_dates = []\r\n        converted_results = {\"dates\": [], \"canopy temperature\": [], \"ambient temperature\": [], \"rh\": [], \"vpd\": [],\r\n                             \"vwc 8\": [], \"vwc 16\": [], \"vwc 24\": [], \"daily gallons\": [], \"daily switch\": []}\r\n        for ind, val in enumerate(raw_data[\"dates\"]):\r\n            # print('Checking' + str(val))\r\n            if val in converted_results[\"dates\"]:\r\n                duplicates.append(ind)\r\n                duplicate_dates.append(val)\r\n            else:\r\n                converted_results[\"dates\"].append(raw_data[\"dates\"][ind])\r\n                converted_results[\"canopy temperature\"].append(raw_data[\"canopy temperature\"][ind])\r\n                converted_results[\"ambient temperature\"].append(raw_data[\"ambient temperature\"][ind])\r\n                converted_results[\"rh\"].append(raw_data[\"rh\"][ind])\r\n                converted_results[\"vpd\"].append(raw_data[\"vpd\"][ind])\r\n                converted_results[\"vwc 8\"].append(raw_data[\"vwc 8\"][ind])\r\n                converted_results[\"vwc 16\"].append(raw_data[\"vwc 16\"][ind])\r\n                converted_results[\"vwc 24\"].append(raw_data[\"vwc 24\"][ind])\r\n                if self.switch_connected(ports):\r\n                    converted_results[\"daily switch\"].append(raw_data[\"daily switch\"][ind])\r\n\r\n        if duplicates:\r\n            print(\"-Duplicates found at:\")\r\n            print(duplicates)\r\n        else:\r\n            print(\"-No duplicates found\")\r\n        # print(duplicate_dates)        #To show duplicate dates\r\n        print()\r\n        return converted_results\r\n\r\n    def remove_duplicate_data_2(self, raw_data: dict):\r\n        duplicates = []\r\n        duplicate_dates = []\r\n\r\n        tally = defaultdict(list)\r\n        for i, item in enumerate(raw_data):\r\n            tally[item].append(i)\r\n        returned = ((key, locs) for key, locs in tally.items()\r\n                    if len(locs) > 1)\r\n\r\n        for dup in returned:\r\n            print(dup)\r\n            duplicate_dates.append(dup[-1][-1])\r\n        print(duplicate_dates)\r\n\r\n    def remove_out_of_order_data(self, raw_data: dict, ports: dict) -> dict:\r\n        print('Looking for out of order data-')\r\n        mark_for_removal = []\r\n        out_of_order = []\r\n        latest = raw_data[\"dates\"][0]\r\n        for ind, i in enumerate(raw_data[\"dates\"]):\r\n            if latest > i:\r\n                mark_for_removal.append(ind)\r\n                out_of_order.append(i)\r\n            else:\r\n                latest = i\r\n\r\n        if mark_for_removal:\r\n            print(\"-Out of order data found at:\")\r\n            print(mark_for_removal)\r\n            print(out_of_order)\r\n            print(\"--Removing out of order data\")\r\n\r\n            # Reversing the list that is marked for removal so that when we start removing those indeces we don't\r\n            # have issues with later indeces being changed from the deletion of earlier ones\r\n            mark_for_removal.reverse()\r\n            for ind in mark_for_removal:\r\n                del raw_data[\"dates\"][ind]\r\n                del raw_data[\"canopy temperature\"][ind]\r\n                del raw_data[\"ambient temperature\"][ind]\r\n                del raw_data[\"rh\"][ind]\r\n                del raw_data[\"vpd\"][ind]\r\n                del raw_data[\"vwc 8\"][ind]\r\n                del raw_data[\"vwc 16\"][ind]\r\n                del raw_data[\"vwc 24\"][ind]\r\n                if self.switch_connected(ports):\r\n                    del raw_data[\"daily switch\"][ind]\r\n\r\n            print(\"--Removed\")\r\n        else:\r\n            print(\"-No out of order data found\")\r\n        print()\r\n\r\n        return raw_data\r\n\r\n    def sensor_name(self, sensor: int) -> str:\r\n        \"\"\"\r\n        Function to return the sensor name given its numeric value.\r\n\r\n        Used for printing what sensor we are working on.\r\n        :param sensor: Number assigned to the type of sensor\r\n        :return:\r\n            String with the sensor name\r\n        \"\"\"\r\n        return {\r\n            64: 'Infra Red',\r\n            67: 'Infra Red',\r\n            68: 'Infra Red',\r\n            102: 'VP4',\r\n            123: 'Atmos 14',\r\n            241: 'GS1',\r\n            238: 'Terros 10',\r\n            119: 'GS3',\r\n            103: 'Terros 12',\r\n            180: 'Flow Meter',\r\n            183: 'Flow Meter',\r\n            220: 'Switch',\r\n            221: 'Switch',\r\n            133: 'Battery',\r\n            134: 'Logger',\r\n            93: 'Atmos 41'\r\n        }.get(sensor, 'Other')\r\n\r\n    ##########################################################\r\n    # Converts decagon date into standard date               #\r\n    ##########################################################\r\n    def convert_dates(self, raw_date):\r\n        \"\"\"\r\n        Function to convert decagon date into standard date.\r\n\r\n        :param raw_date:\r\n        :return:\r\n            datetime_val: datetime object with the converted date\r\n        \"\"\"\r\n\r\n        converted_date = time.gmtime(raw_date + 946684800)\r\n        datetime_val = datetime(*converted_date[:6])\r\n        return datetime_val\r\n\r\n    def convert_datetime_to_timestamp(self, raw_datetime):\r\n        return datetime.timestamp(raw_datetime)\r\n\r\n    def convert_last_download_time_to_datetime(self, raw_time):\r\n        return datetime.strptime(raw_time, '%Y-%m-%dT%H:%M:%SZ')\r\n\r\n    def convert_utc_timestamp_to_utc_datetime(self, raw_timestamp):\r\n        utc_dt = datetime.utcfromtimestamp(raw_timestamp)\r\n        return utc_dt\r\n\r\n    def convert_utc_datetime_to_local_datetime(self, utc_raw_time):\r\n        from_zone = tz.tzutc()\r\n        to_zone = tz.tzlocal()\r\n\r\n        utc = utc_raw_time.replace(tzinfo=from_zone)\r\n        pacific = utc.astimezone(to_zone)\r\n        return pacific\r\n\r\n    def convert_timestamp_to_local_datetime(self, timestamp):\r\n        time_start = self.convert_utc_timestamp_to_utc_datetime(timestamp)\r\n        time_local_start = self.convert_utc_datetime_to_local_datetime(time_start)\r\n        dt = time_local_start.replace(tzinfo=None)\r\n        return dt\r\n\r\n    def flow_meter_connected(self, raw_ports: dict) -> bool:\r\n        \"\"\"\r\n        Function to determine whether a Flow Meter is connected based on its sensor number.\r\n\r\n        :param raw_ports: Dictionary with the ports information for the logger. We want to check port 5 for\r\n            a flow meter\r\n        :return:\r\n            Boolean indicating if a flow meter is connected\r\n        \"\"\"\r\n\r\n        # 180 and 183 are the sensor numbers for flow meters\r\n        if 6 in raw_ports.keys():\r\n            if raw_ports[6] == 180 or raw_ports[6] == 183:\r\n                return True\r\n            else:\r\n                return False\r\n        else:\r\n            return False\r\n\r\n    def switch_connected(self, raw_ports: dict) -> bool:\r\n        \"\"\"\r\n        Function to determine whether a Pressure Switch is connected based on its sensor number.\r\n\r\n        :param raw_ports: Dictionary with the ports information for the logger. We want to check port 5 for\r\n            a pressure switch\r\n        :return:\r\n            Boolean indicating if a pressure switch is connected\r\n        \"\"\"\r\n\r\n        # 220 and 221 are the sensor numbers for pressure switches\r\n        if 6 in raw_ports.keys():\r\n            if raw_ports[6] == 'Switch':\r\n                return True\r\n            else:\r\n                return False\r\n        else:\r\n            return False\r\n\r\n    def vp4_connected(self, raw_ports: dict) -> bool:\r\n        \"\"\"\r\n        Function to determine whether a VP4 is connected based on its sensor number.\r\n\r\n        :param raw_ports: Dictionary with the ports information for the logger. We want to check port 2 for a VP4\r\n        :return:\r\n            Boolean indicating if a vp4 is connected\r\n        \"\"\"\r\n        ## Bandaid for Casey\r\n        if 2 not in raw_ports:\r\n            if raw_ports[3] == 102 or 123:\r\n                return True\r\n        if raw_ports[2] == 102 or 123:\r\n            return True\r\n        else:\r\n            return False\r\n\r\n    def delete_last_day(self, data: dict) -> dict:\r\n        \"\"\"\r\n        Function to delete the last row of data if it is from today.\r\n\r\n        We only want to write information from the previous day. If some information is in the DXD from today because\r\n            download is at midnight, that data needs to be deleted and not written into the GSheet.\r\n        Since we only care about data at the hottest time of the day, we won't lose any important data by doing this\r\n            deletion other than possibly losing gallons or switch minutes if irrigation was on at midnight.\r\n        In this case, we will simply subtract the gallons or minutes from the previous values in the logger so the next\r\n            time it check if the new value is higher it is artificially inflated with the gallons lost in the midnight data\r\n            deletion\r\n        :param data:\r\n        :return:\r\n        \"\"\"\r\n\r\n        print('\\tDelete Last Day')\r\n        todayRaw = date.today()\r\n        lastElement = len(data[\"dates\"]) - 1\r\n        if lastElement >= 0:\r\n            if data[\"dates\"][lastElement].date() == todayRaw:\r\n                print(\"\\tDeleting extra same day data\")\r\n                # try:\r\n                #     if self.switch_connected(self.ports):\r\n                #         print('Switch totals: ')\r\n                #         print(self.daily_switch)\r\n                #         if len(self.daily_switch) >= 1:\r\n                #             last_swi = self.daily_switch[-1]\r\n                #             del self.daily_switch[-1]\r\n                #         else:\r\n                #             last_swi = 0\r\n                #         print('Leftover switch total: ' + str(last_swi))\r\n                #\r\n                #         self.prev_day_switch = last_swi\r\n                # except Exception as error:\r\n                #     print('\\tSome error in delete_last_day - water part')\r\n                #     print(error)\r\n\r\n                try:\r\n                    for key in data.keys():\r\n                        if data[key]:\r\n                            del data[key][lastElement]\r\n                except Exception as error:\r\n                    print('\\tSome error in delete_last_day - results part Z6')\r\n                    print(error)\r\n        return data\r\n\r\n    def get_kc(self, data: dict) -> dict:\r\n        \"\"\"\r\n        Function to get the kc for a crop\r\n\r\n        :param data: Dictionary holding all the data that is still missing kc info\r\n        :return: data with kc information added\r\n        \"\"\"\r\n        self.crop_coefficient = CropCoefficient()\r\n        data['kc'] = []\r\n        for data_point in data[\"dates\"]:\r\n            kc = self.crop_coefficient.get_kc(\r\n                self.crop_type.lower(), data_point.date(),\r\n                planting_date=self.planting_date\r\n            )\r\n            if kc is None:\r\n                data['kc'].append(0)\r\n            else:\r\n                data['kc'].append(kc)\r\n\r\n        # print()\r\n        # print('\\tGot KC data:')\r\n        # print('\\t', data['kc'])\r\n        # print()\r\n        return data\r\n\r\n    def update_eto(self, latest_et: float):\r\n        print('   Updating eto for logger:' + str(self.id))\r\n        self.dbwriter = DBWriter()\r\n        yesterdayRaw = date.today() - timedelta(1)\r\n        yesterday = '{0}-{1}-{2}'.format(yesterdayRaw.year, yesterdayRaw.month, yesterdayRaw.day)\r\n        yesterday = \"'\" + yesterday + \"'\"\r\n        # print(yesterday)\r\n        # print(latest_et)\r\n        logger_name = self.name\r\n        field_name = self.field.name\r\n        field_name = self.dbwriter.remove_unwanted_chars_for_db_dataset(field_name)\r\n        project = self.dbwriter.get_db_project(self.crop_type)\r\n        dataset_id = project + '.' + field_name + '.' + logger_name\r\n        dataset_id = \"`\" + dataset_id + \"`\"\r\n        if latest_et == None:\r\n            latest_et = 0\r\n        dml_statement = \"UPDATE \" + str(dataset_id) + ' SET eto = ' + str(latest_et) + ' WHERE date = ' + yesterday \\\r\n            # '@yesterday'\r\n\r\n        self.dbwriter.run_dml(dml_statement, project=project)\r\n\r\n    def update_etc(self):\r\n        print('   Updating etc for logger:' + str(self.id))\r\n        self.dbwriter = DBWriter()\r\n        yesterdayRaw = date.today() - timedelta(1)\r\n        yesterday = '{0}-{1}-{2}'.format(yesterdayRaw.year, yesterdayRaw.month, yesterdayRaw.day)\r\n        yesterday = \"'\" + yesterday + \"'\"\r\n        logger_name = self.name\r\n        field_name = self.field.name\r\n        field_name = self.dbwriter.remove_unwanted_chars_for_db_dataset(field_name)\r\n        project = self.dbwriter.get_db_project(self.crop_type)\r\n        dataset_id = project + '.' + field_name + '.' + logger_name\r\n        dataset_id = \"`\" + dataset_id + \"`\"\r\n\r\n        # TODO check if kc is null\r\n        kc = self.get_kc_from_db()\r\n        if kc != None:\r\n            dml_statement = \"UPDATE \" + dataset_id + 'SET etc = eto * kc WHERE date = ' + yesterday\r\n            self.dbwriter.run_dml(dml_statement, project=project)\r\n        else:\r\n            print(\"       No kc, can't calculate ETc\")\r\n        return kc\r\n\r\n    def update_et_hours(self):\r\n        self.dbwriter = DBWriter()\r\n        yesterdayRaw = date.today() - timedelta(1)\r\n        yesterday = '{0}-{1}-{2}'.format(yesterdayRaw.year, yesterdayRaw.month, yesterdayRaw.day)\r\n        yesterday = \"'\" + yesterday + \"'\"\r\n        logger_name = self.name\r\n        field_name = self.field.name\r\n        field_name = self.dbwriter.remove_unwanted_chars_for_db_dataset(field_name)\r\n        project = self.dbwriter.get_db_project(self.crop_type)\r\n        dataset_id = project + '.' + field_name + '.' + logger_name\r\n        dataset_id = \"`\" + dataset_id + \"`\"\r\n        # etc_dml_statement = \"SELECT etc FROM \" + dataset_id + ' WHERE date = ' + yesterday\r\n        # etc_dml_statement = \"SELECT etc FROM \" + dataset_id + ' WHERE etc is not NULL ORDER BY date DESC LIMIT 1'\r\n        # print('Getting etc from DB')\r\n        # etc_response = self.dbwriter.run_dml(etc_dml_statement)\r\n        # for e in etc_response:\r\n        #     etc = e[\"etc\"]\r\n        #     print(etc)\r\n        if isinstance(self.irrigation_set_acres, str):\r\n            acres = float(self.irrigation_set_acres.replace(',', ''))\r\n        elif isinstance(self.irrigation_set_acres, int):\r\n            acres = float(self.irrigation_set_acres)\r\n        elif isinstance(self.irrigation_set_acres, float):\r\n            acres = self.irrigation_set_acres\r\n        else:\r\n            acres = 0\r\n        if isinstance(self.gpm, str):\r\n            gpm = float(self.gpm.replace(',', ''))\r\n        elif isinstance(self.gpm, int):\r\n            gpm = float(self.gpm)\r\n        elif isinstance(self.gpm, float):\r\n            gpm = self.gpm\r\n        else:\r\n            gpm = 0\r\n\r\n        if gpm != 0:\r\n            et_hours_pending_etc_mult = ((449 * acres) / (gpm * 0.85))\r\n\r\n            print(\"Updating et_hours data for table: \" + dataset_id)\r\n\r\n            dml_statement = \"UPDATE \" + dataset_id + \" SET et_hours = ROUND(\" \\\r\n                            + str(et_hours_pending_etc_mult) + \" * etc) \" \\\r\n                            + \" WHERE date = \" + yesterday\r\n\r\n            self.dbwriter.run_dml(dml_statement, project=project)\r\n        else:\r\n            print('   GPM is 0')\r\n\r\n    def get_kc_from_db(self) -> float:\r\n        print('   Checking kc for logger:' + str(self.id))\r\n        self.dbwriter = DBWriter()\r\n        yesterdayRaw = date.today() - timedelta(1)\r\n        yesterday = '{0}-{1}-{2}'.format(yesterdayRaw.year, yesterdayRaw.month, yesterdayRaw.day)\r\n        yesterday = \"'\" + yesterday + \"'\"\r\n        logger_name = self.name\r\n        field_name = self.field.name\r\n        field_name = self.dbwriter.remove_unwanted_chars_for_db_dataset(field_name)\r\n        project = self.dbwriter.get_db_project(self.crop_type)\r\n        dataset_id = project + '.' + field_name + '.' + logger_name\r\n        dataset_id = \"`\" + dataset_id + \"`\"\r\n\r\n        dml_statement = \"SELECT kc FROM \" + dataset_id + ' WHERE date = ' + yesterday\r\n        kc_response = self.dbwriter.run_dml(dml_statement, project=project)\r\n        kc = 0\r\n        for e in kc_response:\r\n            # print(e[\"eto\"])\r\n            kc = e[\"kc\"]\r\n        print('   Got KC = ' + str(kc))\r\n        print()\r\n        return kc\r\n\r\n    def merge_et_db_with_logger_db_values(self):\r\n        self.dbwriter = DBWriter()\r\n        logger_name = self.name\r\n        field_name = self.field.name\r\n        field_name = self.dbwriter.remove_unwanted_chars_for_db_dataset(field_name)\r\n        project = self.dbwriter.get_db_project(self.crop_type)\r\n        dataset_id = project + '.' + field_name + '.' + logger_name\r\n        dataset_id = \"`\" + dataset_id + \"`\"\r\n        et_id = \"stomato-info.ET.\" + str(self.field.cimis_station)\r\n        et_id = \"`\" + et_id + \"`\"\r\n\r\n        acres = self.irrigation_set_acres\r\n        gpm = self.gpm\r\n\r\n        if gpm != 0:\r\n            et_hours_pending_etc_mult = ((449 * acres) / (gpm * 0.85))\r\n\r\n            print(f\"\\t\\tUpdating all et data for table: {dataset_id} with ET station: {self.field.cimis_station}\")\r\n\r\n            dml_statement = \"MERGE \" + dataset_id + \" T \" \\\r\n                            + \"USING \" + et_id + \" S \" \\\r\n                            + \"ON (T.date = S.date AND T.eto IS NULL)  \" \\\r\n                            + \"WHEN MATCHED THEN \" \\\r\n                            + \"UPDATE SET eto = s.eto, etc = s.eto * t.kc, et_hours = ROUND(\" + str(\r\n                et_hours_pending_etc_mult) + \" * s.eto * t.kc)\"\r\n        else:\r\n            print('   GPM is 0')\r\n            dml_statement = \"MERGE \" + dataset_id + \" T \" \\\r\n                            + \"USING \" + et_id + \" S \" \\\r\n                            + \"ON (T.date = S.date AND T.eto IS NULL)  \" \\\r\n                            + \"WHEN MATCHED THEN \" \\\r\n                            + \"UPDATE SET eto = s.eto, etc = s.eto * t.kc \"\r\n        self.dbwriter.run_dml(dml_statement, project=project)\r\n\r\n    def grab_portal_data(self, data: dict) -> dict:\r\n        \"\"\"\r\n        Function to grab just the last day of data from the dictionary data\r\n         and assign it to a new dictionary called portal_data_dict and return that\r\n\r\n        :param data: dict (str, list)\r\n        :return portal_data_dict: dict (str, value)\r\n        \"\"\"\r\n        data_keys = list(data.keys())\r\n        portal_data_dict = dict.fromkeys(data_keys)\r\n        for key in data_keys:\r\n            if len(data[key]) > 0:\r\n                portal_data_dict[key] = data[key][-1]\r\n        return portal_data_dict\r\n\r\n    def check_for_notifications_final_results(self, final_results_converted: dict, warnings: bool = True,\r\n                                              errors: bool = True):\r\n        \"\"\"\r\n        Function to check the daily values after conversion and processing to see if any of the numbers are\r\n        outside of the thresholds\r\n\r\n        :param errors:\r\n        :param warnings:\r\n        :param field_name:\r\n        :param final_results_converted: Dictionary holding the values for the previous day\r\n            final_results_converted = {\"dates\": [], \"canopy temperature\": [], \"ambient temperature\": [], \"relative_humidity\": [],\r\n            \"vpd\": [], \"vwc 18\": [], \"vwc 36\": [], \"sdd\": [], \"cwsi\": [], \"daily gallons\": [], \"daily switch\": []}\r\n        :return:\r\n        \"\"\"\r\n        technician = self.field.grower.technician\r\n        field_name = self.field.name\r\n\r\n        thresholds = Thresholds()\r\n\r\n        if path.exists(DXD_DIRECTORY):\r\n            file_name = DXD_DIRECTORY + self.id + '.dxd'\r\n\r\n        battery_level = self.read_battery_level(file_name)\r\n\r\n        if errors:\r\n            if battery_level < thresholds.battery_threshold and battery_level is not None:\r\n                technician.all_notifications.add_notification(\r\n                    Notification_SensorError(\r\n                        datetime.now(),\r\n                        field_name,\r\n                        self,\r\n                        \"Battery\",\r\n                        \"Battery Level of: \" + str(battery_level) + \" is less than \" + str(thresholds.battery_threshold)\r\n                    )\r\n                )\r\n\r\n        if final_results_converted[\"dates\"]:\r\n            for ind, date in enumerate(final_results_converted[\"dates\"]):\r\n                cwsi = final_results_converted[\"cwsi\"][ind]\r\n                vwc_1 = final_results_converted[\"vwc_1\"][ind]\r\n                vwc_2 = final_results_converted[\"vwc_2\"][ind]\r\n                vwc_3 = final_results_converted[\"vwc_3\"][ind]\r\n                sdd = final_results_converted[\"sdd\"][ind]\r\n                air_temperature = final_results_converted[\"ambient temperature\"][ind]\r\n                # TODO Check lowest temperature values\r\n                canopy_temperature = final_results_converted[\"canopy temperature\"][ind]\r\n                relative_humidity = final_results_converted[\"rh\"][ind]\r\n                vpd = final_results_converted[\"vpd\"][ind]\r\n\r\n                if self.field.field_type != 'R&D':\r\n\r\n                    self.vp4_notifications(field_name, date, air_temperature, relative_humidity, vpd, technician,\r\n                                           thresholds, warnings=warnings, errors=errors)\r\n\r\n                    if self.ir_active:\r\n                        self.canopy_temperature_notifications(field_name, date, canopy_temperature, technician,\r\n                                                              thresholds, warnings=warnings, errors=errors)\r\n\r\n                        self.psi_notifications(field_name, date, cwsi, technician, thresholds, warnings=warnings,\r\n                                               errors=errors)\r\n                    else:\r\n                        self.psi_not_active_notification(field_name, date, technician, warnings=warnings, errors=errors)\r\n\r\n                    self.vwc_notifications(field_name, self.soil, date, vwc_1, vwc_2, vwc_3, technician, thresholds,\r\n                                           warnings=warnings, errors=errors)\r\n\r\n    def check_for_notifications_all_data(self, all_data: dict):\r\n        \"\"\"\r\n        Function to check the daily values after conversion and before processing to see if any of the numbers are\r\n        None for the whole day. If they are, create a notification for the sensor\r\n\r\n        :param all_data: Dictionary holding the values for all data from the API call for all hours\r\n        \"\"\"\r\n        technician = self.field.grower.technician\r\n\r\n        canopy_temperature_issue = False\r\n        vp4_issue = False\r\n        vwc_1_issue = False\r\n        vwc_2_issue = False\r\n        vwc_3_issue = False\r\n\r\n        # Check if logger is sending data\r\n        required_data_points = 2\r\n        if len(all_data['dates']) < required_data_points:\r\n            if all_data['dates']:\r\n                dates_date = all_data['dates'][-1]\r\n            else:\r\n                dates_date = datetime.now() - timedelta(days=1)\r\n            technician.all_notifications.add_notification(\r\n                Notification_SensorError(\r\n                    dates_date,\r\n                    self.field.name,\r\n                    self,\r\n                    \"Z6 Logger\",\r\n                    \"We did not get any data for this logger. Signal issue? Connection issue? Got hit by a tractor issue?\"\r\n                )\r\n            )\r\n\r\n        # Loop through all values and check for None values\r\n        for ind, dp in enumerate(all_data['dates']):\r\n            if all_data['canopy temperature'][ind] is None:\r\n                canopy_temperature_issue = True\r\n                canopy_temperature_date = dp\r\n            if (all_data['ambient temperature'][ind] is None or\r\n                    all_data['rh'][ind] is None or\r\n                    all_data['vpd'][ind] is None):\r\n                vp4_issue = True\r\n                vp4_date = dp\r\n            if all_data['vwc_1'][ind] is None:\r\n                vwc_1_issue = True\r\n                vwc_1_date = dp\r\n            if all_data['vwc_2'][ind] is None:\r\n                vwc_2_issue = True\r\n                vwc_2_date = dp\r\n            if all_data['vwc_3'][ind] is None:\r\n                vwc_3_issue = True\r\n                vwc_3_date = dp\r\n\r\n        # Canopy Temp\r\n        if canopy_temperature_issue:\r\n            technician.all_notifications.add_notification(\r\n                Notification_SensorError(\r\n                    canopy_temperature_date,\r\n                    self.field.name,\r\n                    self,\r\n                    \"Canopy Temp\",\r\n                    \"Canopy Temp is showing None at some point in the day (Not necessarily at the hottest time). Connection issue?\"\r\n                )\r\n            )\r\n\r\n        # VP4\r\n        if vp4_issue:\r\n            technician.all_notifications.add_notification(\r\n                Notification_SensorError(\r\n                    vp4_date,\r\n                    self.field.name,\r\n                    self,\r\n                    \"VP4\",\r\n                    \"VP4 is showing None at some point in the day (Not necessarily at the hottest time). Connection issue?\"\r\n                )\r\n            )\r\n\r\n        # VWC_1\r\n        if vwc_1_issue:\r\n            technician.all_notifications.add_notification(\r\n                Notification_SensorError(\r\n                    vwc_1_date,\r\n                    self.field.name,\r\n                    self,\r\n                    \"VWC_1\",\r\n                    \"VWC_1 is showing None at some point in the day (Not necessarily at the hottest time). Connection issue?\"\r\n                )\r\n            )\r\n\r\n        # VWC_2\r\n        if vwc_2_issue:\r\n            technician.all_notifications.add_notification(\r\n                Notification_SensorError(\r\n                    vwc_2_date,\r\n                    self.field.name,\r\n                    self,\r\n                    \"VWC_2\",\r\n                    \"VWC_2 is showing None at some point in the day (Not necessarily at the hottest time). Connection issue?\"\r\n                )\r\n            )\r\n\r\n        # VWC_3\r\n        if vwc_3_issue:\r\n            technician.all_notifications.add_notification(\r\n                Notification_SensorError(\r\n                    vwc_3_date,\r\n                    self.field.name,\r\n                    self,\r\n                    \"VWC_3\",\r\n                    \"VWC_3 is showing None at some point in the day (Not necessarily at the hottest time). Connection issue?\"\r\n                )\r\n            )\r\n\r\n    def vwc_notifications(\r\n            self,\r\n            field_name: str,\r\n            soil,\r\n            date: datetime,\r\n            vwc_1: float,\r\n            vwc_2: float,\r\n            vwc_3: float,\r\n            technician: Technician,\r\n            thresholds: Thresholds,\r\n            warnings: bool = True,\r\n            errors: bool = True\r\n    ):\r\n        if vwc_1 is not None:\r\n            if vwc_1 < thresholds.error_vwc_lower or vwc_1 > thresholds.error_vwc_upper:\r\n                if errors:\r\n                    technician.all_notifications.add_notification(\r\n                        Notification_SensorError(\r\n                            date,\r\n                            field_name,\r\n                            self,\r\n                            \"VWC_1\",\r\n                            f\"VWC_1 is out of reasonable bounds: {vwc_1}. Connection issue?\"\r\n                        )\r\n                    )\r\n            elif vwc_1 < soil.very_low_upper:\r\n                if warnings:\r\n                    technician.all_notifications.add_notification(\r\n                        Notification_TechnicianWarning(\r\n                            date,\r\n                            field_name,\r\n                            self,\r\n                            \"VWC_1\",\r\n                            f\"!!!! VWC_1 in VERY LOW levels: {str(round(vwc_1, 1))} < {str(soil.very_low_upper)} for soil type: {soil.soil_type}\"\r\n                        )\r\n                    )\r\n            elif vwc_1 < soil.low_upper:\r\n                if warnings:\r\n                    technician.all_notifications.add_notification(\r\n                        Notification_TechnicianWarning(\r\n                            date,\r\n                            field_name,\r\n                            self,\r\n                            \"VWC_1\",\r\n                            f\"!! VWC_1 in LOW levels: {str(round(vwc_1, 1))} < {str(soil.low_upper)} for soil type: {soil.soil_type}\"\r\n                        )\r\n                    )\r\n            elif vwc_1 > soil.very_high_lower:\r\n                if warnings:\r\n                    technician.all_notifications.add_notification(\r\n                        Notification_TechnicianWarning(\r\n                            date,\r\n                            field_name,\r\n                            self,\r\n                            \"VWC_1\",\r\n                            f\"!!!! VWC_1 in VERY HIGH levels: {str(round(vwc_1, 1))} > {str(soil.very_high_lower)} for soil type: {soil.soil_type}\"\r\n                        )\r\n                    )\r\n\r\n        if vwc_2 is not None:\r\n            if vwc_2 < thresholds.error_vwc_lower or vwc_2 > thresholds.error_vwc_upper:\r\n                if errors:\r\n                    technician.all_notifications.add_notification(\r\n                        Notification_SensorError(\r\n                            date,\r\n                            field_name,\r\n                            self,\r\n                            \"VWC_2\",\r\n                            f\"VWC_2 is out of reasonable bounds: {vwc_2}. Connection issue?\"\r\n                        )\r\n                    )\r\n            elif vwc_2 < soil.very_low_upper:\r\n                if warnings:\r\n                    technician.all_notifications.add_notification(\r\n                        Notification_TechnicianWarning(\r\n                            date,\r\n                            field_name,\r\n                            self,\r\n                            \"VWC_2\",\r\n                            f\"!!!! VWC_2 in VERY LOW levels: {str(round(vwc_2, 1))} < {str(soil.very_low_upper)} for soil type: {soil.soil_type}\"\r\n                        )\r\n                    )\r\n            elif vwc_2 < soil.low_upper:\r\n                if warnings:\r\n                    technician.all_notifications.add_notification(\r\n                        Notification_TechnicianWarning(\r\n                            date,\r\n                            field_name,\r\n                            self,\r\n                            \"VWC_2\",\r\n                            f\"!! VWC_2 in LOW levels: {str(round(vwc_2, 1))} < {str(soil.low_upper)} for soil type: {soil.soil_type}\"\r\n                        )\r\n                    )\r\n            elif vwc_2 > soil.very_high_lower:\r\n                if warnings:\r\n                    technician.all_notifications.add_notification(\r\n                        Notification_TechnicianWarning(\r\n                            date,\r\n                            field_name,\r\n                            self,\r\n                            \"VWC_2\",\r\n                            f\"!!!! VWC_2 in VERY HIGH levels: {str(round(vwc_2, 1))} > {str(soil.very_high_lower)} for soil type: {soil.soil_type}\"\r\n                        )\r\n                    )\r\n\r\n        if vwc_3 is not None:\r\n            if vwc_3 < thresholds.error_vwc_lower or vwc_3 > thresholds.error_vwc_upper:\r\n                if errors:\r\n                    technician.all_notifications.add_notification(\r\n                        Notification_SensorError(\r\n                            date,\r\n                            field_name,\r\n                            self,\r\n                            \"VWC_3\",\r\n                            f\"VWC_3 is out of reasonable bounds: {vwc_3}. Connection issue?\"\r\n                        )\r\n                    )\r\n            elif vwc_3 < soil.very_low_upper:\r\n                if warnings:\r\n                    technician.all_notifications.add_notification(\r\n                        Notification_TechnicianWarning(\r\n                            date,\r\n                            field_name,\r\n                            self,\r\n                            \"VWC_3\",\r\n                            f\"!!!! VWC_3 in VERY LOW levels: {str(round(vwc_3, 1))} < {str(soil.very_low_upper)} for soil type: {soil.soil_type}\"\r\n                        )\r\n                    )\r\n            elif vwc_3 < soil.low_upper:\r\n                if warnings:\r\n                    technician.all_notifications.add_notification(\r\n                        Notification_TechnicianWarning(\r\n                            date,\r\n                            field_name,\r\n                            self,\r\n                            \"VWC_3\",\r\n                            f\"!! VWC_3 in LOW levels: {str(round(vwc_3, 1))} < {str(soil.low_upper)} for soil type: {soil.soil_type}\"\r\n                        )\r\n                    )\r\n            elif vwc_3 > soil.very_high_lower:\r\n                if warnings:\r\n                    technician.all_notifications.add_notification(\r\n                        Notification_TechnicianWarning(\r\n                            date,\r\n                            field_name,\r\n                            self,\r\n                            \"VWC_3\",\r\n                            f\"!!!! VWC_3 in VERY HIGH levels: {str(round(vwc_3, 1))} > {str(soil.very_high_lower)} for soil type: {soil.soil_type}\"\r\n                        )\r\n                    )\r\n\r\n    def vp4_notifications(\r\n            self,\r\n            field_name: str,\r\n            date: datetime,\r\n            air_temp: float,\r\n            relative_humidity: float,\r\n            vpd: float,\r\n            technician: Technician,\r\n            thresholds: Thresholds,\r\n            warnings: bool = True,\r\n            errors: bool = True\r\n    ):\r\n        # VP4 NOTIFICATIONS ----\r\n        if errors:\r\n            # COMMENTING THIS OUT FOR NOW SINCE WE CHECK FOR NONES IN ALL VALUES BEFORE THIS AND THIS WAS CAUSING\r\n            # DUPLICATE NOTIFICATIONS\r\n            # Check for None values\r\n            # vp4_values = [air_temp, relative_humidity, vpd]\r\n            # vp4_error = False\r\n            # if any(value is None or value == 'None' for value in vp4_values):\r\n            #     vp4_error = True\r\n            #     technician.all_notifications.add_notification(\r\n            #         Notification_SensorError(\r\n            #             date,\r\n            #             field_name,\r\n            #             self,\r\n            #             \"VP4\",\r\n            #             \"VP4 is showing None. Connection issue?\"\r\n            #         )\r\n            #     )\r\n\r\n            # If we didn't get a None on the VP4 values, check for threshold errors\r\n            # if not vp4_error:\r\n            if air_temp is not None:\r\n                if air_temp < thresholds.error_temp_lower:\r\n                    technician.all_notifications.add_notification(\r\n                        Notification_SensorError(\r\n                            date,\r\n                            field_name,\r\n                            self,\r\n                            \"VP4\",\r\n                            f\"Air Temp of: {str(air_temp)} is < {str(thresholds.error_temp_lower)}\"\r\n                        )\r\n                    )\r\n                elif air_temp > thresholds.error_temp_upper:\r\n                    technician.all_notifications.add_notification(\r\n                        Notification_SensorError(\r\n                            date,\r\n                            field_name,\r\n                            self,\r\n                            \"VP4\",\r\n                            f\"Air Temp of: {str(air_temp)} is > {str(thresholds.error_temp_upper)}\"\r\n                        )\r\n                    )\r\n\r\n            if relative_humidity is not None:\r\n                if relative_humidity < thresholds.error_rh_lower:\r\n                    technician.all_notifications.add_notification(\r\n                        Notification_SensorError(\r\n                            date,\r\n                            field_name,\r\n                            self,\r\n                            \"VP4\",\r\n                            f\"Relative Humidity of: {str(relative_humidity)} is < {str(thresholds.error_rh_lower)}\"\r\n                        )\r\n                    )\r\n                elif relative_humidity > thresholds.error_rh_upper:\r\n                    technician.all_notifications.add_notification(\r\n                        Notification_SensorError(\r\n                            date,\r\n                            field_name,\r\n                            self,\r\n                            \"VP4\",\r\n                            f\"Relative Humidity of: {str(relative_humidity)} is > {str(thresholds.error_rh_upper)}\"\r\n                        )\r\n                    )\r\n\r\n            if vpd is not None:\r\n                if vpd < thresholds.error_vpd_lower:\r\n                    technician.all_notifications.add_notification(\r\n                        Notification_SensorError(\r\n                            date,\r\n                            field_name,\r\n                            self,\r\n                            \"VP4\",\r\n                            f\"VPD of: {str(vpd)} is < {str(thresholds.error_vpd_lower)}\"\r\n                        )\r\n                    )\r\n                elif vpd > thresholds.error_vpd_upper:\r\n                    technician.all_notifications.add_notification(\r\n                        Notification_SensorError(\r\n                            date,\r\n                            field_name,\r\n                            self,\r\n                            \"VP4\",\r\n                            f\"VPD of: {str(vpd)} is > {str(thresholds.error_vpd_upper)}\"\r\n                        )\r\n                    )\r\n\r\n    def canopy_temperature_notifications(\r\n            self,\r\n            field_name: str,\r\n            date: datetime,\r\n            canopy_temp: float,\r\n            technician: Technician,\r\n            thresholds: Thresholds,\r\n            warnings: bool = True,\r\n            errors: bool = True\r\n    ):\r\n        if errors:\r\n            if canopy_temp is None:\r\n                # Not reporting this anymore since its caught by the Notification checks in for All Data, not just\r\n                # hottest time of day\r\n                # technician.all_notifications.add_notification(\r\n                #     Notification_SensorError(\r\n                #         date,\r\n                #         field_name,\r\n                #         self,\r\n                #         \"Canopy Temp\",\r\n                #         \"Canopy Temp is showing None. Connection issue?\"\r\n                #     )\r\n                # )\r\n                pass\r\n            elif canopy_temp < thresholds.error_temp_lower:\r\n                technician.all_notifications.add_notification(\r\n                    Notification_SensorError(\r\n                        date,\r\n                        field_name,\r\n                        self,\r\n                        \"Canopy Temp\",\r\n                        f\"Canopy Temp of: {str(canopy_temp)} is < {str(thresholds.error_temp_lower)}\"\r\n                    )\r\n                )\r\n            elif canopy_temp > thresholds.error_temp_upper:\r\n                technician.all_notifications.add_notification(\r\n                    Notification_SensorError(\r\n                        date,\r\n                        field_name,\r\n                        self,\r\n                        \"Canopy Temp\",\r\n                        f\"Canopy Temp of: {str(canopy_temp)} is > {str(thresholds.error_temp_upper)}\"\r\n                    )\r\n                )\r\n\r\n    def psi_notifications(\r\n            self,\r\n            field_name: str,\r\n            date: datetime,\r\n            psi: float,\r\n            technician: Technician,\r\n            thresholds: Thresholds,\r\n            warnings: bool = True,\r\n            errors: bool = True\r\n    ):\r\n        if psi is None:\r\n            # Not reporting this anymore since its caught by the Notification checks in for All Data, not just\r\n            # hottest time of day\r\n            # if errors:\r\n            #     technician.all_notifications.add_notification(\r\n            #         Notification_SensorError(\r\n            #             date,\r\n            #             field_name,\r\n            #             self,\r\n            #             \"PSI\",\r\n            #             \"PSI is showing None. Connection issue?\"\r\n            #         )\r\n            #     )\r\n            pass\r\n        elif self.field.crop_type in ['Tomatoes', 'Tomato', 'tomatoes', 'tomato']:\r\n            # Tomato PSI Notifications\r\n            if warnings:\r\n                if psi > thresholds.tomato_psi_danger:\r\n                    technician.all_notifications.add_notification(\r\n                        Notification_TechnicianWarning(\r\n                            date,\r\n                            field_name,\r\n                            self,\r\n                            \"PSI\",\r\n                            f\"!!!! PSI in CRITICAL HIGH levels: {str(round(psi, 1))} > {str(thresholds.tomato_psi_danger)}\"\r\n                        )\r\n                    )\r\n        else:\r\n            # Permanent Crop PSI Notifications\r\n            if warnings:\r\n                if psi > thresholds.permanent_psi_danger:\r\n                    technician.all_notifications.add_notification(\r\n                        Notification_TechnicianWarning(\r\n                            date,\r\n                            field_name,\r\n                            self,\r\n                            \"PSI\",\r\n                            f\"!!!! PSI in CRITICAL HIGH levels: {str(round(psi, 1))} > {str(thresholds.permanent_psi_danger)}\"\r\n                        )\r\n                    )\r\n\r\n    def psi_not_active_notification(\r\n            self,\r\n            field_name: str,\r\n            date: datetime,\r\n            technician: Technician,\r\n            warnings: bool = True,\r\n            errors: bool = True\r\n    ):\r\n        if errors:\r\n            formatted_consecutive_psi = [round(tup[0], 2) for tup in self.consecutive_ir_values]\r\n            formatted_consecutive_sdd = [round(tup[1], 2) for tup in self.consecutive_ir_values]\r\n            if self.field.crop_type in ['Tomatoes', 'Tomato', 'tomatoes', 'tomato']:\r\n                # Tomato PSI Notifications\r\n                planting_date_plus_70 = self.planting_date + timedelta(days=70)\r\n                if date.date() >= planting_date_plus_70:\r\n                    technician.all_notifications.add_notification(\r\n                        Notification_SensorError(\r\n                            date,\r\n                            field_name,\r\n                            self,\r\n                            \"PSI\",\r\n                            f\"It's been more than 70 days from planting date and IR is not active. Date: {date.date()} >= Planting + 70: {planting_date_plus_70}. Should it be on? The last 3 psi values were: {formatted_consecutive_psi}. Last 3 sdd values were: {formatted_consecutive_sdd}\"\r\n                        )\r\n                    )\r\n\r\n            elif self.field.crop_type in ['Pistachio', 'pistachio', 'Pistachios', 'pistachios']:\r\n                # Pistachio PSI Notifications\r\n                ir_start_date = datetime(date.year, 3, 1).date()\r\n                ir_start_date_plus_15 = ir_start_date + timedelta(days=15)\r\n                if (2 < date.month < 10) and (date.date() >= ir_start_date_plus_15):\r\n                    technician.all_notifications.add_notification(\r\n                        Notification_SensorError(\r\n                            date,\r\n                            field_name,\r\n                            self,\r\n                            \"PSI\",\r\n                            f\"It's been more than 15 days from activation month and IR is not active. Date: {date.date()} >= March 1 + 15: {ir_start_date_plus_15}. Should it be on? The last 3 psi values were: {formatted_consecutive_psi}. Last 3 sdd values were: {formatted_consecutive_sdd}\"\r\n                        )\r\n                    )\r\n\r\n            elif self.field.crop_type in ['Almond', 'almond', 'Almonds', 'almonds']:\r\n                # Almond PSI Notifications\r\n                ir_start_date = datetime(date.year, 5, 1).date()\r\n                ir_start_date_plus_15 = ir_start_date + timedelta(days=15)\r\n                if (4 < date.month < 11) and (date.date() >= ir_start_date_plus_15):\r\n                    technician.all_notifications.add_notification(\r\n                        Notification_SensorError(\r\n                            date,\r\n                            field_name,\r\n                            self,\r\n                            \"PSI\",\r\n                            f\"It's been more than 15 days from activation month and IR is not active. Date: {date.date()} >= May 1 + 15: {ir_start_date_plus_15}. Should it be on? The last 3 psi values were: {formatted_consecutive_psi}. Last 3 sdd values were: {formatted_consecutive_sdd}\"\r\n                        )\r\n                    )\r\n            else:\r\n                # Other crops\r\n                pass\r\n\r\n\r\n    def update(\r\n            self,\r\n            cimis_stations_pickle,\r\n            write_to_db: bool = False,\r\n            check_for_notifications: bool = False,\r\n            specific_mrid: int = None,\r\n            subtract_from_mrid: int = 0,\r\n            check_updated: bool = False\r\n    ) -> dict:\r\n        \"\"\"\r\n        Function to update the information from each Logger\r\n\r\n        :param write_to_db:\r\n        :param check_for_notifications:\r\n        :param specific_mrid:\r\n        :param subtract_from_mrid:\r\n        :param check_updated:\r\n        :param subtract_from_mrid: Int used to subtract a specific amount from the logger MRIDs for API calls\r\n        :return:\r\n        \"\"\"\r\n        if self.active:\r\n            try:\r\n                portal_data = None\r\n                if self.updated:\r\n                    print('\\tLogger: ' + self.id + '  already updated. Skipping...')\r\n                    return portal_data\r\n                else:\r\n                    print(\r\n                        '========================================================================================================================'\r\n                    )\r\n                    print('LOGGER updating: ')\r\n                    self.to_string()\r\n                    print(\r\n                        '------------------------------------------------------------------------------------------------------------------------'\r\n                    )\r\n                    # Download dxd files from Decaon API\r\n                    print()\r\n                    print('Downloading Data into DXD files-')\r\n                    response_success = self.get_logger_data(\r\n                        specific_mrid=specific_mrid,\r\n                        subtract_from_mrid=subtract_from_mrid\r\n                    )\r\n                    # print('-METER API Quota Delay')\r\n                    # time.sleep(20)\r\n                    print('-Finished')\r\n                    print()\r\n\r\n                    # Read data in dxd files\r\n                    if response_success:\r\n                        print('Reading data-')\r\n                        raw_dxd = self.read_dxd()\r\n                        raw_data = self.get_all_ports_information(raw_dxd)\r\n                        print('-Finished')\r\n                        print()\r\n                        converted_raw_data = raw_data  # NO DUPLICATE REMOVAL\r\n                        # converted_raw_data = self.remove_duplicate_data(raw_data)         #DUPLICATE REMOVAL\r\n                        # self.remove_duplicate_data_2(raw_data)\r\n                        # Testing removal of duplicate data algorithm\r\n                        # converted_raw_data = self.remove_out_of_order_data(converted_raw_data)          #OUT OF ORDER DATA REMOVAL\r\n\r\n                        if check_for_notifications:\r\n                            # Check for notifications\r\n                            try:\r\n                                print('Checking for notifications on all data-')\r\n                                self.check_for_notifications_all_data(converted_raw_data)\r\n                                print('\\t-Finished')\r\n                            except Exception as e:\r\n                                print(\"Error in Logger check_for_notifications_all_data - \" + self.name)\r\n                                print(\"Error type: \" + str(e))\r\n\r\n                        # Process data\r\n                        print('Processing data-')\r\n                        print()\r\n                        print('\\tAll Results Converted -> Before Processing: ')\r\n                        for key, values in converted_raw_data.items():\r\n                            print('\\t', key, \" : \", values)\r\n                            if len(values) > 5:\r\n                                print()\r\n\r\n                        # Update irrigation ledger\r\n                        print('\\tCleaning irrigation ledger')\r\n                        self.cwsi_processor.clean_irrigation_ledger(self.irrigation_ledger)\r\n\r\n                        print('\\tUpdating irrigation ledger')\r\n                        self.cwsi_processor.update_irrigation_ledger(converted_raw_data, self.irrigation_ledger)\r\n                        print('\\t Ledger after update:')\r\n                        self.show_irrigation_ledger()\r\n                        print('\\t ...done')\r\n                        print()\r\n\r\n                        print('\\tGetting hottest and coldest temperatures')\r\n                        highest_temp_values_ind, lowest_temp_values_ind, _ = \\\r\n                            self.cwsi_processor.get_highest_and_lowest_temperature_indexes(converted_raw_data)\r\n\r\n                        print('\\tFinal Results processing')\r\n                        final_results_converted = self.cwsi_processor.final_results(\r\n                            converted_raw_data, highest_temp_values_ind, lowest_temp_values_ind, self\r\n                        )\r\n\r\n                        print('\\tLedger after final results:')\r\n                        self.show_irrigation_ledger()\r\n\r\n                        print()\r\n                        print(\"\\tFinal Results Converted -> After Processing:\")\r\n                        for key, values in final_results_converted.items():\r\n                            print('\\t', key, \" : \", values)\r\n                            if len(values) > 5:\r\n                                print()\r\n\r\n                        # Check irrigation_ledger for delayed completed ledger date switch lists and update db\r\n                        self.check_and_update_delayed_ledger_filled_lists()\r\n                        print()\r\n                        print('\\tLedger after delayed update:')\r\n                        self.show_irrigation_ledger()\r\n\r\n                        # Getting kc\r\n                        final_results_converted = self.get_kc(final_results_converted)\r\n\r\n                        # Get ET data\r\n                        # final_results_converted = self.get_et(final_results_converted, cimis_stations_pickle)\r\n\r\n                        # Delete last row of data if it is from today\r\n                        final_results_converted = self.delete_last_day(final_results_converted)\r\n\r\n                        if self.crop_type.lower() == 'tomatoes' or self.crop_type.lower() == 'tomato':\r\n                            final_results_converted = self.calculate_total_gdd_and_crop_stage(final_results_converted)\r\n                        print('-Finished')\r\n                        print()\r\n\r\n                        # PLUG IN AI ENGINE THROUGH CWSI PROCESSOR\r\n                        # final_results_converted = self.cwsi_processor.irrigation_ai_processing(final_results_converted, self)\r\n\r\n                        print()\r\n                        print('\\tFinal Results before DB write-')\r\n                        for key, values in final_results_converted.items():\r\n                            print('\\t', key, \" : \", values)\r\n                            if len(values) > 5:\r\n                                print()\r\n\r\n                        # Grab only last day data for portal\r\n                        print()\r\n                        print('\\tGrab Portal Data:')\r\n                        portal_data = self.grab_portal_data(final_results_converted)\r\n                        print('\\t-Finished')\r\n\r\n                        if check_for_notifications:\r\n                            # Check for notifications\r\n                            try:\r\n                                print()\r\n                                print('\\tChecking for Notifications on final results')\r\n                                self.check_for_notifications_final_results(final_results_converted, warnings=False)\r\n                                print('\\t-Finished')\r\n                            except Exception as e:\r\n                                print(\"Error in Logger check_for_notifications_final_results - \" + self.name)\r\n                                print(\"Error type: \" + str(e))\r\n\r\n                        # Write data to DB\r\n                        if write_to_db:\r\n                            # self.dbwriter = DBWriter()\r\n                            # self.dbwriter.create_dataset(self.grower.name + '_' + self.field.name)\r\n\r\n                            # the database writes for logger\r\n                            try:\r\n                                if final_results_converted[\"dates\"]:\r\n                                    print()\r\n                                    print('\\tWriting to DB-')\r\n                                    # Get project, dataset and table names\r\n                                    project = self.dbwriter.get_db_project(self.crop_type)\r\n                                    dataset_id = self.field.name\r\n                                    table_id = self.name\r\n\r\n                                    schema = [\r\n                                        bigquery.SchemaField(\"logger_id\", \"STRING\"),\r\n                                        bigquery.SchemaField(\"date\", \"DATE\"),\r\n                                        bigquery.SchemaField(\"time\", \"STRING\"),\r\n                                        bigquery.SchemaField(\"canopy_temperature\", \"FLOAT\"),\r\n                                        bigquery.SchemaField(\"canopy_temperature_celsius\", \"FLOAT\"),\r\n                                        bigquery.SchemaField(\"ambient_temperature\", \"FLOAT\"),\r\n                                        bigquery.SchemaField(\"ambient_temperature_celsius\", \"FLOAT\"),\r\n                                        bigquery.SchemaField(\"vpd\", \"FLOAT\"),\r\n                                        bigquery.SchemaField(\"vwc_1\", \"FLOAT\"),\r\n                                        bigquery.SchemaField(\"vwc_2\", \"FLOAT\"),\r\n                                        bigquery.SchemaField(\"vwc_3\", \"FLOAT\"),\r\n                                        bigquery.SchemaField(\"field_capacity\", \"FLOAT\"),\r\n                                        bigquery.SchemaField(\"wilting_point\", \"FLOAT\"),\r\n                                        bigquery.SchemaField(\"daily_gallons\", \"FLOAT\"),\r\n                                        bigquery.SchemaField(\"daily_switch\", \"FLOAT\"),\r\n                                        bigquery.SchemaField(\"daily_hours\", \"FLOAT\"),\r\n                                        bigquery.SchemaField(\"daily_pressure\", \"FLOAT\"),\r\n                                        bigquery.SchemaField(\"daily_inches\", \"FLOAT\"),\r\n                                        bigquery.SchemaField(\"psi\", \"FLOAT\"),\r\n                                        bigquery.SchemaField(\"psi_threshold\", \"FLOAT\"),\r\n                                        bigquery.SchemaField(\"psi_critical\", \"FLOAT\"),\r\n                                        bigquery.SchemaField(\"sdd\", \"FLOAT\"),\r\n                                        bigquery.SchemaField(\"sdd_celsius\", \"FLOAT\"),\r\n                                        bigquery.SchemaField(\"rh\", \"FLOAT\"),\r\n                                        bigquery.SchemaField(\"eto\", \"FLOAT\"),\r\n                                        bigquery.SchemaField(\"kc\", \"FLOAT\"),\r\n                                        bigquery.SchemaField(\"etc\", \"FLOAT\"),\r\n                                        bigquery.SchemaField(\"et_hours\", \"FLOAT\"),\r\n                                        bigquery.SchemaField(\"phase1_adjustment\", \"FLOAT\"),\r\n                                        bigquery.SchemaField(\"phase1_adjusted\", \"FLOAT\"),\r\n                                        bigquery.SchemaField(\"phase2_adjustment\", \"FLOAT\"),\r\n                                        bigquery.SchemaField(\"phase2_adjusted\", \"FLOAT\"),\r\n                                        bigquery.SchemaField(\"phase3_adjustment\", \"FLOAT\"),\r\n                                        bigquery.SchemaField(\"phase3_adjusted\", \"FLOAT\"),\r\n                                        bigquery.SchemaField(\"vwc_1_ec\", \"FLOAT\"),\r\n                                        bigquery.SchemaField(\"vwc_2_ec\", \"FLOAT\"),\r\n                                        bigquery.SchemaField(\"vwc_3_ec\", \"FLOAT\"),\r\n                                        bigquery.SchemaField(\"lowest_ambient_temperature\", \"FLOAT\"),\r\n                                        bigquery.SchemaField(\"lowest_ambient_temperature_celsius\", \"FLOAT\"),\r\n                                        bigquery.SchemaField(\"gdd\", \"FLOAT\"),\r\n                                        bigquery.SchemaField(\"crop_stage\", \"STRING\"),\r\n                                        bigquery.SchemaField(\"id\", \"STRING\"),\r\n                                        bigquery.SchemaField(\"planting_date\", \"DATE\"),\r\n                                        bigquery.SchemaField(\"variety\", \"STRING\"),\r\n                                    ]\r\n\r\n                                    # Check if the data we have is new or already exists in the DB\r\n                                    db_dates = self.dbwriter.grab_specific_column_table_data(\r\n                                        dataset_id,\r\n                                        table_id,\r\n                                        project,\r\n                                        'date'\r\n                                    )\r\n                                    if db_dates is not None:\r\n                                        db_dates_list = [row[0] for row in db_dates]\r\n                                    else:\r\n                                        db_dates_list = []\r\n\r\n                                    # Prepping data to be written\r\n                                    self.cwsi_processor.prep_data_for_writting_db(final_results_converted, self, db_dates_list)\r\n\r\n                                    self.dbwriter.write_to_table_from_csv(\r\n                                        dataset_id, table_id, 'data.csv', schema, project\r\n                                    )\r\n                                    print('\\tMerging ET table with Logger table')\r\n                                    # Merge ET table with Logger table\r\n                                    self.merge_et_db_with_logger_db_values()\r\n                                    print('\\t-Finished')\r\n\r\n                                else:\r\n                                    print('\\tNothing new to write to DB')\r\n                            except Exception as e:\r\n                                print(\"\\tError in logger db write - \" + self.id)\r\n                                print(\"\\tError type: \" + str(e))\r\n                        self.updated = True\r\n                    else:\r\n                        print('\\tResponse not successful')\r\n                        self.updated = False\r\n\r\n                    print()\r\n                    print(\r\n                        '-------------------------------------Logger ' + self.id + ' Done------------------------------------------------------------'\r\n                    )\r\n                    print()\r\n                    self.to_string()\r\n                    print(\r\n                        '========================================================================================================================'\r\n                    )\r\n                    print()\r\n                    print()\r\n                    print()\r\n                    print()\r\n\r\n                    return portal_data\r\n\r\n                # ------------------------------------HERE uncomment for trycatch\r\n            except Exception as e:\r\n                print(\"Error in logger update - \" + self.id)\r\n                print(\"Error type: \" + str(e))\r\n                self.crashed = True\r\n                # -----------------------------------HERE uncomment for trycatch\r\n        else:\r\n            print('Logger - {} not active'.format(self.id))\r\n\r\n    def show_irrigation_ledger(self):\r\n        print('\\tIrrigation Ledger: ')\r\n        for date_key, switch_list in self.irrigation_ledger.items():\r\n            print(f'\\t\\t{date_key} -> {switch_list}')\r\n\r\n    def check_and_update_delayed_ledger_filled_lists(self):\r\n        dates_to_remove = []\r\n        technician = self.field.grower.technician\r\n        for date, switch_list in self.irrigation_ledger.items():\r\n            if None not in switch_list:\r\n                switch_sum = sum(switch_list)\r\n                dates_to_remove.append(date)\r\n                if switch_sum > 0:\r\n                    print(f'\\tDelayed switch data for date: {date}...updating DB')\r\n                    print(f'\\tSwitch sum: {switch_sum}')\r\n                    self.dbwriter.update_overflow_switch_irr_hours_for_date_by_replacing(\r\n                        self,\r\n                        switch_sum,\r\n                        date\r\n                    )\r\n\r\n                    technician.all_notifications.add_notification(\r\n                        Notification_SensorError(\r\n                            date,\r\n                            self.field.name,\r\n                            self,\r\n                            \"Delayed Irrigation Hours Updated\",\r\n                            f\"Updated irrigation hours for date {date}. There may have been a lack of data reported for\"\r\n                            + f\" this date when originally processed so hours have been updated now that the full 24\"\r\n                            + f\" hours of irrigation are accounted for.\"\r\n                        )\r\n                    )\r\n\r\n        for date in dates_to_remove:\r\n            del self.irrigation_ledger[date]\r\n\r\n    def should_ir_be_active(self, date_to_check: datetime.date = datetime.today().date()) -> bool:\r\n        print('\\t\\tChecking if IR should be active:')\r\n\r\n        # Checking to make sure ir_active is False before the other checks so that we don't turn off IR on permanent\r\n        # crops if it's already on. For tomatoes, ir_active will always be false if we come into this function.\r\n        if self.ir_active is False:\r\n            # Do we have at least 3 tuples of values?\r\n            if len(self.consecutive_ir_values) < 3:\r\n                print(f'\\t\\t\\t Not enough consecutive values to activate yet: {len(self.consecutive_ir_values)}')\r\n                return False\r\n\r\n            # Are the dates in those tuples consecutive dates?\r\n            sorted_dq_dates = sorted([item[2] for item in self.consecutive_ir_values])\r\n            dates_are_consecutive = self.are_dates_consecutive(sorted_dq_dates)\r\n            if not dates_are_consecutive:\r\n                print(f'\\t\\t\\t Dates are NOT consecutive: {sorted_dq_dates}')\r\n                return False\r\n\r\n        if self.crop_type.lower() in ['tomato', 'tomatoes']:\r\n            psi_threshold_high = 1.6\r\n            sdd_threshold = -5.0\r\n            planting_date_plus_30 = self.planting_date + timedelta(days=30)\r\n            if date_to_check > planting_date_plus_30:\r\n                for psi_val, sdd_val, _ in self.consecutive_ir_values:\r\n                    if psi_val > psi_threshold_high or sdd_val > sdd_threshold:\r\n                        print(f'\\t\\t\\t SI values did not pass:')\r\n                        print(f'\\t\\t\\t PSI -> {psi_val} > {psi_threshold_high}?')\r\n                        print(f'\\t\\t\\t SDD -> {sdd_val} > {sdd_threshold}?')\r\n                        return False\r\n                else:\r\n                    # IR Should be turned on\r\n                    technician = self.field.grower.technician\r\n                    # Tomato PSI Turning on Notification\r\n                    technician.all_notifications.add_notification(\r\n                        Notification_SensorError(\r\n                            date_to_check,\r\n                            self.field.name,\r\n                            self,\r\n                            \"PSI\",\r\n                            f\"PSI just turned on for {self.name}\"\r\n                        )\r\n                    )\r\n                    print('\\t IR Turning On')\r\n                    return True\r\n\r\n        elif self.crop_type.lower() in ['almond', 'almonds']:\r\n            psi_threshold_high = 0.5\r\n\r\n            # SDD checks were not working well since trees turn on earlier in the year so cold temps can lead to\r\n            # deceivingly low SDDs. Off for now\r\n            # sdd_threshold = -3.0\r\n\r\n            # If date_to_check's date is between March and October (Active almond tree cycle)\r\n            if 2 < date_to_check.month < 10:\r\n                # if within months check if IR is On.If on, leave it on\r\n                if self.ir_active:\r\n                    return True\r\n                # if IR has not been turned on see if it passes checks\r\n                else:\r\n                    for psi_val, _, _ in self.consecutive_ir_values:\r\n                        if psi_val > psi_threshold_high:  # or sdd_val > sdd_threshold:\r\n                            return False\r\n                    # if IR is off and did not fail checks turn IR ON\r\n                    else:\r\n                        print('\\t IR Turning On')\r\n                        return True\r\n            # if outside the months turn IR off always\r\n            else:\r\n                return False\r\n\r\n        elif self.crop_type.lower() in ['pistachio', 'pistachios']:\r\n            psi_threshold_high = 0.5\r\n\r\n            # SDD checks were not working well since trees turn on earlier in the year so cold temps can lead to\r\n            # deceivingly low SDDs. Off for now\r\n            # sdd_threshold = -3.0\r\n\r\n            # If date_to_check's date is between May and November (Active pistachio tree cycle)\r\n            if 4 < date_to_check.month < 11:\r\n                # if within months check if IR is On. If on, leave it on\r\n                if self.ir_active:\r\n                    return True\r\n                # if IR has not been turned on see if it passes checks\r\n                else:\r\n                    for psi_val, _, _ in self.consecutive_ir_values:\r\n                        if psi_val > psi_threshold_high:  # or sdd_val > sdd_threshold:\r\n                            return False\r\n                    # if IR is off and did not fail checks turn IR ON\r\n                    else:\r\n                        print('\\t IR Turning On')\r\n                        return True\r\n            # if outside the months turn IR off always\r\n            else:\r\n                # print('\\t\\tIR NOT Active')\r\n                return False\r\n\r\n        elif self.crop_type.lower() == 'dates' or self.crop_type.lower() == 'date':\r\n            # La Quinta requested to always see canopy temp and SDD\r\n            # print('\\t\\tIR Active')\r\n            return True\r\n        return False\r\n\r\n    def are_dates_consecutive(self, dates_to_check):\r\n        \"\"\"\r\n        Function to check if dates in a list are consecutive\r\n\r\n        :param dates_to_check:\r\n        :return:\r\n        \"\"\"\r\n        # Check if the differences between consecutive dates are exactly one day\r\n        for i in range(1, len(dates_to_check)):\r\n            if dates_to_check[i] - dates_to_check[i - 1] != timedelta(days=1):\r\n                return False\r\n        return True\r\n\r\n    def deactivate(self):\r\n        print('Deactivating Logger {}...'.format(self.id))\r\n        self.active = False\r\n        if self.uninstall_date is None:\r\n            self.uninstall_date = datetime.now().date()\r\n        print('Done')\r\n\r\n    def calculate_total_gdd_and_crop_stage(self, final_results_converted: dict) -> dict:\r\n        print('\\tCalculating GDDS and Crop Stage:')\r\n        accumulated_gdd = previous_gdd = self.gdd_total\r\n\r\n        crop_stage = 'NA'\r\n        for gdd in final_results_converted['gdd']:\r\n            accumulated_gdd += gdd\r\n            crop_stage = self.cwsi_processor.get_crop_stage(accumulated_gdd)\r\n            final_results_converted['crop stage'].append(crop_stage)\r\n        self.gdd_total = accumulated_gdd\r\n        self.crop_stage = crop_stage\r\n        print(\r\n            f'\\t GDD before: {str(previous_gdd)} -> GDD after: {str(self.gdd_total)} -> Crop Stage: {self.crop_stage}'\r\n        )\r\n        print()\r\n        return final_results_converted\r\n\r\n    def recalculate_total_gdd(self):\r\n        pass\r\n\r\n    def get_sensor_individual_data_indexes(self):\r\n\r\n        get_sensor_individual_data_indexes = {}\r\n        # Dictionary to hold what index the data is being stored in from the API return for Zentra v1\r\n        get_sensor_individual_data_indexes['ir temp'] = 0\r\n        get_sensor_individual_data_indexes['ir body temp'] = 1\r\n        get_sensor_individual_data_indexes['vp4 air temp'] = 0\r\n        get_sensor_individual_data_indexes['vp4 rh'] = 1\r\n        get_sensor_individual_data_indexes['vp4 pressure'] = 2\r\n        get_sensor_individual_data_indexes['vp4 vpd'] = 3\r\n        get_sensor_individual_data_indexes['vwc volumetric'] = 0\r\n        get_sensor_individual_data_indexes['vwc soil temp'] = 1\r\n        get_sensor_individual_data_indexes['vwc ec'] = 2\r\n        get_sensor_individual_data_indexes['switch minutes'] = 0\r\n\r\n        return get_sensor_individual_data_indexes\r\n\r\n    def get_sensor_individual_data_indexes_weather_stations(self):\r\n        \"\"\"\r\n\r\n        :return:\r\n        \"\"\"\r\n\r\n        get_sensor_individual_data_indexes = {}\r\n        # Dictionary to hold what index the data is being stored in from the API return for Zentra v1\r\n        get_sensor_individual_data_indexes['solar radiation'] = 0\r\n        get_sensor_individual_data_indexes['precipitation'] = 1\r\n        get_sensor_individual_data_indexes['lightning activity'] = 2\r\n        get_sensor_individual_data_indexes['lightning distance'] = 3\r\n        get_sensor_individual_data_indexes['wind direction'] = 4\r\n        get_sensor_individual_data_indexes['wind speed'] = 5\r\n        get_sensor_individual_data_indexes['gust speed'] = 6\r\n        get_sensor_individual_data_indexes['air temperature'] = 7\r\n        get_sensor_individual_data_indexes['relative humidity'] = 8\r\n        get_sensor_individual_data_indexes['atmospheric pressure'] = 9\r\n        get_sensor_individual_data_indexes['x axis level'] = 10\r\n        get_sensor_individual_data_indexes['y axis level'] = 11\r\n        get_sensor_individual_data_indexes['max precip rate'] = 12\r\n        get_sensor_individual_data_indexes['rh sensor temp'] = 13\r\n        get_sensor_individual_data_indexes['vpd'] = 14\r\n\r\n        return get_sensor_individual_data_indexes\r\n\r\n    def set_broken(self):\r\n        self.broken = True\r\n        self.active = False\r\n        self.uninstall_date = datetime.now().date()\r\n\r\n    def update_ir_consecutive_data(self, cwsi: float, sdd: float, date):\r\n        \"\"\"\r\n        Function to handle adding consecutive IR data to a loggers .consecutive_ir_values deque to be able to analyze\r\n        whether an IR should turn on or not\r\n\r\n        :param date: Datetime of the date we want to append to the que\r\n        :param sdd: Float of the SDD we want to append to the que\r\n        :param cwsi: Float of the CWSI we want to append to the que\r\n        \"\"\"\r\n        date = date.date()\r\n        if cwsi is not None and sdd is not None and date is not None:\r\n            date_already_in_dq = self.is_date_in_deque(date, self.consecutive_ir_values)\r\n            if date_already_in_dq:\r\n                print(f'\\t\\tSkipping adding consec IR data for {date} because its already in the dq')\r\n            else:\r\n                # print(f'\\t\\tAdding consecutive IR data to dq')\r\n                self.consecutive_ir_values.append((cwsi, sdd, date))\r\n        if len(self.consecutive_ir_values) > 3:\r\n            self.consecutive_ir_values.popleft()\r\n\r\n    def is_date_in_deque(self, date, dq):\r\n        \"\"\"\r\n        Function to check if a date is in a dq\r\n\r\n        :param date: datetime.date()\r\n        :param dq: [(cwsi, sdd, date), (cwsi, sdd, date), (cwsi, sdd, date)]\r\n        :return:\r\n        \"\"\"\r\n        for _, _, consec_date in dq:\r\n            if consec_date == date:\r\n                return True\r\n        return False\r\n\r\n    def get_et(self, final_results_converted, cimis_stations_pickle):\r\n        # Check - If we only need yesterday's value, we can grab it from the cimis station pickle. If we need more than\r\n        # yesterday's value, we need grab it from the database.\r\n        final_results_converted['eto'] = []\r\n        final_results_converted['etc'] = []\r\n        final_results_converted['et_hours'] = []\r\n\r\n        if len(final_results_converted['dates']) == 1:\r\n            # Grab et data from latest cimis station pickle\r\n            final_results_converted = self.update_et_values_from_cimis_pickle(cimis_stations_pickle,\r\n                                                                              final_results_converted)\r\n        elif len(final_results_converted['dates']) > 1:\r\n            # Merge et data from et db\r\n            final_results_converted = self.update_et_values_from_et_db(final_results_converted)\r\n        return final_results_converted\r\n\r\n    def update_et_values_from_cimis_pickle(self, cimis_stations_pickle, final_results_converted):\r\n        # Grab data from latest cimis station pickle\r\n        print(\"\\tUpdating ET data from pickle\")\r\n        try:\r\n            cimis_station = self.field.cimis_station\r\n            latest_eto = 0\r\n            for station in cimis_stations_pickle:\r\n                if station.station_number == cimis_station:\r\n                    latest_eto = station.latest_eto_value\r\n                    break\r\n            # calculate etc from eto * kc\r\n            acres = self.irrigation_set_acres\r\n            gpm = self.gpm\r\n\r\n            kc = final_results_converted['kc'][0]\r\n            latest_eto = float(latest_eto)\r\n            etc = latest_eto * kc  # etc = eto * kc\r\n            et_hours = None\r\n            # calculate et_hours\r\n            if gpm != 0:\r\n                # et_hours_pending_etc_mult = ((449 * acres) / (gpm * 0.85))\r\n                # et_hours = round(et_hours_pending_etc_mult * latest_eto * kc)\r\n                et_hours = round(etc * ((449 * acres) / (gpm * 0.85)))\r\n            final_results_converted['eto'].append(latest_eto)\r\n            final_results_converted['etc'].append(etc)\r\n            final_results_converted['et_hours'].append(et_hours)\r\n            print(f\"\\tGot ET data from pickle: ETo: {latest_eto}, kc: {kc}, ETc: {etc}, ET Hours: {et_hours}, Acres: {acres}, GPM: {gpm}\")\r\n        except Exception as error:\r\n            print(\"Error in logger ET data grab from pickle - \" + self.name)\r\n            print(\"Error type: \" + str(error))\r\n            print(f\"ET data from pickle: ETo: {latest_eto}, kc: {kc}, ETc: {etc}, ET Hours: {et_hours}, Acres: {acres}, GPM: {gpm}\")\r\n        return final_results_converted\r\n\r\n    def update_et_values_from_et_db(self, final_results_converted):\r\n        print(\"\\tUpdating ET data from DB\")\r\n        try:\r\n            field_name = self.field.name\r\n            field_name = self.dbwriter.remove_unwanted_chars_for_db_dataset(field_name)\r\n            project = 'stomato-info'\r\n            dataset_id = f\"{project}.ET.{str(self.field.cimis_station)}\"\r\n            dataset_id = \"`\" + dataset_id + \"`\"\r\n\r\n            acres = self.irrigation_set_acres\r\n            gpm = self.gpm\r\n\r\n            start_date = final_results_converted['dates'][0].strftime('%Y-%m-%d')\r\n            end_date = final_results_converted['dates'][-1].strftime('%Y-%m-%d')\r\n\r\n            dml_statement = f\"SELECT * FROM {dataset_id} WHERE date BETWEEN DATE(\\'{start_date}\\') AND DATE(\\'{end_date}\\') ORDER BY date ASC\"\r\n            result = self.dbwriter.run_dml(dml_statement, project=project)\r\n\r\n            et_db_dates = []\r\n            et_db_etos = []\r\n            for row in result:\r\n                et_db_dates.append(row['date'])\r\n                et_db_etos.append(row['eto'])\r\n\r\n            for ind, data_date in enumerate(final_results_converted['dates']):\r\n                eto = None\r\n                etc = None\r\n                et_hours = None\r\n\r\n                if data_date.date() in et_db_dates:\r\n                    eto = et_db_etos[et_db_dates.index(data_date.date())]\r\n                    kc = final_results_converted['kc'][ind]\r\n                    etc = eto * kc  # etc = eto * kc\r\n                    if gpm != 0:\r\n                        et_hours_pending_etc_mult = ((449 * acres) / (gpm * 0.85))\r\n                        et_hours = round(et_hours_pending_etc_mult * eto * kc)\r\n                    else:\r\n                        et_hours = None\r\n\r\n                final_results_converted['eto'].append(eto)\r\n                final_results_converted['etc'].append(etc)\r\n                final_results_converted['et_hours'].append(et_hours)\r\n            print(f\"\\tGot ET data from DB:\")\r\n            print(f\"\\t\\tETO: {final_results_converted['eto']}\")\r\n            print(f\"\\t\\tETC: {final_results_converted['etc']}\")\r\n            print(f\"\\t\\tET Hours: {final_results_converted['et_hours']}\")\r\n        except Exception as error:\r\n            print(\"Error in logger ET data grab from DB - \" + self.name)\r\n            print(\"Error type: \" + str(error))\r\n        return final_results_converted\r\n\r\n\r\ndef convert_datetime_to_timestamp(raw_datetime):\r\n    return datetime.timestamp(raw_datetime)\r\n\r\n\r\ndef convert_utc_timestamp_to_utc_datetime(raw_timestamp):\r\n    utc_dt = datetime.utcfromtimestamp(raw_timestamp)\r\n    return utc_dt\r\n\r\n\r\ndef convert_utc_datetime_to_local_datetime(utc_raw_time):\r\n    from_zone = tz.tzutc()\r\n    to_zone = tz.tzlocal()\r\n\r\n    utc = utc_raw_time.replace(tzinfo=from_zone)\r\n    pacific = utc.astimezone(to_zone)\r\n    return pacific\r\n\r\n# timestamp = 1659988800\r\n# timestamp = 1659985200 #8-8-22\r\n# timestamp = 1655154000 #6-13-22\r\n# meter_time = convert_utc_timestamp_to_utc_datetime(timestamp)\r\n# time_local = convert_utc_datetime_to_local_datetime(meter_time)\r\n# time_local = time_local.replace(tzinfo=None)\r\n# print(time_local.date())\r\n# print(type(time_local.date()))\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/Logger.py b/Logger.py
--- a/Logger.py	(revision f164b16925d5aa8f94c3dc6e3611313783643d25)
+++ b/Logger.py	(date 1720742442751)
@@ -53,40 +53,6 @@
     return 0
 
 
-def change_mrid(logger_ids: list, mr_id_to_set: int = 0):
-    """
-
-    :param logger_ids:
-    :param mr_id_to_set:
-    """
-    try:
-
-        for logger_id in logger_ids:
-            file_path = path.join(DXD_DIRECTORY, logger_id + '.dxd')
-
-            print(f'Changing dxd file MRID {file_path}...')
-
-            if not path.isfile(file_path):
-                raise FileNotFoundError(f"The file '{file_path}' does not exist.")
-
-            with open(file_path, 'r', encoding="utf8") as fd:
-                parsed_json = json.load(fd)
-
-            if mr_id_to_set > -1:
-                print(f'Modifying file MRID with the set back MRID: {mr_id_to_set}')
-                if "created" in parsed_json:
-                    # Modify the MRID to the new value
-                    parsed_json['device']['timeseries'][-1]['configuration']['values'][-1][1] = mr_id_to_set
-
-            # Write the modified JSON back to the file
-            with open(file_path, 'w', encoding="utf8") as fd:
-                json.dump(parsed_json, fd, indent=4, sort_keys=True, default=str)
-
-            print('Successfully changed MRID')
-
-    except Exception as error:
-        print(f'ERROR in changing dxd MRID file for json data for {logger_id}')
-        print(error)
 
 
 class Logger(object):
@@ -590,12 +556,12 @@
 
                         # If it has all required ports and each port is set to the correct sensor:
                         if (has_all_ports and
-                            ports[1] in ['Infra Red'] and
-                            ports[2] in ['VP4', 'Atmos 14'] and
-                            ports[3] in ['GS1', 'Terros 10', 'GS3', 'Terros 12'] and
-                            ports[4] in ['GS1', 'Terros 10', 'GS3', 'Terros 12'] and
-                            ports[5] in ['GS1', 'Terros 10', 'GS3', 'Terros 12'] and
-                            ports[6] in ['Switch']):
+                                ports[1] in ['Infra Red'] and
+                                ports[2] in ['VP4', 'Atmos 14'] and
+                                ports[3] in ['GS1', 'Terros 10', 'GS3', 'Terros 12'] and
+                                ports[4] in ['GS1', 'Terros 10', 'GS3', 'Terros 12'] and
+                                ports[5] in ['GS1', 'Terros 10', 'GS3', 'Terros 12'] and
+                                ports[6] in ['Switch']):
 
                             # If we passed all our checks, this is a valid timeseries we care about so append it along with
                             # its ports to all_data_series as a tuple (timeseries['configuration], ports)
@@ -606,7 +572,8 @@
             # For each tuple (chapter, ports) in all the chapters and ports that have data we care about
             for data_series, ports in all_data_series:
                 # print(ports)
-                data_index_offset, ir_port, vp4_port, vwc1_port, vwc2_port, vwc3_port, switch_port = self.get_sensor_port_indexes(ports)
+                data_index_offset, ir_port, vp4_port, vwc1_port, vwc2_port, vwc3_port, switch_port = self.get_sensor_port_indexes(
+                    ports)
                 sensor_data_indexes = self.get_sensor_individual_data_indexes()
 
                 # Grab data
@@ -640,7 +607,8 @@
 
                                 # Grabbing VP4/Atmos 14 data
                                 if vp4_port is not None:
-                                    vp4_air_temp_value = data_point[vp4_port][sensor_data_indexes['vp4 air temp']]['value']
+                                    vp4_air_temp_value = data_point[vp4_port][sensor_data_indexes['vp4 air temp']][
+                                        'value']
                                     if vp4_air_temp_value == 'None' or vp4_air_temp_value == '':
                                         vp4_air_temp_value = None
                                     converted_results["ambient temperature"].append(vp4_air_temp_value)
@@ -712,7 +680,8 @@
 
                                 # Grabbing Switch data
                                 if switch_port is not None:
-                                    switch_value = data_point[switch_port][sensor_data_indexes['switch minutes']]['value']
+                                    switch_value = data_point[switch_port][sensor_data_indexes['switch minutes']][
+                                        'value']
                                     if switch_value == 'None' or switch_value == '':
                                         switch_value = None
                                     converted_results["daily switch"].append(switch_value)
@@ -2023,7 +1992,6 @@
                 # Other crops
                 pass
 
-
     def update(
             self,
             cimis_stations_pickle,
@@ -2255,7 +2223,8 @@
                                         db_dates_list = []
 
                                     # Prepping data to be written
-                                    self.cwsi_processor.prep_data_for_writting_db(final_results_converted, self, db_dates_list)
+                                    self.cwsi_processor.prep_data_for_writting_db(final_results_converted, self,
+                                                                                  db_dates_list)
 
                                     self.dbwriter.write_to_table_from_csv(
                                         dataset_id, table_id, 'data.csv', schema, project
@@ -2601,11 +2570,13 @@
             final_results_converted['eto'].append(latest_eto)
             final_results_converted['etc'].append(etc)
             final_results_converted['et_hours'].append(et_hours)
-            print(f"\tGot ET data from pickle: ETo: {latest_eto}, kc: {kc}, ETc: {etc}, ET Hours: {et_hours}, Acres: {acres}, GPM: {gpm}")
+            print(
+                f"\tGot ET data from pickle: ETo: {latest_eto}, kc: {kc}, ETc: {etc}, ET Hours: {et_hours}, Acres: {acres}, GPM: {gpm}")
         except Exception as error:
             print("Error in logger ET data grab from pickle - " + self.name)
             print("Error type: " + str(error))
-            print(f"ET data from pickle: ETo: {latest_eto}, kc: {kc}, ETc: {etc}, ET Hours: {et_hours}, Acres: {acres}, GPM: {gpm}")
+            print(
+                f"ET data from pickle: ETo: {latest_eto}, kc: {kc}, ETc: {etc}, ET Hours: {et_hours}, Acres: {acres}, GPM: {gpm}")
         return final_results_converted
 
     def update_et_values_from_et_db(self, final_results_converted):
Index: LoggerSetups.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import time\r\nfrom collections import OrderedDict\r\nfrom datetime import datetime, date, timedelta\r\n\r\nimport requests\r\n\r\nimport DBWriter\r\nimport Decagon\r\nimport GSheetCredentialSevice\r\nimport SQLScripts\r\nimport gSheetReader\r\nfrom CIMIS import CIMIS\r\nfrom CimisStation import CimisStation\r\nfrom DBWriter import DBWriter\r\nfrom Field import Field\r\nfrom Grower import Grower\r\nfrom Logger import Logger\r\nfrom Notifications import Notification_LoggerSetups\r\n\r\nLOGGER_SETUPS_SHEET_ID = '1rsPrX44pCHOhbOHZfWubDkuuexP8pSG8kspW03FAqOU'\r\nNEW_LOGGER_SETUPS_SHEET_ID = '1ycEimLNLXpg7rZ5QxAwTfnAO1BkNBIwOzfpM1dqAxdg'\r\n\r\n\r\ndef setup_grower(grower: str, technician_name: str, email: str = '', region: str = ''):\r\n    \"\"\"\r\n    Function setups a new grower in the pickle\r\n    :param grower: name of grower\r\n    :param technician_name: name of technician\r\n    :param email: grower email\r\n    :param region: grower region\r\n    \"\"\"\r\n    print(\"Adding new grower to Pickle: \" + grower)\r\n    newGrower = Decagon.setup_grower(grower, technician_name, email, region=region)\r\n    # Decagon.addGrowerToGrowers(newGrower)\r\n\r\n\r\ndef setup_growers_fields_loggers_lists() -> tuple[list, list, list]:\r\n    \"\"\"\r\n    Function sets up a list of growers, fields, and active loggers in the current pickle\r\n    :return: Returns a grower list, field list, and active logger list\r\n    \"\"\"\r\n    grower_list = []\r\n    field_list = []\r\n    logger_list = []\r\n    growers = Decagon.open_pickle()\r\n    for grower in growers:\r\n        grower_list.append(grower.name)\r\n        for field in grower.fields:\r\n            field_list.append(field.name)\r\n            for logger in field.loggers:\r\n                if logger.active and field.active:\r\n                    logger_list.append(logger.id)\r\n    return grower_list, field_list, logger_list\r\n\r\ndef check_if_grower_in_list(grower_name:str, grower_list:list, technician:str, region:str) -> list:\r\n    \"\"\"\r\n    Checks to see if grower exists in the grower list, and if not it creates a new grower in the pickle\r\n    :param grower_name: Grower Name\r\n    :param grower_list: List of growers\r\n    :param technician: Assigned Technician\r\n    :param region: Grower Region\r\n    :return: Grower List\r\n    \"\"\"\r\n    if grower_name not in grower_list:\r\n        print(\"Did not find grower, setting up new grower\")\r\n        setup_grower(grower_name, technician, region=region)\r\n        grower_list.append(grower_name)\r\n    return grower_list\r\n\r\n\r\ndef add_logger_id_to_pickle(logger_id:str):\r\n    \"\"\"\r\n    Function adds a logger id to the logger id pickle\r\n    :param logger_id: Logger ID\r\n    \"\"\"\r\n    loggers = Decagon.open_pickle(filename=\"loggerList.pickle\")\r\n    loggers.append(logger_id)\r\n    Decagon.write_pickle(loggers, filename=\"loggerList.pickle\")\r\n\r\n\r\ndef remove_field(grower_target:str, field_target:str):\r\n    \"\"\"\r\n    Function removes a field from a grower\r\n    :param grower_target: Grower Name\r\n    :param field_target: Field Name\r\n    \"\"\"\r\n    growers = Decagon.open_pickle()\r\n    for grower in growers:\r\n        if grower.name == grower_target:\r\n            for field in grower.fields:\r\n                if field.name == field_target:\r\n                    print(f\"Removing Field: {field_target}\")\r\n                    for logger in field.loggers:\r\n                        remove_logger_id_from_pickle(logger.id)\r\n                Decagon.remove_field(grower_target, field_target, True)\r\n    # Decagon.deactivate_field(grower, field)\r\n\r\n    # Decagon.write_pickle(growers)\r\n\r\n\r\ndef deactivate_field(grower:str, field: str, uninstall_date:date):\r\n    \"\"\"\r\n    Function deactivates a field from a grower\r\n    :param grower: Grower Name\r\n    :param field: Field Name\r\n    :param uninstall_date: Uninstall Date\r\n    \"\"\"\r\n    growers = Decagon.open_pickle()\r\n    for g in growers:\r\n        if g.name == grower:\r\n            for f in g.fields:\r\n                if f.name == field:\r\n                    print(\"Removing Field: \" + field)\r\n                    for l in f.loggers:\r\n                        remove_logger_id_from_pickle(l.id)\r\n                        l.uninstall_date = uninstall_date\r\n    Decagon.write_pickle(growers)\r\n    Decagon.deactivate_field(grower, field)\r\n\r\n\r\n\r\ndef remove_logger_id_from_pickle(logger_id: str):\r\n    \"\"\"\r\n\r\n    :param logger_id: Logger ID\r\n    \"\"\"\r\n    loggers = Decagon.open_pickle(filename=\"loggerList.pickle\")\r\n    try:\r\n        loggers.remove(logger_id)\r\n        Decagon.write_pickle(loggers, filename=\"loggerList.pickle\")\r\n        print(\"Removed Logger from Logger ID List\")\r\n    except ValueError:\r\n        print(\"Could not find logger: \" + logger_id)\r\n\r\n\r\ndef remove_all_logger_id_from_pickle():\r\n    \"\"\"\r\n    Removes all logger ID's from a pickle file\r\n    \"\"\"\r\n    id_list = Decagon.open_pickle(filename=\"loggerList.pickle\")\r\n    for logger_id in id_list:\r\n        id_list.remove(logger_id)\r\n    Decagon.write_pickle(id_list, filename=\"loggerList.pickle\")\r\n\r\n\r\ndef show_logger_id_pickle():\r\n    \"\"\"\r\n    Show all the logger ID's from a pickle file\r\n    \"\"\"\r\n    logger = Decagon.open_pickle(filename=\"loggerList.pickle\")\r\n    for logger in logger:\r\n        print(logger)\r\n\r\n\r\ndef set_up_password_dict(service, sheet_id: str) -> dict:\r\n    \"\"\"\r\n    Sets up dictionary of passwords that belong to each logger\r\n    :param service: G Sheet Service\r\n    :param sheet_id: G Sheet ID\r\n    :return: Returns Logger Dictionary with passwords\r\n    \"\"\"\r\n    range_name = \"Logger Passwords\"\r\n    logger_passwords_dict = {}\r\n\r\n    result = gSheetReader.getServiceRead(range_name, sheet_id, service)\r\n\r\n    row_result = result['valueRanges'][0]['values']\r\n\r\n    for index, row in enumerate(row_result):\r\n        if index == 0:\r\n            continue\r\n        logger_id = row[0]\r\n        logger_id = logger_id.replace(' ', '')\r\n        logger_password = row[1]\r\n        logger_password = logger_password.replace(' ', '')\r\n        logger_id = logger_id.lower()\r\n\r\n        logger_passwords_dict[logger_id] = logger_password\r\n\r\n    # print(loggerDict)\r\n    return logger_passwords_dict\r\n\r\ndef find_password(logger_id: str, logger_dict: dict) -> str:\r\n    \"\"\"\r\n    Find password for a logger in the given logger dictionary\r\n    :param logger_id: Logger ID\r\n    :param logger_dict: Logger Dictionary\r\n    :return: Logger Password\r\n    \"\"\"\r\n    return logger_dict.get(logger_id)\r\n\r\ndef check_if_logger_has_been_added_to_field_prev(logger_id: str, field_name: str, grower_name: str) -> bool:\r\n    \"\"\"\r\n    Checks if logger has been added to a previous field\r\n    :param logger_id: Logger ID\r\n    :param field_name: Field Name\r\n    :param grower_name: Grower Name\r\n    :return: True if logger has been added to a field previously, else False\r\n    \"\"\"\r\n    growers = Decagon.open_pickle()\r\n    for grower in growers:\r\n        if grower.name == grower_name:\r\n            for field in grower.fields:\r\n                if field.name == field_name:\r\n                    for logger in field.loggers:\r\n                        if logger.id == logger_id:\r\n                            return True\r\n    return False\r\n\r\n\r\n\r\ndef add_logger_to_field(row, result, logger_num: str, logger_dict: dict, logger_list: list, grower_name: str, field_list: list, field_name_pickle: str, tab_id, field_name:str,\r\n                        additional_stations:bool):\r\n    \"\"\"\r\n\r\n    :param row:\r\n    :param result:\r\n    :param logger_num: The index of the logger\r\n    :param logger_dict: Dictionary of all known loggers and their passwords\r\n    :param logger_list: List of loggers\r\n    :param grower_name: Grower Name\r\n    :param field_list: List of fields\r\n    :param field_name_pickle: Field Name in Pickle File\r\n    :param tab_id:\r\n    :param field_name: Field Name\r\n    :param additional_stations: Boolean to know whether we are adding additional stations to a previous field\r\n    :return: None, only used if an error occurred and need to break out.\r\n    \"\"\"\r\n    # Assign Headers\r\n    crop_type_header = gSheetReader.getColumnHeader(\"Crop\", result)\r\n    planting_header = gSheetReader.getColumnHeader(\"Planting Date\", result)\r\n    field_type_header = gSheetReader.getColumnHeader(\"Field type\", result)\r\n    region_header = gSheetReader.getColumnHeader(\"North or South\", result)\r\n    logger_id_name_header = gSheetReader.getColumnHeader(\"Logger ID Logger \" + logger_num, result)\r\n    logger_name_header = gSheetReader.getColumnHeader(\"Logger Name Logger \" + logger_num, result)\r\n    soil_type_header = gSheetReader.getColumnHeader(\"Soil Type Logger \" + logger_num, result)\r\n    gpm_header = gSheetReader.getColumnHeader(\"Gallons Per Minute Logger \" + logger_num, result)\r\n    acres_header = gSheetReader.getColumnHeader(\"Acres Logger \" + logger_num, result)\r\n    lat_name_header = gSheetReader.getColumnHeader(\"Lat Logger \" + logger_num, result)\r\n    long_name_header = gSheetReader.getColumnHeader(\"Long Logger \" + logger_num, result)\r\n    total_acres_header = gSheetReader.getColumnHeader(\"Total Field Acres\", result)\r\n    install_header = gSheetReader.getColumnHeader(\"Install Date\", result)\r\n\r\n    # Assign Values\r\n    logger_id = row[logger_id_name_header]\r\n    logger_name = row[logger_name_header]\r\n    if logger_id == \"\" and logger_name == \"\":\r\n        return\r\n    elif logger_id == \"\" and logger_name:\r\n        print(\"Logger ID is blank for Logger: \" + logger_name)\r\n\r\n    # Removing blanks form Logger ID and Logger Name\r\n    logger_id = logger_id.replace(' ', '')\r\n    logger_name = logger_name.replace(' ', '')\r\n\r\n    # Should always add logger as long as it hasn't been added to the field previously\r\n    # This prevents a bug where logger gets added multiple times to the same field due to additional stations flag\r\n    should_add_logger = True\r\n    if additional_stations:\r\n        should_add_logger = not check_if_logger_has_been_added_to_field_prev(logger_id, field_name, grower_name)\r\n    soil_type = row[soil_type_header]\r\n    gpm = float(row[gpm_header])\r\n    acres = float(row[acres_header])\r\n    total_acres = float(row[total_acres_header])\r\n    planting_date = row[planting_header]\r\n    crop_type = row[crop_type_header]\r\n    field_type = row[field_type_header]\r\n    region = row[region_header]\r\n    install_date = row[install_header]\r\n    install_date_converted = datetime.strptime(install_date, '%m/%d/%Y').date()\r\n    logger_direction = logger_name.split('-')[-1]\r\n    if field_type == \"R&D\":\r\n        rnd = True\r\n    else:\r\n        rnd = False\r\n    lat = row[lat_name_header]\r\n    long = row[long_name_header]\r\n    field_id = \"\"\r\n    # Remove trailing blanks from field name\r\n    if field_name_pickle[-1] == ' ':\r\n        field_name_pickle = field_name_pickle.rstrip()\r\n\r\n    if (lat or long) == \"\":\r\n        print(\"Lat or Long is empty\")\r\n        return\r\n\r\n    planting_date_converted = datetime.strptime(planting_date, '%m/%d/%Y')\r\n\r\n    # Check that planting date is not from a previous year if dealing with tomatoes\r\n    if planting_date_converted.year < datetime.today().year and (crop_type == 'Tomatoes' or crop_type == 'tomatoes'\r\n                                                                 or crop_type == 'tomato' or crop_type == 'Tomato'):\r\n        print(f\"Error with planting date: Planting Date is from a previous year\")\r\n        planting_date_converted = planting_date_converted.replace(year=datetime.today().year)\r\n        planting_date = planting_date_converted.strftime('%m/%d/%Y')\r\n        print(f\"Planting Date Fixed: {planting_date}\")\r\n\r\n    # Check to see if field has been set up previously\r\n    if field_name_pickle not in field_list and logger_id not in logger_list and should_add_logger:\r\n        logger_id = logger_id.replace(' ', '')\r\n        logger_name = logger_name.replace(' ', '')\r\n        logger_password = find_password(logger_id, logger_dict)\r\n\r\n        # If logger password has been found, set up a new field and loggers\r\n        if logger_password:\r\n            print(\"Adding New \" + grower_name + \" Field: \" + field_name)\r\n            try:\r\n                # Get closest cimis station\r\n                print(\"\\tAttempting to get closest cimis station\")\r\n                cimis = CIMIS()\r\n                cimis_station = CimisStation()\r\n                inactive_cimis_station_list = cimis_station.return_inactive_cimis_stations_list()\r\n                cimis_stations_data = cimis.get_list_of_active_eto_stations()\r\n                closest_cimis_station = cimis.get_closest_station(cimis_stations_data, float(lat), float(long),\r\n                                                                        inactive_cimis_station_list)\r\n\r\n                if closest_cimis_station is None:\r\n                    print(\r\n                        \"ERROR with return from cimis.get_closest_cimis_station() - did not get closest cimis station\")\r\n                    return\r\n            except Exception as error:\r\n                print(f'\\tERROR in loggerSetups add_logger_to_field() when trying to get closest cimis station')\r\n                print(error)\r\n                print(\"\\t\\tLat: \" + lat)\r\n                print(\"\\t\\tLong: \" + long)\r\n                return\r\n\r\n            # Setup Field and Logger to Pickle\r\n            field = Decagon.setup_field(field_name_pickle, lat, long, closest_cimis_station, total_acres, crop_type,\r\n                                        field_type=field_type)\r\n            logger = Decagon.setup_logger(logger_id, logger_password, logger_name, crop_type, soil_type, gpm, acres,\r\n                                          logger_direction, lat, long,\r\n                                          install_date_converted, planting_date=planting_date, rnd=rnd)\r\n            field.add_logger(logger)\r\n            Decagon.add_field_to_grower(grower_name, field)\r\n            field_list.append(field_name_pickle)\r\n            add_logger_id_to_pickle(logger_id)\r\n            print(\"Added logger: \" + logger_id)\r\n        else:\r\n            print(\"\\tCouldn't find logger ID in password sheet\")\r\n\r\n    # Checks to see if logger has to be added to field, only if field has been set up previously\r\n    elif logger_id not in logger_list and should_add_logger:\r\n        logger_password = find_password(logger_id, logger_dict)\r\n\r\n        # If logger password has been found, set up new logger\r\n        if logger_password:\r\n            logger = Decagon.setup_logger(logger_id, logger_password, logger_name, crop_type, soil_type, gpm, acres, logger_direction, lat, long,\r\n                                          install_date_converted, planting_date=planting_date, rnd=rnd)\r\n            growers = Decagon.open_pickle()\r\n            print(\"\\tLogger is not in List\")\r\n            for grower in growers:\r\n                for field in grower.fields:\r\n                    if field.name == field_name_pickle and grower.name == grower_name:\r\n                        field.add_logger(logger)\r\n                        for logger in field.loggers:\r\n                            if logger.grower == None:\r\n                                logger.grower = grower\r\n                            if logger.field == None:\r\n                                logger.field = field\r\n                        print(\"\\t\\tAdded logger: \" + logger_id)\r\n                        add_logger_id_to_pickle(logger_id)\r\n            Decagon.write_pickle(growers)\r\n        else:\r\n            print(\"\\t\\tPassword does not match logger ID\")\r\n\r\n\r\ndef loop_through_loggers(field_name_pickle: str, grower_name: str, field_name: str, field_list: list, logger_list: list, row, result, logger_dict: dict, num_of_loggers: int, tab_id=\"\",\r\n                         add_stations: bool = False):\r\n    \"\"\"\r\n\r\n    :param field_name_pickle: Field Name in Pickle File\r\n    :param grower_name: Grower Name\r\n    :param field_name: Field Name\r\n    :param field_list: Field List\r\n    :param logger_list: Logger List\r\n    :param row: G Sheet logger row for current logger\r\n    :param result: G Sheet result that contains all logger rows\r\n    :param logger_dict: Dictionary of all known loggers and their passwords\r\n    :param num_of_loggers: Number of loggers in field\r\n    :param tab_id:\r\n    :param add_stations: Boolean to know whether we are adding additional stations to a previous field\r\n    \"\"\"\r\n    for logger_num in range(1, num_of_loggers + 1):\r\n        add_logger_to_field(row, result, str(logger_num), logger_dict, logger_list, grower_name, field_list, field_name_pickle, tab_id, field_name,\r\n                            add_stations)\r\n\r\n\r\ndef logger_setups_process() -> None:\r\n    \"\"\"\r\n    Function that handles all the logger setups process. This function makes sure that any new field submissions\r\n    to logger setups are created and added to the pickle, and that their irrigation scheduling is set up.\r\n\r\n    \"\"\"\r\n    now = datetime.today()\r\n    print(\">>>>>>>>>>>>>>>>>>>LOGGER SETUPS PROCESS 2.0<<<<<<<<<<<<<<<<<<<\")\r\n    print(\"                                 - \" + now.strftime(\"%m/%d/%y  %I:%M %p\"))\r\n    print()\r\n    print()\r\n\r\n    logger_setups_process_start_time = time.time()\r\n\r\n    # Grab our current growers, fields and loggers from the pickle and create lists for all of them separately\r\n    grower_name_list, field_name_list, logger_id_list = setup_growers_fields_loggers_lists()\r\n\r\n    # Setup Google Sheet information\r\n    sheet_tab_name = 'Setup'\r\n\r\n    # Setups service to read google sheets\r\n    g_sheet = GSheetCredentialSevice.GSheetCredentialSevice()\r\n    service = g_sheet.getService()\r\n\r\n    # Set up logger dictionary with passwords\r\n    logger_passwords_dict = set_up_password_dict(service, NEW_LOGGER_SETUPS_SHEET_ID)\r\n\r\n    # Read Google Sheet\r\n    result = gSheetReader.getServiceRead(sheet_tab_name, NEW_LOGGER_SETUPS_SHEET_ID, service)\r\n    row_result = result['valueRanges'][0]['values']\r\n\r\n    # TODO Store all cimis station locations as well as the last updated list so we don't query cimis every time\r\n    # Other classes and variables we will need\r\n    cimis = CIMIS()\r\n    cimis_station = CimisStation()\r\n    inactive_cimis_station_list = []\r\n    cimis_stations_data = None\r\n    number_of_fields_successfully_setup = number_of_fields_failed_setup = 0\r\n    field_names_successfully_setup = []\r\n    field_names_failed_setup = []\r\n\r\n\r\n\r\n    # Loop through each row in the Google sheet and check if the field is in our current pickle. If it isn't,\r\n    #  go through the setup process using that row's data.\r\n    for row_index, row in enumerate(row_result):\r\n        try:\r\n            # Ignore header row\r\n            if row_index == 0:\r\n                continue\r\n\r\n            # Set up variables\r\n            field_name_col = gSheetReader.getColumnHeader(\"Field Name (Grower)\", row_result)\r\n            field_name = row[field_name_col].strip()\r\n\r\n            grower_name_col = gSheetReader.getColumnHeader(\"Grower Name\", row_result)\r\n            grower_name = row[grower_name_col].strip()\r\n\r\n            field_name_pickle = grower_name + field_name\r\n\r\n            if field_name_pickle not in field_name_list:\r\n                # New Field\r\n                print(f'--------------- New Field Found: {field_name_pickle} ---------------')\r\n                # Boolean that starts as True and changes to False if any of the setup process fails\r\n                successful_setup = True\r\n\r\n                # CIMIS Info setup\r\n                # Grab inactive cimis stations and active eto cimis stations from CIMIS API\r\n                print(f'\\tChecking for CIMIS station data...')\r\n                if len(inactive_cimis_station_list) == 0:\r\n                    inactive_cimis_station_list = cimis_station.return_inactive_cimis_stations_list()\r\n\r\n                cimis_api_attempts = 0\r\n                cimis_api_attempt_limit = 5\r\n                while cimis_stations_data is None and cimis_api_attempts <= cimis_api_attempt_limit:\r\n                    print(f'\\t\\t\\t\\t CIMIS API attempt {cimis_api_attempts}')\r\n                    cimis_stations_data = cimis.get_list_of_active_eto_stations()\r\n                    cimis_api_attempts += 1\r\n\r\n                if cimis_stations_data is None:\r\n                    print()\r\n                    print(f'\\t\\tFailed the CIMIS call for stations {cimis_api_attempt_limit} times. Ending logger setups')\r\n                    successful_setup = False\r\n                    return None\r\n                else:\r\n                    print(f'\\t<-CIMIS station data acquired successfully')\r\n                    print()\r\n\r\n                growers = Decagon.open_pickle()\r\n                if grower_name not in grower_name_list:\r\n                    # New Grower\r\n                    print(f'\\tField is from a new Grower: {grower_name}')\r\n                    new_grower = True\r\n\r\n                    technician_name_col = gSheetReader.getColumnHeader(\"Technician\", row_result)\r\n                    technician_name = row[technician_name_col].strip()\r\n                    technician = Decagon.get_technician(technician_name, growers=growers)\r\n\r\n                    region_name_col = gSheetReader.getColumnHeader(\"Region\", row_result)\r\n                    region = row[region_name_col].strip()\r\n\r\n                    print(f'\\t>Creating new Grower: {grower_name}')\r\n                    grower = Grower(grower_name, [], technician, region=region)\r\n\r\n                    grower_name_list.append(grower_name)\r\n                    print(f'\\t<Grower: {grower_name} Created')\r\n                    print()\r\n                else:\r\n                    # Already existing grower\r\n                    new_grower = False\r\n                    print(f'\\tField is from an already existing Grower: {grower_name}')\r\n                    print()\r\n                    for g in growers:\r\n                        if g.name == grower_name:\r\n                            grower = g\r\n\r\n                # Grab pertinent data and setup new field\r\n                new_field = setup_new_field(\r\n                    field_name_pickle,\r\n                    grower,\r\n                    row,\r\n                    row_result,\r\n                    cimis_stations_data,\r\n                    inactive_cimis_station_list\r\n                )\r\n\r\n                if new_field is None:\r\n                    print(f'\\t\\tERROR')\r\n                    print(f'\\t\\tIssue with field setup for: {field_name_pickle}')\r\n                    print(f'\\t\\tSkipping this field')\r\n                    number_of_fields_failed_setup += 1\r\n                    field_names_failed_setup.append(field_name_pickle)\r\n                    successful_setup = False\r\n                    continue\r\n\r\n                num_of_loggers_col = gSheetReader.getColumnHeader(\"Number of Loggers in Field\", row_result)\r\n                num_of_loggers = int(row[num_of_loggers_col].strip())\r\n\r\n                print(f'\\t\\t\\tProcessing loggers in field {field_name_pickle}. {num_of_loggers} logger/s found')\r\n                print()\r\n                for logger_num in range(1, num_of_loggers + 1):\r\n                    # Get Values\r\n                    logger_name_col = gSheetReader.getColumnHeader(f\"Logger Name (L{logger_num})\", row_result)\r\n                    logger_name = row[logger_name_col].strip()\r\n\r\n                    logger_id_col = gSheetReader.getColumnHeader(f\"Logger ID (L{logger_num})\", row_result)\r\n                    logger_id = row[logger_id_col].strip()\r\n\r\n                    print(f'\\t\\t\\tLogger #{logger_num} -> {logger_id} - {logger_name}...')\r\n\r\n                    if logger_id not in logger_id_list:\r\n                        print(f'\\t\\t\\t{logger_id} is a new Logger')\r\n\r\n                        # Grab pertinent data and setup new logger\r\n                        new_logger = setup_new_logger(\r\n                            logger_id,\r\n                            logger_name,\r\n                            logger_num,\r\n                            logger_passwords_dict,\r\n                            grower,\r\n                            new_field,\r\n                            row,\r\n                            row_result\r\n                        )\r\n\r\n                        if new_logger is None:\r\n                            print(f'\\t\\t\\tERROR')\r\n                            print(f'\\t\\t\\tIssue with logger {logger_id} - {logger_name}')\r\n                            print(f'Skipping field setup for {field_name}')\r\n                            successful_setup = False\r\n                            continue\r\n                        new_field.loggers.append(new_logger)\r\n                        logger_id_list.append(logger_id)\r\n                        print(f'\\t\\t\\t<-Done with logger')\r\n                        print()\r\n                    else:\r\n                        print(f'\\t\\t\\t{logger_id} - {logger_name} already exists, skipping')\r\n                        print()\r\n\r\n                if successful_setup is False:\r\n                    print(f'Skipping field setup for {field_name}')\r\n                    number_of_fields_failed_setup += 1\r\n                    field_names_failed_setup.append(field_name_pickle)\r\n                    continue\r\n\r\n                grower.fields.append(new_field)\r\n                field_name_list.append(new_field.name)\r\n                print(f'--------------- Done with: {field_name_pickle} ---------------')\r\n                print()\r\n\r\n                if new_grower:\r\n                    growers.append(grower)\r\n                Decagon.write_pickle(growers)\r\n\r\n                # Setup Irrigation Scheduling\r\n                print(f'--------------- Setting up Irrigation Scheduling: {field_name_pickle} ---------------\\n')\r\n                # Create irr sched table for field\r\n                setup_irr_scheduling(new_field, cimis_stations_data)\r\n                print(f'--------------- Done with: {field_name_pickle} Irrigation Scheduling ---------------')\r\n                print()\r\n\r\n                number_of_fields_successfully_setup += 1\r\n                field_names_successfully_setup.append(field_name_pickle)\r\n\r\n        except IndexError as err:\r\n            print(err)\r\n            continue\r\n        except Exception as err:\r\n            print(err)\r\n            continue\r\n\r\n    logger_setups_process_start_end_time = time.time()\r\n    print(\"----------FINISHED----------\")\r\n    print()\r\n    print('Done with Logger Setups Process')\r\n    print(f'{number_of_fields_successfully_setup} Fields Successfully Set Up')\r\n    print(f'   -> {field_names_successfully_setup}')\r\n    print(f'{number_of_fields_failed_setup} Fields Failed')\r\n    print(f'   -> {field_names_failed_setup}')\r\n    print()\r\n\r\n    logger_setups_process_elapsed_time_seconds = logger_setups_process_start_end_time - logger_setups_process_start_time\r\n\r\n    logger_setups_process_elapsed_time_hours = int(logger_setups_process_elapsed_time_seconds // 3600)\r\n    logger_setups_process_elapsed_time_minutes = int((logger_setups_process_elapsed_time_seconds % 3600) // 60)\r\n    logger_setups_process_elapsed_time_seconds = int(logger_setups_process_elapsed_time_seconds % 60)\r\n\r\n    print(f\"Logger Setups Process execution time: {logger_setups_process_elapsed_time_hours}:\"\r\n          + f\"{logger_setups_process_elapsed_time_minutes}:\"\r\n          + f\"{logger_setups_process_elapsed_time_seconds} (hours:minutes:seconds)\")\r\n    print()\r\n    print()\r\n\r\n\r\ndef setup_new_logger(logger_id, logger_name, logger_num, logger_passwords_dict, grower, new_field,\r\n                     row, row_result):\r\n    # New Logger\r\n    print(f'\\t\\t\\tLooking for password for logger id: {logger_id}...')\r\n    logger_password = logger_passwords_dict.get(logger_id)\r\n    if logger_password is None:\r\n        print(f'\\t\\t\\tERROR')\r\n        print(f'\\t\\t\\t<-Password for logger {logger_id} not found')\r\n        print(f'\\t\\t\\t<-Skipping this logger so setup might be incomplete')\r\n        return None\r\n    else:\r\n        print(f'\\t\\t\\t<-Password found: {logger_password}')\r\n\r\n    # Grab pertinent variables\r\n    lat_col = gSheetReader.getColumnHeader(f\"Lat (L{logger_num})\", row_result)\r\n    lat = float(row[lat_col].strip())\r\n\r\n    long_col = gSheetReader.getColumnHeader(f\"Long (L{logger_num})\", row_result)\r\n    long = float(row[long_col].strip())\r\n\r\n    gpm_col = gSheetReader.getColumnHeader(f\"Irr. Gallons Per Minute (L{logger_num})\", row_result)\r\n    gpm = float(row[gpm_col].strip())\r\n\r\n    irrigation_set_acres_col = gSheetReader.getColumnHeader(f\"Irrigation Acres (L{logger_num})\", row_result)\r\n    irrigation_set_acres = float(row[irrigation_set_acres_col].strip())\r\n\r\n    soil_type_col = gSheetReader.getColumnHeader(f\"Soil Type (L{logger_num})\", row_result)\r\n    soil_type = row[soil_type_col].strip()\r\n\r\n    if soil_type == '':\r\n        soil_type = get_soil_type_from_coords(lat, long)\r\n\r\n    planting_date_col = gSheetReader.getColumnHeader(\"Planting Date\", row_result)\r\n    planting_date = row[planting_date_col].strip()\r\n    planting_date_converted = datetime.strptime(planting_date, '%Y-%m-%d').date()\r\n\r\n    install_date_col = gSheetReader.getColumnHeader(\"Install Date\", row_result)\r\n    install_date = row[install_date_col].strip()\r\n    install_date_converted = datetime.strptime(install_date, '%Y-%m-%d').date()\r\n\r\n    logger_direction = logger_name.split('-')[-1]\r\n    crop_type = new_field.crop_type\r\n\r\n    print(f'\\t\\t\\tCreating Logger: {logger_name} - {logger_id}...')\r\n    new_logger = Logger(\r\n        logger_id,\r\n        logger_password,\r\n        logger_name,\r\n        crop_type,\r\n        soil_type,\r\n        gpm,\r\n        irrigation_set_acres,\r\n        logger_direction,\r\n        install_date_converted,\r\n        lat,\r\n        long,\r\n        grower=grower,\r\n        field=new_field,\r\n        planting_date=planting_date_converted\r\n    )\r\n\r\n    print(f'\\t\\t\\t<-Logger: {logger_name} Created')\r\n    return new_logger\r\n\r\n\r\ndef setup_new_field(field_name, grower, row, row_result, cimis_stations_data, inactive_cimis_station_list):\r\n    \"\"\"\r\n\r\n    :param field_name:\r\n    :param grower:\r\n    :param row:\r\n    :param row_result:\r\n    :return:\r\n    \"\"\"\r\n    print(f'\\t\\tSetting up new field: {field_name}...')\r\n\r\n    field_type_col = gSheetReader.getColumnHeader(\"Field Type\", row_result)\r\n    field_type = row[field_type_col].strip()\r\n\r\n    region_col = gSheetReader.getColumnHeader(\"Region\", row_result)\r\n    region = row[region_col].strip()\r\n\r\n    field_acres_col = gSheetReader.getColumnHeader(\"Total Field Acres\", row_result)\r\n    field_acres = float(row[field_acres_col].strip())\r\n\r\n    crop_type_col = gSheetReader.getColumnHeader(\"Crop\", row_result)\r\n    crop_type = row[crop_type_col].strip()\r\n\r\n    # Lat long are from the first logger in the field\r\n    lat_col = gSheetReader.getColumnHeader(\"Lat (L1)\", row_result)\r\n    lat = float(row[lat_col].strip())\r\n\r\n    long_col = gSheetReader.getColumnHeader(\"Long (L1)\", row_result)\r\n    long = float(row[long_col].strip())\r\n\r\n    print(f'\\t\\tGrabbing closest cimis station...')\r\n    cimis = CIMIS()\r\n    closest_cimis_station = cimis.get_closest_station(cimis_stations_data, float(lat), float(long),\r\n                                                      inactive_cimis_station_list)\r\n    print(f'\\t\\t<-Closest cimis station found {closest_cimis_station}')\r\n\r\n    print(f'\\t\\tCreating new Field...')\r\n    new_field = Field(\r\n        field_name,\r\n        [],\r\n        lat,\r\n        long,\r\n        closest_cimis_station,\r\n        field_acres,\r\n        crop_type,\r\n        grower=grower,\r\n        field_type=field_type\r\n    )\r\n\r\n    print(f'\\t\\t<-Field: {field_name} Created')\r\n    print()\r\n    return new_field\r\n\r\n\r\ndef setup_field():\r\n    \"\"\"\r\n    Starts Logger Setup Process to setup loggers using the logger setup form Google sheet\r\n    \"\"\"\r\n\r\n    # Logger Setups Google Sheet ID\r\n    sheet_id = '1rsPrX44pCHOhbOHZfWubDkuuexP8pSG8kspW03FAqOU'\r\n    logger_dict = {}\r\n\r\n    # Logger Setups Tab Name\r\n    range_name = \"S-Logger Info\"\r\n\r\n    # Setups service to read google sheets\r\n    g_sheet = GSheetCredentialSevice.GSheetCredentialSevice()\r\n    service = g_sheet.getService()\r\n\r\n    # Set up logger dictionary with passwords\r\n    logger_dict = set_up_password_dict(service, sheet_id)\r\n\r\n    # Read Google Sheet\r\n    result = gSheetReader.getServiceRead(range_name, sheet_id, service)\r\n    row_result = result['valueRanges'][0]['values']\r\n\r\n    # Assign indexes to columns\r\n    grower_header = gSheetReader.getColumnHeader(\"Grower Name\", row_result)\r\n    field_name_header = gSheetReader.getColumnHeader(\"Grower Field\", row_result)\r\n    region_header = gSheetReader.getColumnHeader(\"North or South\", row_result)\r\n    technician_header = gSheetReader.getColumnHeader(\"Technician\", row_result)\r\n    email_header = gSheetReader.getColumnHeader(\"Grower Email\", row_result)\r\n    num_of_loggers_header = gSheetReader.getColumnHeader(\"Total Number of Loggers in Field\", row_result)\r\n    adding_additional_stations_header = gSheetReader.getColumnHeader(\"Are you adding stations to a previous field?\",\r\n                                                                     row_result)\r\n    planting_header = gSheetReader.getColumnHeader(\"Planting Date\", row_result)\r\n    install_header = gSheetReader.getColumnHeader(\"Install Date\", row_result)\r\n\r\n    # Setup grower, field, and logger lists\r\n    grower_list, field_list, logger_list = setup_growers_fields_loggers_lists()\r\n\r\n    # Loop through each row in the Google sheet and set it up if it hasn't been set up before\r\n    for row_index, row in enumerate(row_result):\r\n        try:\r\n            # Ignore header row and empty rows\r\n            if row_index == 0:\r\n                continue\r\n            elif row[grower_header] == \"\":\r\n                continue\r\n\r\n            # Set up variables\r\n            grower_name = row[grower_header]\r\n            field_name = row[field_name_header].rstrip()\r\n            field_name_pickle = grower_name + field_name\r\n            region = row[region_header]\r\n            email = row[email_header]\r\n            technician = row[technician_header]\r\n            num_of_loggers = int(row[num_of_loggers_header])\r\n            additional_stations = row[adding_additional_stations_header]\r\n            planting_date = row[planting_header]\r\n            install_date = row[install_header]\r\n            planting_date_converted = datetime.strptime(planting_date, '%m/%d/%Y')\r\n            install_date_converted = datetime.strptime(install_date, '%m/%d/%Y')\r\n\r\n            # Check to make sure technician entered planting date correctly\r\n            if planting_date_converted > install_date_converted:\r\n                # Send Notification that field was setup incorrectly. Planting date is after install date\r\n                print(f\"Error Planting Date is after Install Date For Field: {field_name}\")\r\n                print(f\"\\tPlanting Date: {planting_date}; Install Date: {install_date}\")\r\n                growers = Decagon.open_pickle()\r\n                for grower in growers:\r\n                    if grower.name == grower_name:\r\n                        grower.technician.all_notifications.add_notification(\r\n                            Notification_LoggerSetups(\r\n                                datetime.now(),\r\n                                grower_name,\r\n                                field_name,\r\n                                issue=\"Planting Date is after Install Date\"\r\n                            )\r\n                        )\r\n                Decagon.write_pickle(growers)\r\n                continue\r\n\r\n            # Check to see if field has already been added before\r\n            if field_name_pickle not in field_list or additional_stations == 'Yes':\r\n                # If field has been added and this is an additional station, assign additional station flag\r\n                if additional_stations == 'Yes':\r\n                    print('Attempting to add additional stations')\r\n                    additional_stations_boolean = True\r\n\r\n                else:\r\n                    print(\"Field is not in pickle, setting up new field: \" + field_name_pickle)\r\n                    print(\"Checking to see if grower is in list\")\r\n                    grower_list = check_if_grower_in_list(grower_name, grower_list, technician, region=region)\r\n                    additional_stations_boolean = False\r\n\r\n                # Loop through each logger that is in the form for that specific field\r\n                loop_through_loggers(field_name_pickle, grower_name, field_name, field_list, logger_list, row, row_result, logger_dict,\r\n                                     num_of_loggers, add_stations=additional_stations_boolean)\r\n\r\n                # Sets up nickname for field\r\n                setup_nickname(field_name_pickle)\r\n\r\n                # Setup Irrigation Scheduling\r\n                if additional_stations == 'No':\r\n                    growers = Decagon.open_pickle()\r\n                    for grower in growers:\r\n                        for field in grower.fields:\r\n                            if field.name == field_name_pickle:\r\n                                # Create dataset for field\r\n                                setup_dataset(field)\r\n\r\n                                # Create irr sched table for field\r\n                                # Changed function def to pass in cimis station data so this may break if ever used again\r\n                                setup_irr_scheduling(field, None)\r\n\r\n        except IndexError:\r\n            continue\r\n        except Exception as err:\r\n            print(err)\r\n            continue\r\n\r\n\r\ndef sort_best_station(stations_info: dict) -> str:\r\n    \"\"\"\r\n    Function sorts through stations info dict to find the best station with the most valid data\r\n    :param stations_info: A dictionary of stations and their historical data for previous years\r\n    :return best_station: The station number of most valid station\r\n    \"\"\"\r\n    print(f'Looking for station with most valid data out of tried stations')\r\n\r\n    # check if 30 null points not 30 valid points\r\n    valid_points = 365 - 30\r\n    station_years = {}  # 'station_number' : valid_years\r\n    search_string = 'ET'\r\n\r\n    # Check if the dict is empty meaning we never found even invalid stations - extreme edge case\r\n    if not stations_info:\r\n        print('\\tNo stations to sort through, we cannot make an Irrigation Scheduling Page')\r\n        return None\r\n\r\n    for station in stations_info:\r\n        station_years[station] = 0\r\n        filtered_keys = {key: value for key, value in stations_info[station].items() if search_string in key}\r\n        if len(filtered_keys) > 0:\r\n            # Find the amount of valid years per station\r\n            for year in filtered_keys.values():\r\n                # If for this year there are less than valid_points, return Invalid\r\n                valid = sum(1 for value in year if value is not None)\r\n                if valid >= valid_points:\r\n                    station_years[station] += 1\r\n\r\n    best_station = max(station_years, key=station_years.get)\r\n    if station_years[best_station] < 2:\r\n        print('\\tThe best station has less than 2 years of valid data, we cannot make an Irrigation Scheduling Page')\r\n        return None\r\n    print(f'\\tFound the best station of the already tried: {best_station}')\r\n    return best_station\r\n\r\n\r\ndef setup_irr_scheduling(field, cimis_stations_data):\r\n    \"\"\"\r\n    Create the irrigation scheduling table for the field\r\n    :param field: Field object\r\n    :return:\r\n    \"\"\"\r\n    # print(f'\\tSetting up irrigation scheduling table for {field.name}')\r\n\r\n    dbwriter = DBWriter()\r\n    dataset = \"Historical_ET\"\r\n    station_number = field.cimis_station\r\n\r\n    # Does historical et table exist for this station\r\n    print(f'\\tChecking for existing Historical ET Table for station {station_number}....')\r\n    if not dbwriter.check_if_table_exists(dataset, station_number, project='stomato-info'):\r\n        print()\r\n        # Get stations historical data to make hist et table\r\n\r\n        cimis = CIMIS()\r\n        cached_stations = OrderedDict()\r\n\r\n        # Check if the current station has valid data we can use for historical setup\r\n        station_is_valid, station_results = cimis.new_et_station_data(station_number)\r\n        # cache results for later sorting if necessary\r\n        cached_stations[station_number] = station_results\r\n        if station_is_valid:\r\n            # new ET station we havent used\r\n            Decagon.write_new_historical_et_to_db_2(dataset, station_number, station_results, overwrite=True)\r\n\r\n        # Try all in county stations\r\n        else:\r\n            cimis_station = CimisStation()\r\n            stations_to_skip = cimis_station.return_inactive_cimis_stations_list()\r\n            # active_stations = cimis.get_list_of_active_eto_stations()\r\n\r\n            handle_coastal_list(cimis_stations_data, station_number, stations_to_skip)\r\n\r\n            stations_to_skip.append(station_number)\r\n            station_number = cimis.get_closest_station_in_county(station_number, stations_to_skip, cached_stations, cimis_stations_data)\r\n\r\n            # Found a good county station that's valid\r\n            if station_number is not None:\r\n                Decagon.write_new_historical_et_to_db_2(dataset, station_number, cached_stations[station_number],\r\n                                                        overwrite=True)\r\n            # Tried all in the county now look for closest\r\n            else:\r\n                print('\\tChecking Closest in Range Stations....')\r\n                station_number = cimis.get_closest_valid_station(float(field.lat), float(field.long), stations_to_skip, cached_stations, cimis_stations_data)\r\n                if station_number is not None:\r\n                    Decagon.write_new_historical_et_to_db_2(dataset, station_number, cached_stations[station_number],\r\n                                                            overwrite=True)\r\n                else:\r\n                    print('\\tNo valid stations found in county or range')\r\n                    print('\\t\\tSorting for most Valid CIMIS Station...')\r\n                    station_number = sort_best_station(cached_stations)\r\n                    if station_number is None:\r\n                        return\r\n                    else:\r\n                        Decagon.write_new_historical_et_to_db_2(dataset, station_number,\r\n                                                                cached_stations[station_number],\r\n                                                                overwrite=True)\r\n\r\n    print(\"\\tSetting up irrigation scheduling DB table...\")\r\n    setup_dataset(field)\r\n    SQLScripts.setup_irrigation_scheduling_db(station_number, field.name)\r\n\r\n\r\ndef handle_coastal_list(active_stations, station_number, stations_to_skip):\r\n    # Grabbing all active stations to filter out all coastals not just ones in county,\r\n    # because this skippable list will be used later in the closest range func\r\n    stations_list = [str(station['StationNbr']) for station in active_stations]\r\n    # NOTE: coastals list was aggregated by visually looking at stations\r\n    # on the coast and manually adding, may need annual revision\r\n    coastals = ['187', '157', '254', '213', '253', '171', '178', '104', '209', '129', '116', '193', '193', '210',\r\n                '229', '160', '52', '202', '231', '64', '107', '152', '99', '174', '75', '245', '241', '173', '150',\r\n                '184', '147']\r\n    noncoastals = list(set(stations_list) - set(coastals))\r\n    station_is_coastal = station_number in coastals\r\n    if station_is_coastal:\r\n        stations_to_skip.append(noncoastals)\r\n    else:\r\n        stations_to_skip.append(coastals)\r\n\r\n\r\ndef check_for_missing_report_or_preview_in_pickle():\r\n    \"\"\"\r\n    Sets up missing report preview links for the fields in the pickle\r\n    \"\"\"\r\n\r\n    # Set up a list of missing report and preview fields\r\n    missing_report_preview_list = setup_missing_report_preview_list()\r\n\r\n    # G Sheet ID and tab\r\n    # OLD SHEET ID\r\n    # sheet_id = '1rsPrX44pCHOhbOHZfWubDkuuexP8pSG8kspW03FAqOU'\r\n\r\n    # NEW SHEET ID\r\n    sheet_id = '1ycEimLNLXpg7rZ5QxAwTfnAO1BkNBIwOzfpM1dqAxdg'\r\n    range_name = \"Setup\"\r\n\r\n    g_sheet = GSheetCredentialSevice.GSheetCredentialSevice()\r\n    service = g_sheet.getService()\r\n\r\n    # Grab Sheet Values form S-Logger Info\r\n    result = gSheetReader.getServiceRead(range_name, sheet_id, service)\r\n    row_result = result['valueRanges'][0]['values']\r\n\r\n    # Setup variables\r\n    field_name_header = gSheetReader.getColumnHeader(\"Field Name (Grower)\", row_result)\r\n    grower_header = gSheetReader.getColumnHeader(\"Grower Name\", row_result)\r\n    field_setup_done_header = gSheetReader.getColumnHeader(\"Field Setup Done\", row_result)\r\n    report_header = gSheetReader.getColumnHeader(\"Report\", row_result)\r\n    preview_header = gSheetReader.getColumnHeader(\"Preview\", row_result)\r\n\r\n    # Loop through Sheet values by row\r\n    for ind, row in enumerate(row_result):\r\n        try:\r\n            if ind == 0:\r\n                continue\r\n            elif row[grower_header] == \"\":\r\n                continue\r\n            grower_name = row[grower_header].strip()\r\n            field_name = row[field_name_header].strip()\r\n            field_name_pickle = grower_name + field_name\r\n            field_setup_done = row[field_setup_done_header]\r\n            if field_setup_done == \"TRUE\":\r\n                field_setup_done = True\r\n            else:\r\n                field_setup_done = False\r\n\r\n            field_name_pickle_cleaned_up = field_name_pickle.strip()\r\n\r\n            # Check to see if field has missing report or preview and has been flagged as done\r\n            if field_name_pickle_cleaned_up in missing_report_preview_list and field_setup_done:\r\n                report = row[report_header]\r\n                preview = row[preview_header]\r\n                # Update the field in the pickle with the correct report and preview values\r\n                update_report_and_image_in_pickle(field_name_pickle_cleaned_up, report, preview)\r\n\r\n                # Send notification that field is finished and ready to be shown to the grower\r\n                growers = Decagon.open_pickle()\r\n                for grower in growers:\r\n                    if grower.name == grower_name:\r\n                        grower.technician.all_notifications.add_notification(\r\n                            Notification_LoggerSetups(\r\n                                datetime.now(),\r\n                                grower_name,\r\n                                field_name,\r\n                                page_link=report\r\n                            )\r\n                        )\r\n\r\n                Decagon.write_pickle(growers)\r\n        except IndexError:\r\n            continue\r\n        except Exception as err:\r\n            print(err)\r\n            continue\r\n\r\n\r\ndef notification_setup():\r\n    \"\"\"\r\n\r\n    \"\"\"\r\n    growers = Decagon.open_pickle()\r\n    print('Notification Setup')\r\n    all_technicians = Decagon.get_all_technicians(growers)\r\n    Decagon.reset_notifications(all_technicians)\r\n    Decagon.notifications_setup(growers, all_technicians, file_type='html')\r\n    Decagon.write_pickle(growers)\r\n\r\n\r\ndef setup_missing_report_preview_list() -> list:\r\n    \"\"\"\r\n    Sets up list of fields that has the default report value\r\n    :return: List of fields that has the default report value\r\n    \"\"\"\r\n    missing_report_preview_list = []\r\n    growers = Decagon.open_pickle()\r\n    for grower in growers:\r\n        for field in grower.fields:\r\n            # If report is the default link then add to missing report and preview list\r\n            if field.report_url == 'https://i.imgur.com/04UdmBH.png' and field.active:\r\n                print('Found field without report url:', field.name)\r\n                missing_report_preview_list.append(field.name)\r\n    return missing_report_preview_list\r\n\r\n\r\ndef setup_nickname(field_name: str):\r\n    \"\"\"\r\n    Setups up field nickname as field name without grower name\r\n    :param field_name: Pickle Field Name\r\n    \"\"\"\r\n    growers = Decagon.open_pickle()\r\n    for grower in growers:\r\n        for field in grower.fields:\r\n            if field.name == field_name:\r\n                field.nickname = field.name.split(grower.name)[-1]\r\n    Decagon.write_pickle(growers)\r\n\r\n\r\ndef update_field(grower_name: str, field_name: str, only_one_logger: bool = False, logger_name: str = \"\", subtract_mrid: int = 0, rerun: bool = False):\r\n    \"\"\"\r\n    Updates either the whole field or only a specific logger for the field\r\n    and resets previous day switch and removes duplicate data and updates ET at once\r\n    :param grower_name: Grower Name\r\n    :param field_name: Field Name\r\n    :param only_one_logger: Are you updating only one logger or the whole field?\r\n    :param logger_name: Logger Name\r\n    :param subtract_mrid: Subtract MRID value in case you want to go back days\r\n    :param rerun: Is this field being rerun again after the Default run?\r\n    \"\"\"\r\n    growers = Decagon.open_pickle()\r\n    # Set previous day switch to 0 for loggers that will be updated\r\n    for grower in growers:\r\n        if grower.name == grower_name:\r\n            for field in grower.fields:\r\n                if field.name == field_name:\r\n                    if only_one_logger:\r\n                        for logger in field.loggers:\r\n                            if logger_name == logger.name:\r\n                                if subtract_mrid > 0:\r\n                                    print(\"Setting previous day switch to zero\")\r\n                                    logger.prev_day_switch = 0\r\n                                    logger.crashed = False\r\n                                logger.updated = False\r\n                                Decagon.write_pickle(growers)\r\n                    else:\r\n                        for logger in field.loggers:\r\n                            logger.updated = False\r\n                            # If field is a rerun, need to clear previous day switch value to not mess up any summed totals\r\n                            if rerun:\r\n                                logger.prev_day_switch = 0\r\n                            # print(l.updated)\r\n                            Decagon.write_pickle(growers)\r\n\r\n    # Update field for one logger, and then delete duplicate date and update the logger ET value in the DB\r\n    if only_one_logger:\r\n        Decagon.only_certain_growers_field_logger_update(grower_name, field_name, logger_name,\r\n                                                         write_to_db=True, subtract_from_mrid=subtract_mrid)\r\n        for grower in growers:\r\n            if grower.name == grower_name:\r\n                for field in grower.fields:\r\n                    if field.name == field_name:\r\n                        for logger in field.loggers:\r\n                            if logger.name == logger_name:\r\n                                SQLScripts.remove_duplicate_data(logger)\r\n        SQLScripts.update_logger_et(field_name, logger_name)\r\n    else:\r\n    # Update all loggers in the field\r\n        Decagon.only_certain_growers_field_update(grower_name, field_name, get_et=False, get_weather=True, get_data=True,\r\n                                                  write_to_db=True)\r\n        for grower in growers:\r\n            if grower.name == grower_name:\r\n                for field in grower.fields:\r\n                    if field.name == field_name:\r\n                        for logger in field.loggers:\r\n                            SQLScripts.remove_duplicate_data(logger)\r\n        SQLScripts.update_field_et(field_name)\r\n\r\n\r\ndef col_to_letter(col):\r\n    \"\"\"Gets the letter of a column number, only works for letters A-Z\"\"\"\r\n    r = ''\r\n    v = col % 26\r\n    r = chr(v + 65)\r\n    return r\r\n\r\n\r\ndef setup_dataset(field):\r\n    \"\"\"\r\n    Function to create a dataset for fields, if one already exists do nothing\r\n    :param field:\r\n    \"\"\"\r\n    db = DBWriter()\r\n    field_name = db.remove_unwanted_chars_for_db_dataset(field.name)\r\n    project = db.get_db_project(field.loggers[-1].crop_type)\r\n    dataset = db.check_if_dataset_exists(field_name, project)\r\n    if not dataset:\r\n        db.create_dataset(field_name, project)\r\n\r\n\r\ndef update_report_and_image_in_pickle(field_name_pickle, report, portal_image):\r\n    \"\"\"\r\n\r\n    :param field_name_pickle:\r\n    :param report:\r\n    :param portal_image:\r\n    \"\"\"\r\n    growers = Decagon.open_pickle()\r\n    for grower in growers:\r\n        for field in grower.fields:\r\n            if field.name == field_name_pickle:\r\n                field.report_url = report\r\n                field.preview_url = portal_image\r\n                print('Updated ', field_name_pickle)\r\n                print('\\tReport URL', report)\r\n                print('\\tPortal Image', portal_image)\r\n                print()\r\n    Decagon.write_pickle(growers)\r\n\r\n\r\ndef update_missing_data_yesterday():\r\n    \"\"\"\r\n    Updates any fields with missing data for yesterday\r\n    \"\"\"\r\n    dbwriter = DBWriter.DBWriter()\r\n    growers = Decagon.open_pickle()\r\n    today = datetime.today()\r\n    yesterday = today - timedelta(days=1)\r\n    # Loop through pickle checking active fields to see if they updated yesterday\r\n    for grower in growers:\r\n        for field in grower.fields:\r\n            if field.active:\r\n                field_name = dbwriter.remove_unwanted_chars_for_db_dataset(field.name)\r\n                print(\"Working on Field: \", field.name)\r\n                for logger in field.loggers:\r\n                    print('\\t Working on Logger: ', logger.name)\r\n                    updated = False\r\n                    # Check to see if data updated yesterday for the logger\r\n                    print(\"\\t\\tChecking Date to see if yesterday updated\")\r\n                    project = dbwriter.get_db_project(logger.crop_type)\r\n                    dataset_id = project + \".\" + field_name + \".\" + logger.name\r\n                    dml_statement = \"select date from`\" + dataset_id + \"` order by date\"\r\n                    result = dbwriter.run_dml(dml_statement, project=project)\r\n                    for r in result:\r\n                        if str(r[0]) == str(yesterday.date()):\r\n                            # print(\"Field Updated\")\r\n                            updated = True\r\n                    # If the data wasn't updated for the logger, then update the logger\r\n                    if not updated:\r\n                        print(\"\\t\\t\\tField did not update: \", field.name, \"\\n Logger: \", logger.name)\r\n                        print(\"\\t\\t\\tUpdating Field\")\r\n                        update_field(grower.name, field.name, True, logger.name, subtract_mrid=24)\r\n\r\n\r\ndef return_active_fields(region:str=\"Both\")->list:\r\n    \"\"\"\r\n    Return active fields for a specific region or both regions\r\n    :param region: Default value is \"Both\", but can be \"North\" or \"South\"\r\n    :return: Returns a list of fields that are active and belong to the given region\r\n    \"\"\"\r\n    field_list = []\r\n    growers = Decagon.open_pickle()\r\n\r\n    for grower in growers:\r\n        if region == \"Both\":\r\n            for field in grower.fields:\r\n                if field.active:\r\n                    field_list.append(field.name)\r\n        else:\r\n            if grower.region == region:\r\n                for field in grower.fields:\r\n                    if field.active:\r\n                        field_list.append(field.name)\r\n    return field_list\r\n\r\n\r\ndef check_for_new_cimis_stations():\r\n    \"\"\"\r\n    Check to see if any new cimis stations has been added to the pickle\r\n    \"\"\"\r\n    cimisStation = CimisStation()\r\n    stomato_pickle = Decagon.open_pickle()\r\n    cimisStation.check_for_new_cimis_stations(stomato_pickle=stomato_pickle)\r\n\r\n\r\ndef swap_logger(\r\n        old_logger_id: str,\r\n        new_logger_id: str,\r\n        old_logger_name: str,\r\n        new_logger_password: str,\r\n        swap_date\r\n)->bool:\r\n    \"\"\"\r\n    Function sets up a new logger using the information of the old logger\r\n    :param old_logger_id: Old Logger ID\r\n    :param new_logger_id: New Logger ID\r\n    :param old_logger_name: Old Logger Name\r\n    :param new_logger_password: New Logger Password\r\n    :param swap_date: Date logger swap occurred\r\n    :return: Was logger swapped successfully? True or False\r\n    \"\"\"\r\n    logger_added_successfully = False\r\n    logger_information_saved = False\r\n    growers = Decagon.open_pickle()\r\n    for grower in growers:\r\n        for field in grower.fields:\r\n            for logger in field.loggers:\r\n                # Set up old logger as broken and store old information to set up new logger with\r\n                if old_logger_id == logger.id and old_logger_name == logger.name:\r\n                    print(f'\\t\\tDeactivating old logger')\r\n                    logger.broken = True\r\n                    logger.active = False\r\n                    logger.uninstall_date = swap_date\r\n\r\n                    crop_type = logger.crop_type\r\n                    soil_type = logger.soil.soil_type\r\n                    gpm = logger.gpm\r\n                    irrigation_acres = logger.irrigation_set_acres\r\n                    logger_direction = logger.logger_direction\r\n                    lat = logger.lat\r\n                    long = logger.long\r\n                    planting_date = logger.planting_date\r\n                    rnd = logger.rnd\r\n                    field_name = field.name\r\n                    logger_information_saved = True\r\n                    break\r\n\r\n    # Add new logger using old logger information\r\n    if logger_information_saved:\r\n        print(f'\\t\\tCreating new logger')\r\n        new_logger = Decagon.setup_logger(\r\n            new_logger_id,\r\n            new_logger_password,\r\n            old_logger_name,\r\n            crop_type,\r\n            soil_type,\r\n            gpm,\r\n            irrigation_acres,\r\n            logger_direction,\r\n            lat,\r\n            long,\r\n            swap_date,\r\n            planting_date=planting_date,\r\n            rnd=rnd\r\n        )\r\n        if new_logger is not None:\r\n            logger_added_successfully = True\r\n\r\n        print(f\"\\t\\tAdding new logger to field {field_name}\")\r\n        for grower in growers:\r\n            for field in grower.fields:\r\n                if field_name == field.name:\r\n                    field.add_logger(new_logger)\r\n                    for logger in field.loggers:\r\n                        if logger.grower == None:\r\n                            logger.grower = grower\r\n                        if logger.field == None:\r\n                            logger.field = field\r\n                    print(\"\\tAdded logger: \" + new_logger_id)\r\n                    # Add new logger id to pickle of logger ID's in the pickle\r\n                    add_logger_id_to_pickle(new_logger_id)\r\n\r\n        Decagon.write_pickle(growers)\r\n\r\n    return logger_added_successfully\r\n\r\n\r\ndef logger_swap_process():\r\n    \"\"\"\r\n    Checks to see if there are any new submissions for loggers that need to be swapped\r\n    \"\"\"\r\n    # G Sheet ID and tab name\r\n    # sheet_id = '1rsPrX44pCHOhbOHZfWubDkuuexP8pSG8kspW03FAqOU'\r\n    sheet_id = NEW_LOGGER_SETUPS_SHEET_ID\r\n\r\n    # range_name = \"Logger Swap Form\"\r\n    range_name = \"Logger Swap\"\r\n\r\n    g_sheet = GSheetCredentialSevice.GSheetCredentialSevice()\r\n    service = g_sheet.getService()\r\n\r\n    result = gSheetReader.getServiceRead(range_name, sheet_id, service)\r\n    row_result = result['valueRanges'][0]['values']\r\n\r\n    logger_dict = set_up_password_dict(service, sheet_id)\r\n\r\n    backed_swap_done_header = gSheetReader.getColumnHeader(\"Swap Done\", row_result)\r\n    timestamp_header = gSheetReader.getColumnHeader(\"Timestamp\", row_result)\r\n\r\n    # Loop through each row\r\n    for row_index, row in enumerate(row_result):\r\n        try:\r\n            if row_index == 0:\r\n                continue\r\n            elif row[timestamp_header] == \"\":\r\n                continue\r\n\r\n            backed_swap_done = row[backed_swap_done_header]\r\n            # Check swap flag to see if we have already completed the swap before\r\n            if backed_swap_done == 'FALSE':\r\n                backed_swap_done = False\r\n            if not backed_swap_done:\r\n                # Assign indexes to columns\r\n                print(f'\\tFound logger to swap...')\r\n                old_logger_id_header = gSheetReader.getColumnHeader(\"Old Logger ID\", row_result)\r\n                new_logger_id_header = gSheetReader.getColumnHeader(\"New Logger ID\", row_result)\r\n                old_logger_name_header = gSheetReader.getColumnHeader(\"Old / New Logger Name\", row_result)\r\n                date_swapped_header = gSheetReader.getColumnHeader(\"Date of the Swap\", row_result)\r\n\r\n                old_logger_id = row[old_logger_id_header].strip()\r\n                new_logger_id = row[new_logger_id_header].strip()\r\n                old_logger_name = row[old_logger_name_header].strip()\r\n                date_swapped = row[date_swapped_header].strip()\r\n                date_swapped_converted = datetime.strptime(date_swapped, '%Y-%m-%d').date()\r\n\r\n                print(f'\\tSwapping old logger: {old_logger_id} for new logger: {new_logger_id}')\r\n                # Replace logger\r\n                replaced_logger_successfully = swap_logger(old_logger_id, new_logger_id, old_logger_name, logger_dict[new_logger_id],\r\n                                                           date_swapped_converted)\r\n\r\n                # If logger was replaced update swap flag on G Sheet\r\n                if replaced_logger_successfully:\r\n                    target_cell = f'{range_name}!G{row_index + 1}'\r\n                    # print(target_cell)\r\n                    gSheetReader.write_target_cell(target_cell, True, sheet_id, service)\r\n                    print(\"\\tLogger Replaced Successfully\")\r\n\r\n\r\n        except IndexError:\r\n            continue\r\n\r\n        except Exception as err:\r\n            print(err)\r\n            continue\r\n\r\n\r\ndef change_psi_for_specific_field_logger(field_name: str, logger_name: str, should_be_on: bool = False):\r\n    \"\"\"\r\n    Turns on PSI for a specific logger in a specific field\r\n    :param should_be_on: Should PSI be on or off\r\n    :param field_name: Name of field\r\n    :param logger_name: Name of logger\r\n    :return:\r\n    \"\"\"\r\n    growers = Decagon.open_pickle()\r\n    for grower in growers:\r\n        for field in grower.fields:\r\n            if field.name == field_name:\r\n                for logger in field.loggers:\r\n                    if logger.name == logger_name:\r\n                        if should_be_on:\r\n                            logger.ir_active = True\r\n                            print(f\"Turned on PSI for {field.name}:{logger_name}\")\r\n                        else:\r\n                            logger.ir_active = False\r\n                            print(f\"Turned off PSI for {field.name}:{logger_name}\")\r\n    Decagon.write_pickle(growers)\r\n\r\n\r\ndef get_soil_type_from_coords(latitude, longitude):\r\n    \"\"\"\r\n    Grabs soil type from ADA API given lat, long\r\n    :param latitude:\r\n    :param longitude:\r\n    :return:\r\n    \"\"\"\r\n    point_wkt = f\"POINT({longitude} {latitude})\"\r\n    # SQL query to get soil texture information\r\n    query = f\"\"\"\r\n    SELECT mu.muname, c.cokey\r\n    FROM mapunit AS mu\r\n    JOIN component AS c ON c.mukey = mu.mukey\r\n    JOIN chorizon AS ch ON ch.cokey = c.cokey\r\n    WHERE mu.mukey IN (\r\n            SELECT DISTINCT mukey\r\n            FROM SDA_Get_Mukey_from_intersection_with_WktWgs84('{point_wkt}')\r\n        )\r\n    \"\"\"\r\n\r\n    # SDA request payload\r\n    request_payload = {\r\n        \"format\": \"JSON+COLUMNNAME+METADATA\",\r\n        \"query\": query\r\n    }\r\n\r\n    sda_url = \"https://sdmdataaccess.sc.egov.usda.gov/Tabular/SDMTabularService/post.rest\"\r\n    headers = {'Content-Type': 'application/json'}\r\n    response = requests.post(sda_url, json=request_payload, headers=headers)\r\n\r\n    # soil types intentionally formatted, longest to shortest length\r\n    # so when we check if soil_type in response_string\r\n    soil_types = ['Silty Clay Loam', 'Sandy Clay Loam',\r\n                  'Sandy Clay', 'Silt Loam', 'Clay Loam',\r\n                  'Silty Clay', 'Loamy Sand', 'Sandy Loam',\r\n                  'Sand', 'Clay', 'Loam', 'Silt']\r\n\r\n    if response.status_code == 200:\r\n        data = response.json()\r\n        if \"Table\" in data:\r\n            # the row in the json containing soil texture information\r\n            if data[\"Table\"][2]:\r\n                texture_line = data[\"Table\"][2][0]\r\n                lowercase_input = texture_line.lower()\r\n\r\n                matched_soil_type = None\r\n\r\n                # Iterate through the list of soil types and check for a match\r\n                for soil_type in soil_types:\r\n                    if soil_type.lower() in lowercase_input:\r\n                        matched_soil_type = soil_type\r\n                        break\r\n                return matched_soil_type\r\n        else:\r\n            print(\"No soil information found for the given coordinates.\")\r\n    else:\r\n        print(f\"Error: {response.status_code}, {response.text}\")\r\n\r\n\r\n# setup_field()\r\n\r\n# addLoggerIDToPickle('z6-07262')\r\n# setup_uninstallation_dates_2022()\r\n# setup_installation_dates_2022()\r\n# print(returnActiveFields(\"North\"))\r\n# update_field(\"CM Ochoa\", \"CMOchoaL36\")\r\n# Decagon.only_certain_growers_field_update(\"CM Ochoa\", \"CM OchoaL36\", True, True, True, True, False)\r\n# check_for_new_cimis_stations()\r\n# check_for_broken_loggers()\r\n# Decagon.show_pickle()\r\n# setup_field()\r\n# update_historical_et_for_perennials()\r\n\r\n# growers = Decagon.open_pickle()\r\n# for grower in growers:\r\n#     for field in grower.fields:\r\n#         logger_id_list = []\r\n#         for ind, logger in enumerate(field.loggers):\r\n#             if logger.id in logger_id_list:\r\n#                 print(f\"Duplicate Detected: {field.name}:{logger.name}\")\r\n#                 print(f\"{logger.id}:{ind}\")\r\n#                 del field.loggers[ind]\r\n#             logger_id_list.append(logger.id)\r\n# Decagon.write_pickle(growers)\r\n\r\n# change_psi_for_specific_field_logger(\"Lucero Rio VistaB 1-4, 8\", \"LR-148-NW\", should_be_on=True)\r\n# change_psi_for_specific_field_logger(\"Lucero Rio VistaB 1-4, 8\", \"LR-148-S\", should_be_on=True)\r\n# change_psi_for_specific_field_logger(\"JJB FarmsGI 17\", \"GI-17-C\", should_be_on=True)\r\n# change_psi_for_specific_field_logger(\"Lucero Stokes Tyler Island11, 12, 28\", \"LS-12-C\", should_be_on=True)\r\n# change_psi_for_specific_field_logger(\"Lucero Thornton StokesStokes 1/2\", \"LU-Stokes1-NW\", should_be_on=True)\r\n\r\n# change_psi_for_specific_field_logger(\"Matteoli BrothersN3\", \"MB-N3-S\", should_be_on=True)\r\n# change_psi_for_specific_field_logger(\"Matteoli BrothersN3\", \"MB-N3-N\", should_be_on=True)\r\n# change_psi_for_specific_field_logger(\"Lucero BakersfieldHeadquarters\", \"LB-Blue-W\")\r\n# change_psi_for_specific_field_logger(\"Lucero BakersfieldHeadquarters\", \"LB-Green-S\")\r\n# print(returnActiveFields())\r\n# check_for_missing_report_or_preview_in_pickle()\r\n# SQLScripts.delete_last_day('stomato-permanents', 'Andrew3106', 'AN-3106-C')\r\n# read_harvesting_dates()\r\n# removeField('Lucero Rio Vista','Lucero Rio Vista4')\r\n# updateField('Lucero Rio Vista', 'Lucero Rio Vista3', True, 'RV-03-N')\r\n# update_field('Andrew', 'Andrew3106', True, 'AN-3106-C', 24 * 3)\r\n# Decagon.show_pickle()\r\n# removeField('Lucero Rio Vista','Lucero Rio Vista2')\r\n# remove_field('Dougherty Bros', 'Dougherty BrosKRE')\r\n# remove_field('Rincon Farms Inc.', 'Rincon Farms Inc.2N, 2M, 2S')\r\n# remove_field('Lucero Mandeville', 'Lucero Mandeville5')\r\n# remove_field('Lucero Watermark', 'Lucero Watermark5, 6, 7')\r\n# remove_field('Muller Ag', 'Muller Ag219')\r\n# remove_field('Lucero Goosepond', 'Lucero Goosepondsepond3')\r\n# remove_field('Lucero Goosepond', 'Lucero Goosepond2')\r\n# remove_field('Lucero Goosepond', 'Lucero Goosepond1')\r\n# remove_logger_id_from_pickle('z6-11580')\r\n# SQLScripts.removeDuplicateET()\r\n# SQLScripts.deleteETDay('Matteoli BrothersK7', '2022-07-25', '2022-08-11')\r\n# SQLScripts.update_field_et('KTN JVYA1')\r\n# Decagon.remove_grower('Rincon Farms Inc.')\r\n# setup_field()\r\n# test_bug('Bone Farms LLCR12-13')\r\n# check_for_new_cimis_stations()\r\n# growers = Decagon.open_pickle()\r\n# for grower in growers:\r\n#     for field in grower.fields:\r\n#         if grower.name == 'Bone Farms LLC':\r\n#             for logger in field.loggers:\r\n#                 print(f\"{field.name}:{logger.name}:{logger.uninstall_date}\")\r\n#             field.cimis_station = '148'\r\n            # for logger in field.loggers:\r\n                # if logger.active:\r\n                    # if logger.name == 'LM-5DEF-E':\r\n#                         update_field(grower_name=grower.name, field_name=field.name, only_one_logger=True, logger_name=logger.name,\r\n#                                      subtract_mrid=24*0, rerun=False)\r\n# #                     # SQLScripts.remove_duplicate_data(logger)\r\n# #                     # result = logger.read_dxd()\r\n# #                     # print(result)\r\n# #                 # print(field.name)\r\n# #                 # logger.ir_active = True\r\n# #                 print(f\"{field.name} \\n\\t {logger.name} \\n\\t\\t {logger.ir_active}\")\r\n# Decagon.write_pickle(growers)\r\n\r\n# check_for_missing_report_or_preview_in_pickle()\r\n\r\n# Decagon.show_pickle()\r\n# update_field(\"Bone FarmsLLC\", \"Bone Farms LLCR12-13\")\r\n# updateField(\"Carvalho\", \"Carvalho316\", True, 'CA-316-NW', subtractMRID=24)\r\n# updateField(\"Carvalho\", \"Carvalho308\", True, 'CA-308-SW', subtractMRID=24)\r\n# updateField(\"Hughes\", \"Hughes301\", True, 'HU-301-NW', subtractMRID=24)\r\n# updateField(\"Hughes\", \"Hughes301\", True, 'HU-301-SE', subtractMRID=24)\r\n# updateField(\"Hughes\", \"Hughes303-4\", True, 'JH-303_4-NW', subtractMRID=24)\r\n# updateField(\"Hughes\", \"Hughes303-4\", True, 'JH-303_4-SE', subtractMRID=24)\r\n# update_field('Bone Farms LLC', 'Bone Farms LLCF6', True, 'BO-PI-NW', subtract_mrid=24*10, rerun=True)\r\n\r\n# update_field('Bone Farms LLC', 'Bone Farms LLCN42 N43', True, 'BF-N4243-NE', subtract_mrid=0, rerun=True)\r\n# update_field()\r\n# growers = Decagon.open_pickle()\r\n# for grower in growers:\r\n#     # print(grower.name)\r\n#     # if grower.name == 'Knight Farms':\r\n#     for field in grower.fields:\r\n#         if not hasattr(field, 'crop_type'):\r\n#             print(field.name)\r\n            # field.crop_type = field.loggers[0].crop_type\r\n            # print('\\t', field.crop_type)\r\n        # if field.crop_type == 'Pistachio' or field.crop_type == 'Pistachios':\r\n        #     print(field.name)\r\n        # if field.name == 'Bullseye FarmsOE10' or field.name == 'Matteoli Brothers42' or field.name == 'CM OchoaA11N':\r\n        #     for logger in field.loggers:\r\n                # logger.soil.set_soil_type('Clay')\r\n                # print(logger.name)\r\n                # print(logger.soil.field_capacity)\r\n                # print(logger.soil.wilting_point)\r\n#                 print(f\"{field.acres}: {type(field.acres)}\")\r\n                # field.acres = 113.0\r\n# Decagon.write_pickle(growers)\r\n#                 for logger in field.loggers:\r\n#                     if logger.name == 'BO-PI-NW':\r\n#                         SQLScripts.remove_duplicate_data(logger)\r\n# remove_field('OPC', 'OPC5-4')\r\n# Decagon.only_certain_growers_field_update('T&P', 'T&PCO4', False, True, True, True, True, False)\r\n# growers = Decagon.open_pickle()\r\n# # # # testBug('Lucero Watermark9')\r\n# for g in growers:\r\n#     for f in g.fields:\r\n#         if f.active and f.name == 'Meza':\r\n#             print(f\"{f.name} : {f.cimisStation}\")\r\n#             f.cimisStation = '124'\r\n#             print(f.cimisStation)\r\n# Decagon.write_pickle(growers)\r\n# for l in f.loggers:\r\n# if l.name == 'TAG-WM-SE':\r\n#                 print(l.active)\r\n#             if l.id == 'z6-11556':\r\n#                 print(f.name, \", \", f.id, \", \", l.name, \", \", f.active)\r\n#             print(g.region, ';', g.name, ';', f.nickname)\r\n#\r\n#         if f.name == 'Lucero SE Honkerlake04':\r\n#             SQLScripts.deleteETDay(f.name, '2022-08-29', '2022-09-05')\r\n#         if f.name == \"Lucero Rio Vista3\":\r\n#             for l in f.loggers:\r\n#                 if l.id == 'z6-03544':\r\n#                     l.id = 'z6-12396'\r\n#                     l.password = '84372-16909'\r\n#             print(type(f.cimisStation))\r\n#             for l in f.loggers:\r\n#                 # print(type(l.gpm))\r\n#                 # l.gpm = float(1400)\r\n#                 print(l.gpm)\r\n#     g.technician.logger_setup_notification_file_path = ''\r\n# Decagon.write_pickle(growers)\r\n# techs = Decagon.get_all_technicians(growers)\r\n# for t in techs:\r\n#     print(t)\r\n# t.logger_setup_notification_file_path = ''\r\n# print(t.logger_setup_notification_file_path)\r\n# Decagon.write_pickle(growers)\r\n# uninstallField('OPC', 'OPC3-3')\r\n# SQLScripts.removeDuplicateET()\r\n# SQLScripts.update_field_et('Lucero Watermark9')\r\n# growers = Decagon.open_pickle()\r\n# toma = TomatoKC.TomatoKC()\r\n# # dates = ['2022-07-21', '2022-07-22', '2022-07-23', '2022-07-24', '2022-07-25', '2022-07-26', '2022-07-27', '2022-07-28', '2022-07-29',\r\n# # '2022-07-30', '2022-07-31', '2022-08-01']\r\n# base = datetime.datetime.today() - datetime.timedelta(days=1)\r\n# date_list = [(base - datetime.timedelta(days=x)).date() for x in range(12)]\r\n# for g in growers:\r\n#     for f in g.fields:\r\n#         if f.name == 'Lucero SE Honkerlake04':\r\n#             # SQLScripts.update_field_et(f.name)\r\n#             for d in date_list:\r\n#                 kc = TomatoKC.TomatoKC.get_kc(toma, f.loggers[-1].planting_date, d)\r\n#                 SQLScripts.deleteNegativeET(f.name, d.strftime('%Y-%m-%d'), str(kc))\r\n#             # SQLScripts.deleteETDay(f.name, date_list[-1].strftime('%Y-%m-%d'))\r\n#             SQLScripts.update_field_et(f.name)\r\n\r\n#             testBug(f.name)\r\n\r\n#             for l in f.loggers:\r\n#                 removeLoggerIDFromPickle(l.id)\r\n#                 if l.id == 'z6-07113':\r\n#                     l.prev_day_switch = 240\r\n# Decagon.write_pickle(growers)\r\n#         try:\r\n#             print(\"Setting up Irrigation Scheduling for Field\")\r\n#             stationNumber = int(f.cimisStation)\r\n#             startDate = date(datetime.now().year - 1, 1, 1)\r\n#             endDate = date(datetime.now().year - 1, 12, 31)\r\n#             SQLScripts.setupIrrigationSchedulingDB(stationNumber, f.name, startDate, endDate,\r\n#                                                    datetime.now().year)\r\n#         except Exception as err:\r\n#             print(err)\r\n#             continue\r\n# check_for_missing_report_or_preview_in_pickle()\r\n# addLoggerIDToPickle('z6-11518')\r\n# removeField('Bullseye Farms', 'Bullseye FarmsYO2E East')\r\n# removeField('RnD', 'RnDRate Trial')\r\n# Decagon.removeGrower('Maricopa Orchards')\r\n# update_report_and_image_in_pickle('Dougherty BrosHB',\r\n#                                   'https://datastudio.google.com/reporting/81d4de96-c5d1-4629-b143-7c7324f05d6d',\r\n#                                   'https://i.imgur.com/q35zn9z.png')\r\n# setup_field()\r\n# check_for_new_cimis_stations()\r\n# Decagon.show_pickle()\r\n# SQLScripts.removeDuplicateET()\r\n# growers = Decagon.open_pickle()\r\n# for g in growers:\r\n#     for f in g.fields:\r\n#         if f.name == \"Bullseye FarmsYO2E\":\r\n#             for logger in f.loggers:\r\n#                 update_field(g.name, f.name, True, logger.name, subtract_mrid=19*24)\r\n\r\n#             for l in f.loggers:\r\n#                 if l.id == 'z6-11532':\r\n#                     l.crashed = False\r\n#             for l in f.loggers:\r\n#                 l.prev_day_switch = 0\r\n#             f.preview_url = 'https://i.imgur.com/AXjQrAw.png'\r\n#             for l in f.loggers:\r\n#                 if l.id == 'z6-11959':\r\n#                     l.id = 'z6-01882'\r\n#                     l.password = '36070-33974'\r\n# Decagon.write_pickle(growers)\r\n# updateField('Lucero Rio Vista', 'Lucero Rio Vista3', True, 'RV-03-N', subtractMRID=0)\r\n# if os.path.exists('C:\\\\Users\\\\javie\\\\Projects\\\\S-TOMAto\\\\credentials.json'):\r\n#     os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"C:/Users/javie/Projects/S-TOMAto/credentials.json\"\r\n# elif os.path.exists('C:\\\\Users\\\\javie\\\\PycharmProjects\\\\Stomato\\\\credentials.json'):\r\n#     os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"C:/Users/javie/PycharmProjects/Stomato/credentials.json\"\r\n# elif os.path.exists('C:\\\\Users\\\\jsalcedo\\\\PycharmProjects\\\\Stomato\\\\credentials.json'):\r\n#     os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"C:/Users/jsalcedo/PycharmProjects/Stomato/credentials.json\"\r\n# elif os.path.exists('C:\\\\Users\\\\jesus\\\\PycharmProjects\\\\Stomato\\\\credentials.json'):\r\n#     os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"C:/Users/jesus/PycharmProjects/Stomato/credentials.json\"\r\n#\r\n# updateField(\"Bullseye Farms\", \"Bullseye FarmsRG28\", True, \"Bull-RG28-NW\", subtractMRID=60)\r\n# SQLScripts.update_field_et('Maricopa Orchards1831')\r\n# removeField('F&S', 'F&SVerway FB 1, 8')\r\n# checkIfIrrSchedulingIsSetUp(\"Andrew3125\")\r\n# Decagon.show_pickle()\r\n# updateField(\"Fantozzi\", \"Fantozzi2_7 East\", True, \"DAT-NE\", subtractMRID=0)\r\n# uninstallField('Tim Kalfsbeek', 'Tim KalfsbeekBack 40 Farm')\r\n# updateGpmAcres()\r\n# updateRndLoggers()\r\n# Decagon.show_pickle()\r\n\r\n# growers = Decagon.open_pickle()\r\n# for g in growers:\r\n#     for f in g.fields:\r\n#         # if f.name == \"KTN JVYA1\":\r\n#         if f.active:\r\n#             # if f.name == 'Barrios Farms84':\r\n#             #     f.preview_url = 'https://i.imgur.com/PfzkMVe.png'\r\n#             print(f.name)\r\n#             print(\"\\t\"+f.preview_url)\r\n#             print(\"\\t\"+f.report_url)\r\n#             SQLScripts.update_portal_image(g, f, f.preview_url)\r\n#                 for l in f.loggers:\r\n#                     if l.name == 'BF-84-NE':\r\n#                         # l.id = 'z6-11518'\r\n#                         # l.password = '51428-59165'\r\n#                         print(l.id)\r\n#                         print(l.password)\r\n\r\n\r\n# if g.name == 'CM Ochoa':\r\n#     for l in f.loggers:\r\n#         if l.name == 'CM-L37WM-N' or l.name == 'CM-L37WM2-S' or l.name == 'CM-L35WM-N' or l.name == 'CM-L35WM2-S':\r\n#             print(l.name)\r\n#             l.active = False\r\n#             print('\\t' + str(l.active))\r\n# print(l.active)\r\n# f.cwsi_processor = CwsiProcessor.CwsiProcessor()\r\n# print('done')\r\n# if f.name == 'Dougherty BrosPC':\r\n# f.report_url = 'https://datastudio.google.com/reporting/29513a03-37e1-4873-85e0-17c1bcd2b636'\r\n# f.preview_url = 'https://i.imgur.com/0kQGrxZ.png'\r\n#             print(f.active)\r\n#             f.active = False\r\n#             print(f.active)\r\n#         print(f.name)\r\n# Decagon.write_pickle(growers)\r\n# if g.name == 'Hughes':\r\n# report = input(\"Please input portal report for field: \" + f.name + \"\\n\")\r\n# preview = input(\"Please input portal preview for field: \" + f.name + \"\\n\")\r\n# f.report_url = report\r\n# f.preview_url = preview\r\n# print(f.report_url)\r\n# print(f.preview_url)\r\n#     if newNickname == \"\":\r\n#         print(\"Keeping nickname the same\")\r\n#     else:\r\n#         print(\"Changing nickname to : \" + newNickname)\r\n#         f.nickname = newNickname\r\n# growers = Decagon.open_pickle()\r\n# for g in growers:\r\n#     for f in g.fields:\r\n# if g.name == 'Carvalho':\r\n#     f.nickname = input(\"Enter Nickname for Carvalho Field: \" + f.name + \"\\n\")\r\n# f.nickname = f.name.split(g.name)[-1]\r\n# print(f.nickname)\r\n# Decagon.write_pickle(growers)\r\n\r\n# Decagon.removeGrower('Jesus')\r\n# Decagon.remove_field(\"Fantozzi\", \"Fantozzi2_7 East\")\r\n# removeLoggerIDFromPickle(\"z6-07262\")\r\n# removeLoggerIDFromPickle(\"z6-07264\")\r\n# removeLoggerIDFromPickle(\"z6-12337\")\r\n\r\n# setupField()\r\n# showLoggerIDPickle()\r\n# testBug(\"Bone Farms LLCF7\")\r\n# Decagon.show_pickle()\r\n# fieldName = 'Carvalho315'\r\n# loggerName = ''\r\n# fc = 36\r\n# wp = 22\r\n# dbwriter = DBWriter.DBWriter()\r\n# Decagon.show_pickle()\r\n# Decagon.removeGrower('Lucero Watermark')\r\n# growers = Decagon.open_pickle()\r\n# for g in growers:\r\n#     for f in g.fields:\r\n#         if f.name == 'RKB115_116_107':\r\n#             for l in f.loggers:\r\n#                 if l.active:\r\n#                     print(l.id)\r\n# Decagon.removeLogger('Bullseye Farms', 'Bullseye FarmsME4', 'z6-11968')\r\n#             field_name = dbwriter.remove_unwanted_chars_for_db(f.name)\r\n#             print(\"Working on Field: \", f.name)\r\n#             for l in f.loggers:\r\n#                 print('\\t Working on Logger: ', l.name)\r\n#                 # print(\"\\t\\tDeleting Repeat Data\")\r\n#                 # SQLScripts.delete_repeat_data(f.name,l.name)\r\n#                 updated = False\r\n#                 print(\"\\t\\tChecking Date to see if yesterday updated\")\r\n#                 dataset_id = \"stomato.\" + field_name + \".\" + l.name\r\n#                 dmlStatement = \"select date from`\" + dataset_id + \"` order by date\"\r\n#                 # print(dmlStatement)\r\n#                 result = dbwriter.run_dml(dmlStatement)\r\n#                 for r in result:\r\n#                     if str(r[0]) == '2022-07-15':\r\n#                         # print(\"Field Updated\")\r\n#                         updated = True\r\n#                 if not updated:\r\n#                     print(\"\\t\\t\\tField did not update: \", f.name, \"\\n Logger: \", l.name)\r\n#                     print(\"\\t\\t\\tUpdating Field\")\r\n#                     updateField(g.name, f.name, True, l.name, subtractMRID=24)\r\n# growers = Decagon.open_pickle()\r\n# for g in growers:\r\n#     for f in g.fields:\r\n#         if f.name == 'DCBVerway T1, T2, T3':\r\n#             for l in f.loggers:\r\n#                 if l.id == 'z6-12309':\r\n#                     l.password = '82466-02422'\r\n# Decagon.write_pickle(growers)\r\n#                     print(\"Changing fc and wp for logger: \" + l.name)\r\n#                     l.fieldCapacity = fc\r\n#                     l.wiltingPoint = wp\r\n#                     SQLScripts.update_FC_WP(f.name, l.name, fc, wp)\r\n#                 elif loggerName == '':\r\n#                     print(\"Changing fc and wp for logger: \" + l.name)\r\n#                     l.fieldCapacity = fc\r\n#                     l.wiltingPoint = wp\r\n#                     SQLScripts.update_FC_WP(f.name, l.name, fc, wp)\r\n#\r\n#\r\n# Decagon.write_pickle(growers)\r\n\r\n# count = 0\r\n# growers = Decagon.open_pickle()\r\n# for g in growers:\r\n#     for f in g.fields:\r\n#         if f.active == True:\r\n#             if f.name == \"DCBVerway T1, T2, T3\":\r\n#                 for l in f.loggers:\r\n#                     if l.id == 'z6-01892':\r\n#                         Decagon.removeLogger(g.name, f.name, l.id)\r\n# print(\"Do I have a second logger?\")\r\n#                         l.name = 'Bull-RG60-SE'\r\n#                     if l.id == 'z6-01953':\r\n#                         l.name = 'Bull-RG60-NW'\r\n# #             print(f.name)\r\n#             f.name = 'RKB115_116_107'\r\n#             # for l in f.loggers:\r\n#                 # print(\"\\t\" + l.name)\r\n# Decagon.write_pickle(growers)\r\n# Decagon.show_pickle()\r\n\r\n# l.name = 'SMonica1-BlkY-C'\r\n# if l.name == 'MA-BW-PI':\r\n#     l.name = 'MA-BWPI-SW'\r\n# if l.name == 'MA-YW-PI':\r\n#     l.name = 'MA-YWPI-SW'\r\n# if l.name == 'MA-BE-PI':\r\n#     l.name = 'MA-BEPI-NE'\r\n# if l.name == 'MA-YE-PI':\r\n#     l.name = 'MA-YEPI-NE'\r\n\r\n# if g.name == \"Meza\":\r\n#     for f in g.fields:\r\n#         print(f.name)\r\n#         for l in f.loggers:\r\n#             print(l.name)\r\n#             print(l.id)\r\n#             print(l.password)\r\n# print(count)\r\n# Decagon.deactivate_growers_with_all_inactive_fields()\r\n# fieldName = \"OPC3-2\"\r\n# for g in growers:\r\n#     for f in g.fields:\r\n#         if fieldName == f.name:\r\n#             f.active = True\r\n#             for l in f.loggers:\r\n#                 l.active = True\r\n# Decagon.write_pickle(growers)\r\n#             Decagon.deactivate_field(g.name, f.name)\r\n#             # for l in f.loggers:\r\n#             #     Decagon.deactivate_logger(g.name, f.name, l.id)\r\n# # Decagon.deactivate_field(\"OPC\", 'OPC3-2')\r\n\r\n# SQLScripts.deleteLastDay('DCBNees 7-8', 'z6-12427', '2021-08-18')\r\n# updateField(\"DCB\", \"DCBRogerro 30-40\", True, 'DCB-Rog30_40-N', subtractMRID=24)\r\n\r\n# updateField('RnD', 'RnDDouble-Single Trial', True, 'SLine-DTape-S', subtractMRID=6*24+10)\r\n# updateField('Lucero Bakersfield', 'Lucero BakersfieldTowerline', True, 'TO-Green-N', subtractMRID=0)\r\n# updateField('Carvalho', 'Carvalho308', True, 'z6-03436', subtractMRID=0)\r\n# updateField('JJB Farms', 'JJB FarmsJones Tract', True, 'z6-12336', subtractMRID=31)\r\n# updateField('JJB Farms', 'JJB FarmsJones Tract', True, 'z6-12429', subtractMRID=31)\r\n\r\n# Decagon.removeField(\"JHP\", \"JHPBase 5\")\r\n# removeLoggerIDFromPickle(\"z6-07214\")\r\n# removeLoggerIDFromPickle(\"5G118559\")\r\n# Decagon.removeField(\"DCB\", \"DCBNees 7-8\")\r\n# removeLoggerIDFromPickle(\"z6-11976\")\r\n# removeLoggerIDFromPickle(\"z6-01887\")\r\n# removeLoggerIDFromPickle(\"z6-01871\")\r\n# #\r\n# removeAllLoggerIDFromPickle()\r\n# showLoggerIDPickle()\r\n# Decagon.removeGrower(\"KTN JV\")\r\n# Decagon.removeLastGrower()\r\n# addLoggerIDToPickle(\"5G105816\")\r\n\r\n# updateField('DCB', 'DCBEdgemar 228', hasLogger=True, logger='z6-12309', subtractMRID=24*12)\r\n#\r\n# Decagon.onlyCertainGrowersFieldUpdate(\"UC Davis\", \"UC DavisUCD Veg Crops\", get_et=False, get_weather=True, get_data=False,\r\n#                                       write_to_sheet=True, write_to_portal_sheet=False, write_to_db=False)\r\n# Decagon.onlyCertainGrowersFieldUpdate(\"David Santos\", \"David SantosSP3\", get_et=False, get_weather=True, get_data=True,\r\n#                                       write_to_sheet=True, write_to_portal_sheet=True, write_to_db=True)\r\n\r\n# Decagon.onlyCertainGrowersFieldUpdate(\"DCB\", \"DCBMadd\", get_et=False, get_weather=True, get_data=True,\r\n#                                       write_to_sheet=True, write_to_portal_sheet=True, write_to_db=True)\r\n\r\n# Decagon.onlyCertainGrowersFieldUpdate(\"Maricopa Orchards\", \"Maricopa Orchards3425\", get_et=False, get_weather=True, get_data=True,\r\n#                                       write_to_sheet=True, write_to_portal_sheet=True, write_to_db=True)\r\n\r\n# Decagon.onlyCertainGrowersFieldUpdate(\"Hughes\", \"Hughes301\", get_et=False, get_weather=True, get_data=True,\r\n#                                       write_to_sheet=True, write_to_portal_sheet=True, write_to_db=True)\r\n\r\n# Decagon.onlyCertainGrowersFieldLoggerUpdate(\"Hughes\", \"Hughes309-4\", \"z6-07220\", write_to_sheet=True,\r\n#                                             write_to_portal_sheet=True, write_to_db=True, subtract_from_mrid=28)\r\n\r\n# ET Update\r\n\r\n# Decagon.onlyCertainGrowersETUpdate(\"Hughes\", writeToSheet=False, writeToDB=True)\r\n\r\n# Decagon.onlyCertainGrowersUpdate(\"Carvalho\", get_et=True, write_to_sheet=True)\r\n\r\n# Decagon.onlyCertainGrowersUpdate(\"DCB\", get_et=False, get_weather=True, get_data=True,\r\n#                                  write_to_sheet=True, write_to_portal_sheet=True, write_to_db=False)\r\n\r\n# os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"C:\\\\Users\\\\jsalcedo\\\\PycharmProjects\\\\Stomato\\\\credentials.json\"\r\n\r\n# Decagon.onlyCertainGrowersFieldLoggerUpdate(\"Maricopa Orchards\", \"Maricopa Orchards3425\", \"z6-06012\", write_to_sheet=True,\r\n#                                             write_to_portal_sheet=True, write_to_db=True, subtract_from_mrid=33)\r\n\r\n# Decagon.show_pickle()\r\n\r\n# Decagon.onlyCertainGrowersFieldLoggerUpdate(\"OPC\", \"OPC24-3\", \"5G129309\", write_to_sheet=False,\r\n#                                             write_to_portal_sheet=False, write_to_db=False, subtract_from_mrid=80)\r\n\r\n# creds = GSheetCredentialSevice.GSheetCredentialSevice()\r\n# creds.getCreds()\r\n\r\n# growers = Decagon.open_pickle()\r\n# for g in growers:\r\n#     for f in g.fields:\r\n#         if f.name == \"DCBMaricopa West\" and g.name == \"DCB\":\r\n#             print(type(f.cimisStation))\r\n#             f.cimisStation = \"105\"\r\n#             print(f.cimisStation)\r\n#\r\n#             Decagon.write_pickle(growers)\r\n\r\n# setup_field()\r\n# check_for_missing_report_or_preview_in_pickle()\r\n# print('Logger setups')\r\n# logger_setups_process()\r\n\r\n# setup_field()\r\n# remove_field('Matteoli Bros', 'Matteoli BrosM14')\r\n# remove_field('Dougherty Bros', 'Dougherty BrosT3')\r\n# logger_setups_process()\r\n# notification_setup()\r\n\r\n# check_for_missing_report_or_preview_in_pickle()\r\n\r\n# check_for_broken_loggers()\r\n\r\n# remove_field('Kubo & Young', 'Kubo & YoungKF1')\r\n# logger_setups_process()
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/LoggerSetups.py b/LoggerSetups.py
--- a/LoggerSetups.py	(revision 5cf7d78a1d94023d00661edf5517badacc064130)
+++ b/LoggerSetups.py	(date 1720741974344)
@@ -52,7 +52,8 @@
                     logger_list.append(logger.id)
     return grower_list, field_list, logger_list
 
-def check_if_grower_in_list(grower_name:str, grower_list:list, technician:str, region:str) -> list:
+
+def check_if_grower_in_list(grower_name: str, grower_list: list, technician: str, region: str) -> list:
     """
     Checks to see if grower exists in the grower list, and if not it creates a new grower in the pickle
     :param grower_name: Grower Name
@@ -68,7 +69,7 @@
     return grower_list
 
 
-def add_logger_id_to_pickle(logger_id:str):
+def add_logger_id_to_pickle(logger_id: str):
     """
     Function adds a logger id to the logger id pickle
     :param logger_id: Logger ID
@@ -78,7 +79,7 @@
     Decagon.write_pickle(loggers, filename="loggerList.pickle")
 
 
-def remove_field(grower_target:str, field_target:str):
+def remove_field(grower_target: str, field_target: str):
     """
     Function removes a field from a grower
     :param grower_target: Grower Name
@@ -98,7 +99,7 @@
     # Decagon.write_pickle(growers)
 
 
-def deactivate_field(grower:str, field: str, uninstall_date:date):
+def deactivate_field(grower: str, field: str, uninstall_date: date):
     """
     Function deactivates a field from a grower
     :param grower: Grower Name
@@ -118,7 +119,6 @@
     Decagon.deactivate_field(grower, field)
 
 
-
 def remove_logger_id_from_pickle(logger_id: str):
     """
 
@@ -180,6 +180,7 @@
     # print(loggerDict)
     return logger_passwords_dict
 
+
 def find_password(logger_id: str, logger_dict: dict) -> str:
     """
     Find password for a logger in the given logger dictionary
@@ -189,6 +190,7 @@
     """
     return logger_dict.get(logger_id)
 
+
 def check_if_logger_has_been_added_to_field_prev(logger_id: str, field_name: str, grower_name: str) -> bool:
     """
     Checks if logger has been added to a previous field
@@ -208,9 +210,9 @@
     return False
 
 
-
-def add_logger_to_field(row, result, logger_num: str, logger_dict: dict, logger_list: list, grower_name: str, field_list: list, field_name_pickle: str, tab_id, field_name:str,
-                        additional_stations:bool):
+def add_logger_to_field(row, result, logger_num: str, logger_dict: dict, logger_list: list, grower_name: str,
+                        field_list: list, field_name_pickle: str, tab_id, field_name: str,
+                        additional_stations: bool):
     """
 
     :param row:
@@ -311,7 +313,7 @@
                 inactive_cimis_station_list = cimis_station.return_inactive_cimis_stations_list()
                 cimis_stations_data = cimis.get_list_of_active_eto_stations()
                 closest_cimis_station = cimis.get_closest_station(cimis_stations_data, float(lat), float(long),
-                                                                        inactive_cimis_station_list)
+                                                                  inactive_cimis_station_list)
 
                 if closest_cimis_station is None:
                     print(
@@ -344,7 +346,8 @@
 
         # If logger password has been found, set up new logger
         if logger_password:
-            logger = Decagon.setup_logger(logger_id, logger_password, logger_name, crop_type, soil_type, gpm, acres, logger_direction, lat, long,
+            logger = Decagon.setup_logger(logger_id, logger_password, logger_name, crop_type, soil_type, gpm, acres,
+                                          logger_direction, lat, long,
                                           install_date_converted, planting_date=planting_date, rnd=rnd)
             growers = Decagon.open_pickle()
             print("\tLogger is not in List")
@@ -364,7 +367,8 @@
             print("\t\tPassword does not match logger ID")
 
 
-def loop_through_loggers(field_name_pickle: str, grower_name: str, field_name: str, field_list: list, logger_list: list, row, result, logger_dict: dict, num_of_loggers: int, tab_id="",
+def loop_through_loggers(field_name_pickle: str, grower_name: str, field_name: str, field_list: list, logger_list: list,
+                         row, result, logger_dict: dict, num_of_loggers: int, tab_id="",
                          add_stations: bool = False):
     """
 
@@ -381,7 +385,8 @@
     :param add_stations: Boolean to know whether we are adding additional stations to a previous field
     """
     for logger_num in range(1, num_of_loggers + 1):
-        add_logger_to_field(row, result, str(logger_num), logger_dict, logger_list, grower_name, field_list, field_name_pickle, tab_id, field_name,
+        add_logger_to_field(row, result, str(logger_num), logger_dict, logger_list, grower_name, field_list,
+                            field_name_pickle, tab_id, field_name,
                             add_stations)
 
 
@@ -426,8 +431,6 @@
     field_names_successfully_setup = []
     field_names_failed_setup = []
 
-
-
     # Loop through each row in the Google sheet and check if the field is in our current pickle. If it isn't,
     #  go through the setup process using that row's data.
     for row_index, row in enumerate(row_result):
@@ -466,7 +469,8 @@
 
                 if cimis_stations_data is None:
                     print()
-                    print(f'\t\tFailed the CIMIS call for stations {cimis_api_attempt_limit} times. Ending logger setups')
+                    print(
+                        f'\t\tFailed the CIMIS call for stations {cimis_api_attempt_limit} times. Ending logger setups')
                     successful_setup = False
                     return None
                 else:
@@ -832,7 +836,8 @@
                     additional_stations_boolean = False
 
                 # Loop through each logger that is in the form for that specific field
-                loop_through_loggers(field_name_pickle, grower_name, field_name, field_list, logger_list, row, row_result, logger_dict,
+                loop_through_loggers(field_name_pickle, grower_name, field_name, field_list, logger_list, row,
+                                     row_result, logger_dict,
                                      num_of_loggers, add_stations=additional_stations_boolean)
 
                 # Sets up nickname for field
@@ -933,7 +938,8 @@
             handle_coastal_list(cimis_stations_data, station_number, stations_to_skip)
 
             stations_to_skip.append(station_number)
-            station_number = cimis.get_closest_station_in_county(station_number, stations_to_skip, cached_stations, cimis_stations_data)
+            station_number = cimis.get_closest_station_in_county(station_number, stations_to_skip, cached_stations,
+                                                                 cimis_stations_data)
 
             # Found a good county station that's valid
             if station_number is not None:
@@ -942,7 +948,8 @@
             # Tried all in the county now look for closest
             else:
                 print('\tChecking Closest in Range Stations....')
-                station_number = cimis.get_closest_valid_station(float(field.lat), float(field.long), stations_to_skip, cached_stations, cimis_stations_data)
+                station_number = cimis.get_closest_valid_station(float(field.lat), float(field.long), stations_to_skip,
+                                                                 cached_stations, cimis_stations_data)
                 if station_number is not None:
                     Decagon.write_new_historical_et_to_db_2(dataset, station_number, cached_stations[station_number],
                                                             overwrite=True)
@@ -1031,7 +1038,8 @@
             if field_name_pickle_cleaned_up in missing_report_preview_list and field_setup_done:
                 report = row[report_header]
                 preview = row[preview_header]
-                # Update the field in the pickle with the correct report and preview values
+
+                # Update the field in the pickle and DB with the correct report and preview values
                 update_report_and_image_in_pickle(field_name_pickle_cleaned_up, report, preview)
 
                 # Send notification that field is finished and ready to be shown to the grower
@@ -1096,7 +1104,8 @@
     Decagon.write_pickle(growers)
 
 
-def update_field(grower_name: str, field_name: str, only_one_logger: bool = False, logger_name: str = "", subtract_mrid: int = 0, rerun: bool = False):
+def update_field(grower_name: str, field_name: str, only_one_logger: bool = False, logger_name: str = "",
+                 subtract_mrid: int = 0, rerun: bool = False):
     """
     Updates either the whole field or only a specific logger for the field
     and resets previous day switch and removes duplicate data and updates ET at once
@@ -1144,8 +1153,9 @@
                                 SQLScripts.remove_duplicate_data(logger)
         SQLScripts.update_logger_et(field_name, logger_name)
     else:
-    # Update all loggers in the field
-        Decagon.only_certain_growers_field_update(grower_name, field_name, get_et=False, get_weather=True, get_data=True,
+        # Update all loggers in the field
+        Decagon.only_certain_growers_field_update(grower_name, field_name, get_et=False, get_weather=True,
+                                                  get_data=True,
                                                   write_to_db=True)
         for grower in growers:
             if grower.name == grower_name:
@@ -1231,7 +1241,7 @@
                         update_field(grower.name, field.name, True, logger.name, subtract_mrid=24)
 
 
-def return_active_fields(region:str="Both")->list:
+def return_active_fields(region: str = "Both") -> list:
     """
     Return active fields for a specific region or both regions
     :param region: Default value is "Both", but can be "North" or "South"
@@ -1268,7 +1278,7 @@
         old_logger_name: str,
         new_logger_password: str,
         swap_date
-)->bool:
+) -> bool:
     """
     Function sets up a new logger using the information of the old logger
     :param old_logger_id: Old Logger ID
@@ -1394,7 +1404,8 @@
 
                 print(f'\tSwapping old logger: {old_logger_id} for new logger: {new_logger_id}')
                 # Replace logger
-                replaced_logger_successfully = swap_logger(old_logger_id, new_logger_id, old_logger_name, logger_dict[new_logger_id],
+                replaced_logger_successfully = swap_logger(old_logger_id, new_logger_id, old_logger_name,
+                                                           logger_dict[new_logger_id],
                                                            date_swapped_converted)
 
                 # If logger was replaced update swap flag on G Sheet
@@ -1479,14 +1490,15 @@
             # the row in the json containing soil texture information
             if data["Table"][2]:
                 texture_line = data["Table"][2][0]
-                lowercase_input = texture_line.lower()
+                lowercase_soil_line = texture_line.lower()
 
                 matched_soil_type = None
 
                 # Iterate through the list of soil types and check for a match
                 for soil_type in soil_types:
-                    if soil_type.lower() in lowercase_input:
+                    if soil_type.lower() in lowercase_soil_line:
                         matched_soil_type = soil_type
+                        print(f'\t\t\tFound soil type: {lowercase_soil_line}')
                         break
                 return matched_soil_type
         else:
@@ -1495,373 +1507,20 @@
         print(f"Error: {response.status_code}, {response.text}")
 
 
-# setup_field()
-
-# addLoggerIDToPickle('z6-07262')
-# setup_uninstallation_dates_2022()
-# setup_installation_dates_2022()
-# print(returnActiveFields("North"))
-# update_field("CM Ochoa", "CMOchoaL36")
-# Decagon.only_certain_growers_field_update("CM Ochoa", "CM OchoaL36", True, True, True, True, False)
-# check_for_new_cimis_stations()
-# check_for_broken_loggers()
-# Decagon.show_pickle()
-# setup_field()
-# update_historical_et_for_perennials()
-
-# growers = Decagon.open_pickle()
-# for grower in growers:
-#     for field in grower.fields:
-#         logger_id_list = []
-#         for ind, logger in enumerate(field.loggers):
-#             if logger.id in logger_id_list:
-#                 print(f"Duplicate Detected: {field.name}:{logger.name}")
-#                 print(f"{logger.id}:{ind}")
-#                 del field.loggers[ind]
-#             logger_id_list.append(logger.id)
-# Decagon.write_pickle(growers)
-
-# change_psi_for_specific_field_logger("Lucero Rio VistaB 1-4, 8", "LR-148-NW", should_be_on=True)
-# change_psi_for_specific_field_logger("Lucero Rio VistaB 1-4, 8", "LR-148-S", should_be_on=True)
-# change_psi_for_specific_field_logger("JJB FarmsGI 17", "GI-17-C", should_be_on=True)
-# change_psi_for_specific_field_logger("Lucero Stokes Tyler Island11, 12, 28", "LS-12-C", should_be_on=True)
-# change_psi_for_specific_field_logger("Lucero Thornton StokesStokes 1/2", "LU-Stokes1-NW", should_be_on=True)
-
-# change_psi_for_specific_field_logger("Matteoli BrothersN3", "MB-N3-S", should_be_on=True)
-# change_psi_for_specific_field_logger("Matteoli BrothersN3", "MB-N3-N", should_be_on=True)
-# change_psi_for_specific_field_logger("Lucero BakersfieldHeadquarters", "LB-Blue-W")
-# change_psi_for_specific_field_logger("Lucero BakersfieldHeadquarters", "LB-Green-S")
-# print(returnActiveFields())
-# check_for_missing_report_or_preview_in_pickle()
-# SQLScripts.delete_last_day('stomato-permanents', 'Andrew3106', 'AN-3106-C')
-# read_harvesting_dates()
-# removeField('Lucero Rio Vista','Lucero Rio Vista4')
-# updateField('Lucero Rio Vista', 'Lucero Rio Vista3', True, 'RV-03-N')
-# update_field('Andrew', 'Andrew3106', True, 'AN-3106-C', 24 * 3)
-# Decagon.show_pickle()
-# removeField('Lucero Rio Vista','Lucero Rio Vista2')
-# remove_field('Dougherty Bros', 'Dougherty BrosKRE')
-# remove_field('Rincon Farms Inc.', 'Rincon Farms Inc.2N, 2M, 2S')
-# remove_field('Lucero Mandeville', 'Lucero Mandeville5')
-# remove_field('Lucero Watermark', 'Lucero Watermark5, 6, 7')
-# remove_field('Muller Ag', 'Muller Ag219')
-# remove_field('Lucero Goosepond', 'Lucero Goosepondsepond3')
-# remove_field('Lucero Goosepond', 'Lucero Goosepond2')
-# remove_field('Lucero Goosepond', 'Lucero Goosepond1')
-# remove_logger_id_from_pickle('z6-11580')
-# SQLScripts.removeDuplicateET()
-# SQLScripts.deleteETDay('Matteoli BrothersK7', '2022-07-25', '2022-08-11')
-# SQLScripts.update_field_et('KTN JVYA1')
-# Decagon.remove_grower('Rincon Farms Inc.')
-# setup_field()
-# test_bug('Bone Farms LLCR12-13')
-# check_for_new_cimis_stations()
-# growers = Decagon.open_pickle()
-# for grower in growers:
-#     for field in grower.fields:
-#         if grower.name == 'Bone Farms LLC':
-#             for logger in field.loggers:
-#                 print(f"{field.name}:{logger.name}:{logger.uninstall_date}")
-#             field.cimis_station = '148'
-            # for logger in field.loggers:
-                # if logger.active:
-                    # if logger.name == 'LM-5DEF-E':
-#                         update_field(grower_name=grower.name, field_name=field.name, only_one_logger=True, logger_name=logger.name,
-#                                      subtract_mrid=24*0, rerun=False)
-# #                     # SQLScripts.remove_duplicate_data(logger)
-# #                     # result = logger.read_dxd()
-# #                     # print(result)
-# #                 # print(field.name)
-# #                 # logger.ir_active = True
-# #                 print(f"{field.name} \n\t {logger.name} \n\t\t {logger.ir_active}")
-# Decagon.write_pickle(growers)
-
-# check_for_missing_report_or_preview_in_pickle()
-
-# Decagon.show_pickle()
-# update_field("Bone FarmsLLC", "Bone Farms LLCR12-13")
-# updateField("Carvalho", "Carvalho316", True, 'CA-316-NW', subtractMRID=24)
-# updateField("Carvalho", "Carvalho308", True, 'CA-308-SW', subtractMRID=24)
-# updateField("Hughes", "Hughes301", True, 'HU-301-NW', subtractMRID=24)
-# updateField("Hughes", "Hughes301", True, 'HU-301-SE', subtractMRID=24)
-# updateField("Hughes", "Hughes303-4", True, 'JH-303_4-NW', subtractMRID=24)
-# updateField("Hughes", "Hughes303-4", True, 'JH-303_4-SE', subtractMRID=24)
-# update_field('Bone Farms LLC', 'Bone Farms LLCF6', True, 'BO-PI-NW', subtract_mrid=24*10, rerun=True)
-
-# update_field('Bone Farms LLC', 'Bone Farms LLCN42 N43', True, 'BF-N4243-NE', subtract_mrid=0, rerun=True)
-# update_field()
-# growers = Decagon.open_pickle()
-# for grower in growers:
-#     # print(grower.name)
-#     # if grower.name == 'Knight Farms':
-#     for field in grower.fields:
-#         if not hasattr(field, 'crop_type'):
-#             print(field.name)
-            # field.crop_type = field.loggers[0].crop_type
-            # print('\t', field.crop_type)
-        # if field.crop_type == 'Pistachio' or field.crop_type == 'Pistachios':
-        #     print(field.name)
-        # if field.name == 'Bullseye FarmsOE10' or field.name == 'Matteoli Brothers42' or field.name == 'CM OchoaA11N':
-        #     for logger in field.loggers:
-                # logger.soil.set_soil_type('Clay')
-                # print(logger.name)
-                # print(logger.soil.field_capacity)
-                # print(logger.soil.wilting_point)
-#                 print(f"{field.acres}: {type(field.acres)}")
-                # field.acres = 113.0
-# Decagon.write_pickle(growers)
-#                 for logger in field.loggers:
-#                     if logger.name == 'BO-PI-NW':
-#                         SQLScripts.remove_duplicate_data(logger)
-# remove_field('OPC', 'OPC5-4')
-# Decagon.only_certain_growers_field_update('T&P', 'T&PCO4', False, True, True, True, True, False)
-# growers = Decagon.open_pickle()
-# # # # testBug('Lucero Watermark9')
-# for g in growers:
-#     for f in g.fields:
-#         if f.active and f.name == 'Meza':
-#             print(f"{f.name} : {f.cimisStation}")
-#             f.cimisStation = '124'
-#             print(f.cimisStation)
-# Decagon.write_pickle(growers)
-# for l in f.loggers:
-# if l.name == 'TAG-WM-SE':
-#                 print(l.active)
-#             if l.id == 'z6-11556':
-#                 print(f.name, ", ", f.id, ", ", l.name, ", ", f.active)
-#             print(g.region, ';', g.name, ';', f.nickname)
-#
-#         if f.name == 'Lucero SE Honkerlake04':
-#             SQLScripts.deleteETDay(f.name, '2022-08-29', '2022-09-05')
-#         if f.name == "Lucero Rio Vista3":
-#             for l in f.loggers:
-#                 if l.id == 'z6-03544':
-#                     l.id = 'z6-12396'
-#                     l.password = '84372-16909'
-#             print(type(f.cimisStation))
-#             for l in f.loggers:
-#                 # print(type(l.gpm))
-#                 # l.gpm = float(1400)
-#                 print(l.gpm)
-#     g.technician.logger_setup_notification_file_path = ''
-# Decagon.write_pickle(growers)
-# techs = Decagon.get_all_technicians(growers)
-# for t in techs:
-#     print(t)
-# t.logger_setup_notification_file_path = ''
-# print(t.logger_setup_notification_file_path)
-# Decagon.write_pickle(growers)
-# uninstallField('OPC', 'OPC3-3')
-# SQLScripts.removeDuplicateET()
-# SQLScripts.update_field_et('Lucero Watermark9')
-# growers = Decagon.open_pickle()
-# toma = TomatoKC.TomatoKC()
-# # dates = ['2022-07-21', '2022-07-22', '2022-07-23', '2022-07-24', '2022-07-25', '2022-07-26', '2022-07-27', '2022-07-28', '2022-07-29',
-# # '2022-07-30', '2022-07-31', '2022-08-01']
-# base = datetime.datetime.today() - datetime.timedelta(days=1)
-# date_list = [(base - datetime.timedelta(days=x)).date() for x in range(12)]
-# for g in growers:
-#     for f in g.fields:
-#         if f.name == 'Lucero SE Honkerlake04':
-#             # SQLScripts.update_field_et(f.name)
-#             for d in date_list:
-#                 kc = TomatoKC.TomatoKC.get_kc(toma, f.loggers[-1].planting_date, d)
-#                 SQLScripts.deleteNegativeET(f.name, d.strftime('%Y-%m-%d'), str(kc))
-#             # SQLScripts.deleteETDay(f.name, date_list[-1].strftime('%Y-%m-%d'))
-#             SQLScripts.update_field_et(f.name)
+def same_day_setup(field_names):
+    """
+    :param field_names: list of fields to run, grower name + field name
+    Function to set up a field in one day: setting up logger, running that field, subtracting from the MRID
+    """
+    logger_setups_process()
+    Decagon.only_certain_growers_fields_update(field_names, get_weather=True, get_data=True, write_to_portal=True, write_to_db=True)
+    Decagon.change
+    # new_field = what fields are new to the pickle? or a return from loggersetupsprocess
+    pass
 
-#             testBug(f.name)
-
-#             for l in f.loggers:
-#                 removeLoggerIDFromPickle(l.id)
-#                 if l.id == 'z6-07113':
-#                     l.prev_day_switch = 240
-# Decagon.write_pickle(growers)
-#         try:
-#             print("Setting up Irrigation Scheduling for Field")
-#             stationNumber = int(f.cimisStation)
-#             startDate = date(datetime.now().year - 1, 1, 1)
-#             endDate = date(datetime.now().year - 1, 12, 31)
-#             SQLScripts.setupIrrigationSchedulingDB(stationNumber, f.name, startDate, endDate,
-#                                                    datetime.now().year)
-#         except Exception as err:
-#             print(err)
-#             continue
-# check_for_missing_report_or_preview_in_pickle()
-# addLoggerIDToPickle('z6-11518')
-# removeField('Bullseye Farms', 'Bullseye FarmsYO2E East')
-# removeField('RnD', 'RnDRate Trial')
-# Decagon.removeGrower('Maricopa Orchards')
-# update_report_and_image_in_pickle('Dougherty BrosHB',
-#                                   'https://datastudio.google.com/reporting/81d4de96-c5d1-4629-b143-7c7324f05d6d',
-#                                   'https://i.imgur.com/q35zn9z.png')
 # setup_field()
-# check_for_new_cimis_stations()
-# Decagon.show_pickle()
-# SQLScripts.removeDuplicateET()
-# growers = Decagon.open_pickle()
-# for g in growers:
-#     for f in g.fields:
-#         if f.name == "Bullseye FarmsYO2E":
-#             for logger in f.loggers:
-#                 update_field(g.name, f.name, True, logger.name, subtract_mrid=19*24)
-
-#             for l in f.loggers:
-#                 if l.id == 'z6-11532':
-#                     l.crashed = False
-#             for l in f.loggers:
-#                 l.prev_day_switch = 0
-#             f.preview_url = 'https://i.imgur.com/AXjQrAw.png'
-#             for l in f.loggers:
-#                 if l.id == 'z6-11959':
-#                     l.id = 'z6-01882'
-#                     l.password = '36070-33974'
-# Decagon.write_pickle(growers)
-# updateField('Lucero Rio Vista', 'Lucero Rio Vista3', True, 'RV-03-N', subtractMRID=0)
-# if os.path.exists('C:\\Users\\javie\\Projects\\S-TOMAto\\credentials.json'):
-#     os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "C:/Users/javie/Projects/S-TOMAto/credentials.json"
-# elif os.path.exists('C:\\Users\\javie\\PycharmProjects\\Stomato\\credentials.json'):
-#     os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "C:/Users/javie/PycharmProjects/Stomato/credentials.json"
-# elif os.path.exists('C:\\Users\\jsalcedo\\PycharmProjects\\Stomato\\credentials.json'):
-#     os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "C:/Users/jsalcedo/PycharmProjects/Stomato/credentials.json"
-# elif os.path.exists('C:\\Users\\jesus\\PycharmProjects\\Stomato\\credentials.json'):
-#     os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = "C:/Users/jesus/PycharmProjects/Stomato/credentials.json"
-#
-# updateField("Bullseye Farms", "Bullseye FarmsRG28", True, "Bull-RG28-NW", subtractMRID=60)
-# SQLScripts.update_field_et('Maricopa Orchards1831')
-# removeField('F&S', 'F&SVerway FB 1, 8')
-# checkIfIrrSchedulingIsSetUp("Andrew3125")
-# Decagon.show_pickle()
-# updateField("Fantozzi", "Fantozzi2_7 East", True, "DAT-NE", subtractMRID=0)
-# uninstallField('Tim Kalfsbeek', 'Tim KalfsbeekBack 40 Farm')
-# updateGpmAcres()
-# updateRndLoggers()
-# Decagon.show_pickle()
-
-# growers = Decagon.open_pickle()
-# for g in growers:
-#     for f in g.fields:
-#         # if f.name == "KTN JVYA1":
-#         if f.active:
-#             # if f.name == 'Barrios Farms84':
-#             #     f.preview_url = 'https://i.imgur.com/PfzkMVe.png'
-#             print(f.name)
-#             print("\t"+f.preview_url)
-#             print("\t"+f.report_url)
-#             SQLScripts.update_portal_image(g, f, f.preview_url)
-#                 for l in f.loggers:
-#                     if l.name == 'BF-84-NE':
-#                         # l.id = 'z6-11518'
-#                         # l.password = '51428-59165'
-#                         print(l.id)
-#                         print(l.password)
-
-
-# if g.name == 'CM Ochoa':
-#     for l in f.loggers:
-#         if l.name == 'CM-L37WM-N' or l.name == 'CM-L37WM2-S' or l.name == 'CM-L35WM-N' or l.name == 'CM-L35WM2-S':
-#             print(l.name)
-#             l.active = False
-#             print('\t' + str(l.active))
-# print(l.active)
-# f.cwsi_processor = CwsiProcessor.CwsiProcessor()
-# print('done')
-# if f.name == 'Dougherty BrosPC':
-# f.report_url = 'https://datastudio.google.com/reporting/29513a03-37e1-4873-85e0-17c1bcd2b636'
-# f.preview_url = 'https://i.imgur.com/0kQGrxZ.png'
-#             print(f.active)
-#             f.active = False
-#             print(f.active)
-#         print(f.name)
-# Decagon.write_pickle(growers)
-# if g.name == 'Hughes':
-# report = input("Please input portal report for field: " + f.name + "\n")
-# preview = input("Please input portal preview for field: " + f.name + "\n")
-# f.report_url = report
-# f.preview_url = preview
-# print(f.report_url)
-# print(f.preview_url)
-#     if newNickname == "":
-#         print("Keeping nickname the same")
-#     else:
-#         print("Changing nickname to : " + newNickname)
-#         f.nickname = newNickname
-# growers = Decagon.open_pickle()
-# for g in growers:
-#     for f in g.fields:
-# if g.name == 'Carvalho':
-#     f.nickname = input("Enter Nickname for Carvalho Field: " + f.name + "\n")
-# f.nickname = f.name.split(g.name)[-1]
-# print(f.nickname)
-# Decagon.write_pickle(growers)
-
-# Decagon.removeGrower('Jesus')
-# Decagon.remove_field("Fantozzi", "Fantozzi2_7 East")
-# removeLoggerIDFromPickle("z6-07262")
-# removeLoggerIDFromPickle("z6-07264")
-# removeLoggerIDFromPickle("z6-12337")
-
-# setupField()
-# showLoggerIDPickle()
-# testBug("Bone Farms LLCF7")
-# Decagon.show_pickle()
-# fieldName = 'Carvalho315'
-# loggerName = ''
-# fc = 36
-# wp = 22
-# dbwriter = DBWriter.DBWriter()
-# Decagon.show_pickle()
-# Decagon.removeGrower('Lucero Watermark')
-# growers = Decagon.open_pickle()
-# for g in growers:
-#     for f in g.fields:
-#         if f.name == 'RKB115_116_107':
-#             for l in f.loggers:
-#                 if l.active:
-#                     print(l.id)
-# Decagon.removeLogger('Bullseye Farms', 'Bullseye FarmsME4', 'z6-11968')
-#             field_name = dbwriter.remove_unwanted_chars_for_db(f.name)
-#             print("Working on Field: ", f.name)
-#             for l in f.loggers:
-#                 print('\t Working on Logger: ', l.name)
-#                 # print("\t\tDeleting Repeat Data")
-#                 # SQLScripts.delete_repeat_data(f.name,l.name)
-#                 updated = False
-#                 print("\t\tChecking Date to see if yesterday updated")
-#                 dataset_id = "stomato." + field_name + "." + l.name
-#                 dmlStatement = "select date from`" + dataset_id + "` order by date"
-#                 # print(dmlStatement)
-#                 result = dbwriter.run_dml(dmlStatement)
-#                 for r in result:
-#                     if str(r[0]) == '2022-07-15':
-#                         # print("Field Updated")
-#                         updated = True
-#                 if not updated:
-#                     print("\t\t\tField did not update: ", f.name, "\n Logger: ", l.name)
-#                     print("\t\t\tUpdating Field")
-#                     updateField(g.name, f.name, True, l.name, subtractMRID=24)
-# growers = Decagon.open_pickle()
-# for g in growers:
-#     for f in g.fields:
-#         if f.name == 'DCBVerway T1, T2, T3':
-#             for l in f.loggers:
-#                 if l.id == 'z6-12309':
-#                     l.password = '82466-02422'
-# Decagon.write_pickle(growers)
-#                     print("Changing fc and wp for logger: " + l.name)
-#                     l.fieldCapacity = fc
-#                     l.wiltingPoint = wp
-#                     SQLScripts.update_FC_WP(f.name, l.name, fc, wp)
-#                 elif loggerName == '':
-#                     print("Changing fc and wp for logger: " + l.name)
-#                     l.fieldCapacity = fc
-#                     l.wiltingPoint = wp
-#                     SQLScripts.update_FC_WP(f.name, l.name, fc, wp)
-#
-#
-# Decagon.write_pickle(growers)
-
-# count = 0
+
+
 # growers = Decagon.open_pickle()
 # for g in growers:
 #     for f in g.fields:
@@ -1936,24 +1595,6 @@
 # Decagon.removeLastGrower()
 # addLoggerIDToPickle("5G105816")
 
-# updateField('DCB', 'DCBEdgemar 228', hasLogger=True, logger='z6-12309', subtractMRID=24*12)
-#
-# Decagon.onlyCertainGrowersFieldUpdate("UC Davis", "UC DavisUCD Veg Crops", get_et=False, get_weather=True, get_data=False,
-#                                       write_to_sheet=True, write_to_portal_sheet=False, write_to_db=False)
-# Decagon.onlyCertainGrowersFieldUpdate("David Santos", "David SantosSP3", get_et=False, get_weather=True, get_data=True,
-#                                       write_to_sheet=True, write_to_portal_sheet=True, write_to_db=True)
-
-# Decagon.onlyCertainGrowersFieldUpdate("DCB", "DCBMadd", get_et=False, get_weather=True, get_data=True,
-#                                       write_to_sheet=True, write_to_portal_sheet=True, write_to_db=True)
-
-# Decagon.onlyCertainGrowersFieldUpdate("Maricopa Orchards", "Maricopa Orchards3425", get_et=False, get_weather=True, get_data=True,
-#                                       write_to_sheet=True, write_to_portal_sheet=True, write_to_db=True)
-
-# Decagon.onlyCertainGrowersFieldUpdate("Hughes", "Hughes301", get_et=False, get_weather=True, get_data=True,
-#                                       write_to_sheet=True, write_to_portal_sheet=True, write_to_db=True)
-
-# Decagon.onlyCertainGrowersFieldLoggerUpdate("Hughes", "Hughes309-4", "z6-07220", write_to_sheet=True,
-#                                             write_to_portal_sheet=True, write_to_db=True, subtract_from_mrid=28)
 
 # ET Update
 
@@ -1976,16 +1617,15 @@
 
 # creds = GSheetCredentialSevice.GSheetCredentialSevice()
 # creds.getCreds()
-
+# cimis = CIMIS()
+# cimis_stations_data = cimis.get_list_of_active_eto_stations()
+#
+#
 # growers = Decagon.open_pickle()
 # for g in growers:
 #     for f in g.fields:
-#         if f.name == "DCBMaricopa West" and g.name == "DCB":
-#             print(type(f.cimisStation))
-#             f.cimisStation = "105"
-#             print(f.cimisStation)
-#
-#             Decagon.write_pickle(growers)
+#         if f.name == "Lucero NeesField 10":
+#             setup_irr_scheduling(f, cimis_stations_data)
 
 # setup_field()
 # check_for_missing_report_or_preview_in_pickle()
@@ -2003,4 +1643,4 @@
 # check_for_broken_loggers()
 
 # remove_field('Kubo & Young', 'Kubo & YoungKF1')
-# logger_setups_process()
\ No newline at end of file
+# logger_setups_process()
Index: SQLScripts.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import itertools\r\nimport pickle\r\nimport pprint\r\nfrom datetime import datetime, timedelta, date\r\nfrom os import path\r\nfrom random import choices\r\nfrom statistics import mean\r\nfrom string import ascii_uppercase, digits\r\n\r\nimport google.api_core.exceptions\r\nimport numpy\r\nfrom dateutil.relativedelta import relativedelta\r\nfrom google.api_core import exceptions\r\nfrom google.cloud import bigquery\r\n\r\nimport Decagon\r\nfrom CIMIS import CIMIS\r\nfrom CwsiProcessor import CwsiProcessor\r\nfrom DBWriter import DBWriter\r\n\r\ndbwriter = DBWriter()\r\nDIRECTORY_YEAR = \"2023\"\r\nPICKLE_DIRECTORY = \"H:\\\\Shared drives\\\\Stomato\\\\\" + DIRECTORY_YEAR + \"\\\\Pickle\\\\\"\r\n\r\ndef update_value_for_date(project, field_name, logger_name, date, value_name, value):\r\n    dml = 'UPDATE `' + str(project) + '.' + str(field_name) + '.' + str(logger_name) + '`' \\\r\n          + ' SET ' + str(value_name) + ' = ' + str(value) \\\r\n          + \" WHERE date = '\" + str(date) + \"'\"\r\n    print(dml)\r\n\r\n    dbwriter.run_dml(dml, project=project)\r\n\r\n\r\ndef delete_null_rows(project, field, logger, row_value='', start_date='', end_date=''):\r\n    print('Deleting nulls in: ' + str(field) + ' ' + str(logger))\r\n    if start_date == '' and end_date == '':\r\n        if row_value == '':\r\n            dml = \"DELETE   FROM \" \\\r\n                  + \"`\" + project + \".\" + str(field) + \".\" + str(logger) + \"`\" \\\r\n                  + \" WHERE logger_id is NULL\"\r\n        else:\r\n            dml = \"DELETE   FROM \" \\\r\n                  + \"`\" + project + \".\" + str(field) + \".\" + str(logger) + \"`\" \\\r\n                  + \" WHERE \" + row_value + \" is NULL\"\r\n    else:\r\n        if row_value == '':\r\n            dml = \"DELETE   FROM \" \\\r\n                  + \"`\" + project + \".\" + str(field) + \".\" + str(logger) + \"`\" \\\r\n                  + \" WHERE logger_id is NULL AND date BETWEEN DATE('\" + start_date + \"') AND DATE('\" + end_date + \"') \"\r\n        else:\r\n            dml = \"DELETE   FROM \" \\\r\n                  + \"`\" + project + \".\" + str(field) + \".\" + str(logger) + \"`\" \\\r\n                  + \" WHERE \" + row_value + \" is NULL AND date BETWEEN DATE('\" + start_date + \"') AND DATE('\" + end_date + \"') \"\r\n    dbwriter.run_dml(dml, project=project)\r\n\r\n\r\ndef delete_all_null_rows(project, row_value='', start_date='', end_date=''):\r\n    datasets = dbwriter.get_datasets(project=project)\r\n    for d in datasets[0]:\r\n        if d.dataset_id == 'ET' or d.dataset_id == 'Historical_ET':\r\n            continue\r\n        tables = dbwriter.get_tables(d.dataset_id, project=project)\r\n        for t in tables:\r\n            if t.table_id == 'Lat Long Trial':\r\n                continue\r\n            delete_null_rows(project, d.dataset_id, t.table_id, row_value, start_date, end_date)\r\n\r\n\r\ndef update_irrigation_hours_for_date(project, field_name, logger_name, daily_hours, date):\r\n    # Get gpm and acres\r\n    gpm = Decagon.get_gpm(field_name, logger_name)\r\n    acres = Decagon.get_acres(field_name, logger_name)\r\n    field_db = dbwriter.remove_unwanted_chars_for_db_dataset(field_name)\r\n    # Calculate daily hours, flow, and inches using Switch Data\r\n    switch_data = daily_hours * 60\r\n    daily_inches = round((switch_data * float(gpm)) / (float(acres) * 27154), 1)\r\n    # Set up and run Query\r\n    dml = \"UPDATE `\" + str(project) + \".\" + str(field_db) + \".\" + str(logger_name) + \"`\" \\\r\n          + \" SET daily_switch = \" + str(switch_data) + \", daily_hours = \" + str(daily_hours) + \", daily_inches = \" \\\r\n          + str(daily_inches) + \" WHERE date = '\" + str(date) + \"'\"\r\n    # print(dml)\r\n    dbwriter.run_dml(dml, project=project)\r\n    print(\"Done Updating Irr. Hours\")\r\n\r\n# update_irrigation_hours_for_date('stomato-2023', 'Lucero Mandeville17 I J', 'LM-17J-S', 4.2, '2023-08-08')\r\n# update_irrigation_hours_for_date('stomato-2023', 'Lucero Mandeville17 I J', 'LM-17J-S', 5.1, '2023-08-10')\r\n# update_irrigation_hours_for_date('stomato-2023', 'Lucero Mandeville17 I J', 'LM-17J-S', 3.5, '2023-08-12')\r\n# update_irrigation_hours_for_date('stomato-2023', 'Lucero Mandeville17 I J', 'LM-17J-S', 13.8, '2023-08-15')\r\n# update_irrigation_hours_for_date('stomato-2023', 'Lucero Mandeville17 I J', 'LM-17J-S', 6.8, '2023-08-16')\r\n\r\ndef update_irrigation_inches_for_whole_table(project, field_name, logger_name):\r\n    # Useful when gpm or acres change and you want to recalculate all the inches\r\n    gpm = Decagon.get_gpm(field_name, logger_name)\r\n    acres = Decagon.get_acres(field_name, logger_name)\r\n    field_db = dbwriter.remove_unwanted_chars_for_db_dataset(field_name)\r\n    # Calculate inches using Switch Data\r\n    # daily_inches = round((switch_data * float(gpm)) / (float(acres) * 27154), 1)\r\n    # Set up and run Query\r\n    dml = f\"UPDATE `{str(project)}.{str(field_db)}.{str(logger_name)}`\" \\\r\n          + f\" SET  daily_inches = ((daily_switch * {float(gpm)}) / ({float(acres)} * 27154))\" \\\r\n          + f\" WHERE logger_id is not null\"\r\n    # print(dml)\r\n    dbwriter.run_dml(dml, project=project)\r\n    print(\"Done Updating Irr. Inches\")\r\n# update_irr_inches_for_date('stomato-2023', 'Lucero Dillard RoadD4', 'DI-D4-W')\r\n\r\n\r\ndef update_irrigation_hours_for_date_range(project, field_name, logger_name, daily_hours, start_date, end_date):\r\n    # Get gpm and acres\r\n    gpm = Decagon.get_gpm(field_name, logger_name)\r\n    acres = Decagon.get_acres(field_name, logger_name)\r\n\r\n    field_db = dbwriter.remove_unwanted_chars_for_db_dataset(field_name)\r\n\r\n    # Turn date string into datetimes\r\n    start_date_dt = datetime.strptime(start_date, \"%Y-%m-%d\")\r\n    end_date_dt = datetime.strptime(end_date, \"%Y-%m-%d\")\r\n    delta = end_date_dt - start_date_dt + timedelta(days=1)\r\n\r\n    # Calculate daily hours, flow, and inches using Switch Data\r\n    switch_data = daily_hours * 60\r\n    flow = round((switch_data * float(gpm)) / float(acres))\r\n    daily_inches = flow / 27154\r\n    # Set up and run Query\r\n    dml = \"UPDATE `\" + str(project) + \".\" + str(field_db) + \".\" + str(logger_name) + \"`\" \\\r\n          + \" SET daily_switch = \" + str(switch_data) + \", daily_hours = \" + str(daily_hours) + \", daily_inches = \" \\\r\n          + str(daily_inches) + \" WHERE date BETWEEN DATE('\" + start_date + \"') AND DATE('\" + end_date + \"') \"\r\n    dbwriter.run_dml(dml, project=project)\r\n    print(\"Done Updating Irr. Hours\")\r\n\r\ndef update_eto_etc(project, field_name, logger_name, list_etos, start_date, end_date):\r\n    field_name = dbwriter.remove_unwanted_chars_for_db_dataset(field_name)\r\n    list_etos = list_etos\r\n\r\n    # Setup dataset_id from passed in field_name and logger_name parameters\r\n    dataset_id = project + '.' + field_name + '.' + logger_name\r\n    dataset_id = \"`\" + dataset_id + \"`\"\r\n\r\n    # Turn date string into datetimes\r\n    start_date_dt = datetime.strptime(start_date, \"%Y-%m-%d\")\r\n    end_date_dt = datetime.strptime(end_date, \"%Y-%m-%d\")\r\n    delta = end_date_dt - start_date_dt + timedelta(days=1)\r\n\r\n    # check that length of listETos = endDate - startDate\r\n    if len(list_etos) == delta.days:\r\n        for eto in list_etos:\r\n            # startDateS = datetime.strftime(start_date_dt, \"%Y-%m-%d\")\r\n            startDateS = '{0}-{1}-{2}'.format(start_date_dt.year, start_date_dt.month, start_date_dt.day)\r\n            startDateS = \"'\" + startDateS + \"'\"\r\n            kc_dml_statement = \"SELECT kc FROM \" + dataset_id + ' WHERE date = ' + startDateS\r\n            print('Getting kc from DB')\r\n            kc_response = dbwriter.run_dml(kc_dml_statement, project=project)\r\n            kc = 0\r\n            for e in kc_response:\r\n                kc = e[\"kc\"]\r\n                print(kc)\r\n            print(' Done. Got kc: ' + str(kc))\r\n            etc = kc * eto\r\n            print()\r\n            print('etc: ' + str(etc))\r\n            print('Updating etc and eto in DB')\r\n            etc_dml_statement = \"UPDATE \" + dataset_id + ' SET etc = ' + str(etc) + ', eto = ' + str(\r\n                eto) + ' WHERE date = ' + startDateS\r\n            dbwriter.run_dml(etc_dml_statement, project=project)\r\n            start_date_dt = start_date_dt + timedelta(days=1)\r\n            print()\r\n\r\n            if start_date_dt == end_date_dt + timedelta(1):\r\n                print('Start date = End date')\r\n                break\r\n\r\n\r\ndef update_kcs(project, field_name, logger_name, list_kcs, start_date, end_date):\r\n    field_name = dbwriter.remove_unwanted_chars_for_db_dataset(field_name)\r\n\r\n    # Setup dataset_id from passed in field and logger_id parameters\r\n    dataset_id = project + '.' + field_name + '.' + logger_name\r\n    dataset_id = \"`\" + dataset_id + \"`\"\r\n\r\n    # Turn date string into datetimes\r\n    start_date_dt = datetime.strptime(start_date, \"%Y-%m-%d\")\r\n    end_date_dt = datetime.strptime(end_date, \"%Y-%m-%d\")\r\n    delta = end_date_dt - start_date_dt + timedelta(days=1)\r\n\r\n    # check that length of listETos = endDate - startDate\r\n    if len(list_kcs) == delta.days:\r\n        for kc in list_kcs:\r\n            # startDateS = datetime.strftime(start_date_dt, \"%Y-%m-%d\")\r\n            startDateS = '{0}-{1}-{2}'.format(start_date_dt.year, start_date_dt.month, start_date_dt.day)\r\n            startDateS = \"'\" + startDateS + \"'\"\r\n\r\n            print('Updating kc DB for date: ' + startDateS)\r\n            kc_dml_statement = \"UPDATE \" + dataset_id + ' SET kc = ' + str(kc) + ' WHERE date = ' + startDateS\r\n            dbwriter.run_dml(kc_dml_statement, project=project)\r\n            start_date_dt = start_date_dt + timedelta(days=1)\r\n            print()\r\n\r\n            if start_date_dt == end_date_dt + timedelta(1):\r\n                print('Start date = End date')\r\n                break\r\n\r\n\r\ndef update_values_for_date_range(project, field_name, logger_name, value_name, values_list, start_date, end_date):\r\n    field_name = dbwriter.remove_unwanted_chars_for_db_dataset(field_name)\r\n\r\n    # Setup dataset_id from passed in field and logger_id parameters\r\n    dataset_id = project + '.' + field_name + '.' + logger_name\r\n    dataset_id = \"`\" + dataset_id + \"`\"\r\n\r\n    # Turn date string into datetimes\r\n    start_date_dt = datetime.strptime(start_date, \"%Y-%m-%d\")\r\n    end_date_dt = datetime.strptime(end_date, \"%Y-%m-%d\")\r\n    delta = end_date_dt - start_date_dt + timedelta(days=1)\r\n\r\n    # check that length of listETos = endDate - startDate\r\n    if len(values_list) == delta.days:\r\n        for val in values_list:\r\n            # start_date_s = datetime.strftime(start_date_dt, \"%Y-%m-%d\")\r\n            start_date_s = '{0}-{1}-{2}'.format(start_date_dt.year, start_date_dt.month, start_date_dt.day)\r\n            start_date_s = \"'\" + start_date_s + \"'\"\r\n\r\n            print('Updating val DB for date: ' + start_date_s)\r\n            dml = \"UPDATE \" + dataset_id + ' SET ' + str(value_name) + ' = ' + str(\r\n                val) + ' WHERE date = ' + start_date_s\r\n            dbwriter.run_dml(dml, project=project)\r\n            start_date_dt = start_date_dt + timedelta(days=1)\r\n            print()\r\n\r\n            if start_date_dt == end_date_dt + timedelta(1):\r\n                print('Start date = End date')\r\n                break\r\n    else:\r\n        print('Values and dates dont match')\r\n        print('Values: ' + str(len(values_list)))\r\n        print(\"Days: \" + str(delta.days))\r\n\r\n\r\ndef copy_values_from_table_to_table():\r\n    table = '`stomato-permanents.Riley_Chaney_Farms16.RC-16-E`'\r\n    dml = f\"SELECT date, daily_inches, daily_hours, daily_switch, canopy_temperature, ambient_temperature, vpd, psi, sdd, rh, lowest_ambient_temperature FROM {table} WHERE date > '2024-01-01' ORDER BY date ASC\"\r\n    result = dbwriter.run_dml(dml, project='stomato')\r\n    for e in result:\r\n        date = e[\"date\"]\r\n        if date is not None:\r\n            inches = e[\"daily_inches\"]\r\n            hours = e[\"daily_hours\"]\r\n            switch = e[\"daily_switch\"]\r\n\r\n\r\n            dml = \"UPDATE `stomato-permanents.Riley_Chaney_Farms16.RC-16-W` \" \\\r\n                  + \" SET daily_switch = \" + str(switch) + \\\r\n                  \", daily_hours = \" + str(hours) + \\\r\n                  \", daily_inches = \" + str(inches) + \\\r\n                  \" WHERE date = '\" + str(date) + \"'\"\r\n            dbwriter.run_dml(dml, project='stomato')\r\n    print('done')\r\n\r\n\r\ndef copy_gdd_values_from_temp_table_to_table(project, field, original_table, temp_table):\r\n    dml_statement = \"MERGE `\" + project + \".\" + field + \".\" + original_table + \"` T \" \\\r\n                    + \"USING `\" + project + \".\" + field + \".\" + temp_table + \"` S \" \\\r\n                    + \"ON T.date = S.date \" \\\r\n                    + \"WHEN MATCHED THEN \" \\\r\n                    + \"UPDATE SET \" \\\r\n                      \"lowest_ambient_temperature = s.lowest_ambient_temperature, \" \\\r\n                      \"gdd = s.gdd,\" \\\r\n                      \"crop_stage = s.crop_stage, \" \\\r\n                      \"id = s.id, \" \\\r\n                      \"planting_date = s.planting_date\"\r\n\r\n    result = dbwriter.run_dml(dml_statement, project=project)\r\n    print('done')\r\n\r\n\r\ndef copy_vp4_vals_from_table_to_table(project, fieldName, source, target, date):\r\n    fieldName = dbwriter.remove_unwanted_chars_for_db_dataset(fieldName)\r\n    # print(date)\r\n    dml = f\"SELECT date, ambient_temperature, rh, vpd FROM `{project}.{fieldName}.{source}` ORDER BY date ASC\"\r\n    print(dml)\r\n    result = dbwriter.run_dml(dml, project=project)\r\n    for e in result:\r\n        # print(e)\r\n        dbDate = e[\"date\"]\r\n        # print(dbDate)\r\n        if str(dbDate) == date:\r\n            ambient_temperature = e[\"ambient_temperature\"]\r\n            rh = e[\"rh\"]\r\n            vpd = e[\"vpd\"]\r\n\r\n            dml = \"UPDATE `\" + project + \".\" + fieldName + \".\" + target + \"`\" \\\r\n                  + \" SET ambient_temperature = \" + str(ambient_temperature) + \\\r\n                  \", rh = \" + str(rh) + \\\r\n                  \", vpd = \" + str(vpd) + \\\r\n                  \" WHERE date = '\" + str(date) + \"'\"\r\n            print(dml)\r\n            dbwriter.run_dml(dml, project=project)\r\n    print('done updating VP4 Data')\r\n\r\n\r\ndef merge_table_into_table_updating_some_values(main_dataset_id: str, merge_dataset_id: str):\r\n    #\r\n    # dml = \"MERGE \" + dataset_id + \" T \" \\\r\n    #                         + \"USING \" + et_id + \" S \" \\\r\n    #                         + \"ON (T.date = S.date AND T.eto IS NULL)  \" \\\r\n    #                         + \"WHEN MATCHED THEN \" \\\r\n    #                         + \"UPDATE SET eto = s.eto, etc = s.eto * t.kc, et_hours = ROUND(\" + str(\r\n    #             et_hours_pending_etc_mult) + \" * s.eto * t.kc)\"\r\n\r\n    # date, daily_inches, daily_hours, daily_switch, canopy_temperature, ambient_temperature, vpd, psi, sdd, rh, lowest_ambient_temperature\r\n\r\n    dml = \"MERGE `\" + main_dataset_id + \"` T \" \\\r\n        + \"USING `\" + merge_dataset_id + \"` S \" \\\r\n        + \"ON (t.date = s.date AND T.date > '2024-01-01') \" \\\r\n        + \"WHEN MATCHED THEN \" \\\r\n        + \"UPDATE SET \" \\\r\n          \"daily_inches = s.daily_inches, \" \\\r\n          \"daily_hours = s.daily_hours,\" \\\r\n          \"daily_switch = s.daily_switch, \" \\\r\n          \"canopy_temperature = s.canopy_temperature, \" \\\r\n          \"ambient_temperature = s.ambient_temperature, \" \\\r\n          \"vpd = s.vpd, \" \\\r\n          \"psi = s.psi, \" \\\r\n          \"sdd = s.sdd, \" \\\r\n          \"rh = s.rh, \" \\\r\n          \"lowest_ambient_temperature = s.lowest_ambient_temperature\"\r\n    result = dbwriter.run_dml(dml, project='stomato-permanents')\r\n\r\n# merge_table_into_table_updating_some_values('stomato-permanents.Barrios_Farms22.BF-22-NW2', 'stomato-permanents.Barrios_Farms22.BF-22-NW')\r\n\r\n\r\n\r\ndef daterange(start_date, end_date):\r\n    for n in range(int((end_date - start_date).days)):\r\n        yield start_date + timedelta(n)\r\n\r\n\r\ndef keep_db_days(field_name, startDate, endDate):\r\n    field_name = dbwriter.remove_unwanted_chars_for_db_dataset(field_name)\r\n\r\n    growers = Decagon.open_pickle()\r\n    for g in growers:\r\n        for f in g.fields:\r\n            if f.name == field_name:\r\n                for log in f.loggers:\r\n                    logger_id = log.id\r\n                    # Setup dataset_id from passed in field and logger_id parameters\r\n                    project = dbwriter.get_db_project(log.crop_type)\r\n                    dataset_id = project + '.' + field_name + '.' + logger_id\r\n                    dataset_id = \"`\" + dataset_id + \"`\"\r\n\r\n                    # Keep dates specified by user Start and End Date\r\n                    val_dml_statement = \"CREATE OR REPLACE TABLE \" + dataset_id + \" AS \" + \"SELECT * FROM \" + dataset_id \\\r\n                                        + \"WHERE date BETWEEN DATE('\" + startDate + \"') AND DATE('\" + endDate + \"') \"\r\n                    print(val_dml_statement)\r\n                    dbwriter.run_dml(val_dml_statement, project=project)\r\n\r\n\r\ndef delete_and_update_db(grower, field, db_end_date, start_date):\r\n    keep_db_days(field, '2021-01-01', db_end_date)\r\n    Decagon.get_previous_data_field(grower, field, start_date, write_to_sheet=True, write_to_db=True)\r\n\r\n\r\ndef delete_and_update_db_grower(grower, db_end_date, start_date):\r\n    growers = Decagon.open_pickle()\r\n    for g in growers:\r\n        if g.name == grower:\r\n            print(\"Found grower, these are his fields: \")\r\n            for f in g.fields:\r\n                field = f.name\r\n                print(f.name)\r\n                keep_db_days(field, '2021-01-01', db_end_date)\r\n                # Todo pass in field and grower objects to parameters\r\n                Decagon.get_previous_data_field(grower, field, start_date, write_to_sheet=True, write_to_db=True)\r\n\r\n\r\ndef remove_psi_specific(project, field_name, logger_name, start_date, end_date):\r\n    field_name = dbwriter.remove_unwanted_chars_for_db_dataset(field_name)\r\n    dataset_id = project + '.' + field_name + '.' + logger_name\r\n    dataset_id = \"`\" + dataset_id + \"`\"\r\n    val_dml_statement = \"Update \" + dataset_id + \" Set \" + \" psi = null, sdd = null, canopy_temperature = null \" \\\r\n                        + \"WHERE date BETWEEN DATE('\" + start_date + \"') AND DATE('\" + end_date + \"') \"\r\n    # print(val_dml_statement)\r\n    print(\"Removing PSI from field pages\")\r\n    dbwriter.run_dml(val_dml_statement, project=project)\r\n\r\n\r\ndef remove_psi(field_name_pickle, start_date, end_date, portal_year):\r\n    field_name = dbwriter.remove_unwanted_chars_for_db_dataset(field_name_pickle)\r\n    growers = Decagon.open_pickle()\r\n    for g in growers:\r\n        for f in g.fields:\r\n            if f.name == field_name_pickle:\r\n                grower_name = dbwriter.remove_unwanted_chars_for_db_dataset(g.name)\r\n                print(\"Removing PSI from field pages\")\r\n                for log in f.loggers:\r\n                    logger_name = log.name\r\n                    project = dbwriter.get_db_project(log.crop_type)\r\n                    dataset_id = project + '.' + field_name + '.' + logger_name\r\n                    dataset_id = \"`\" + dataset_id + \"`\"\r\n                    val_dml_statement = \"Update \" + dataset_id + \" Set \" + \" psi = null, sdd = null, canopy_temperature = null \" \\\r\n                                        + \"WHERE date BETWEEN DATE('\" + start_date + \"') AND DATE('\" + end_date + \"') \"\r\n                    # print(val_dml_statement)\r\n                    dbwriter.run_dml(val_dml_statement, project=project)\r\n                print(\"Removing PSI from portal\")\r\n                dataset_id_portal_field_averages = 'growers-' + portal_year + '.' + grower_name + '.field_averages'\r\n                dataset_id_portal_field_averages = \"`\" + dataset_id_portal_field_averages + \"`\"\r\n                dataset_id_portal_loggers = 'growers-' + portal_year + '.' + grower_name + '.loggers'\r\n                dataset_id_portal_loggers = \"`\" + dataset_id_portal_loggers + \"`\"\r\n\r\n                val_dml_statement = \"Update \" + dataset_id_portal_field_averages + \" Set\" + \" si_num = null, si_desc = null \" \\\r\n                                    + \"Where field = '\" + f.nickname + \"'\"\r\n                print(\"Removing PSI from field_averages\")\r\n                dbwriter.run_dml(val_dml_statement, project='growers-' + portal_year)\r\n                val_dml_statement = \"Update \" + dataset_id_portal_loggers + \" Set\" + \" si_num = null, si_desc = null \" \\\r\n                                    + \"Where field = '\" + f.nickname + \"'\"\r\n                print(\"Removing PSI from loggers\")\r\n                dbwriter.run_dml(val_dml_statement, project='growers-' + portal_year)\r\n\r\n\r\ndef update_missing_et_data(logger):\r\n    field_name = dbwriter.remove_unwanted_chars_for_db_dataset(logger.field.name)\r\n    project = dbwriter.get_db_project(logger.crop_type)\r\n    dataset_id = project + \".\" + field_name + \".\" + logger.name\r\n    et_id = \"stomato-info.ET.\" + str(logger.field.cimis_station)\r\n\r\n    if isinstance(logger.irrigation_set_acres, str):\r\n        acres = float(logger.irrigation_set_acres.replace(',', ''))\r\n    elif isinstance(logger.irrigation_set_acres, int):\r\n        acres = float(logger.irrigation_set_acres)\r\n    elif isinstance(logger.irrigation_set_acres, float):\r\n        acres = logger.irrigation_set_acres\r\n    else:\r\n        acres = 0\r\n    if isinstance(logger.gpm, str):\r\n        gpm = float(logger.gpm.replace(',', ''))\r\n    elif isinstance(logger.gpm, int):\r\n        gpm = float(logger.gpm)\r\n    elif isinstance(logger.gpm, float):\r\n        gpm = logger.gpm\r\n    else:\r\n        gpm = 0\r\n\r\n    if gpm != 0:\r\n        et_hours_pending_etc_mult = ((449 * acres) / (gpm * 0.85))\r\n\r\n        print(\"Updating eto data for table: \" + dataset_id)\r\n        print(\" from\")\r\n        print(\"ET table: \" + et_id)\r\n        # Use when eto is NULL\r\n        # dml_statement = \"MERGE `\" + dataset_id + \"` T \" \\\r\n        #                 + \"USING `\" + et_id + \"` S \" \\\r\n        #                 + \"ON T.date = S.date \" \\\r\n        #                 + \"WHEN MATCHED AND t.eto is NULL THEN \" \\\r\n        #                 + \"UPDATE SET eto = s.eto, etc = s.eto * t.kc, et_hours = ROUND(\" + str(\r\n        #     et_hours_pending_etc_mult) + \" * s.eto * t.kc)\"\r\n\r\n        # Use when eto is not NULL\r\n\r\n        dml_statement = \"MERGE `\" + dataset_id + \"` T \" \\\r\n                        + \"USING `\" + et_id + \"` S \" \\\r\n                        + \"ON (T.date = S.date AND T.eto IS NULL)  \" \\\r\n                        + \"WHEN MATCHED THEN \" \\\r\n                        + \"UPDATE SET eto = s.eto, etc = s.eto * t.kc, et_hours = ROUND(\" + str(\r\n            et_hours_pending_etc_mult) + \" * s.eto * t.kc)\"\r\n        # print(dml_statement)\r\n        dbwriter.run_dml(dml_statement, project=project)\r\n\r\n\r\ndef update_missing_et_hours(logger):\r\n    field_name = dbwriter.remove_unwanted_chars_for_db_dataset(logger.field.name)\r\n    project = dbwriter.get_db_project(logger.crop_type)\r\n    dataset_id = project + \".\" + field_name + \".\" + logger.name\r\n\r\n    if isinstance(logger.irrigation_set_acres, str):\r\n        acres = float(logger.irrigation_set_acres.replace(',', ''))\r\n    elif isinstance(logger.irrigation_set_acres, int):\r\n        acres = float(logger.irrigation_set_acres)\r\n    else:\r\n        acres = 0\r\n    if isinstance(logger.gpm, str):\r\n        gpm = float(logger.gpm.replace(',', ''))\r\n    elif isinstance(logger.gpm, int):\r\n        gpm = float(logger.gpm)\r\n    else:\r\n        gpm = 0\r\n\r\n    if gpm != 0:\r\n        et_hours_pending_etc_mult = ((449 * acres) / (gpm * 0.85))\r\n\r\n        print(\"Updating et_hours data for table: \" + dataset_id)\r\n\r\n        dml_statement = \"UPDATE `\" + dataset_id + \"` SET et_hours = ROUND(\" + str(\r\n            et_hours_pending_etc_mult) + \" * eto * kc) \" + \" WHERE et_hours is NULL\"\r\n\r\n        dbwriter.run_dml(dml_statement, project=project)\r\n    else:\r\n        print('Cannot calculate ET Hours. GPM is: ' + str(gpm))\r\n\r\n\r\ndef add_column_to_db_specific_field(field_name: str, column_name_and_type_dict: dict, project='stomato'):\r\n    \"\"\"\r\n\r\n    :param field_name:\r\n    :param column_name_and_type_dict: Dict[column_name] = column_type\r\n    :return:\r\n    \"\"\"\r\n    fieldName = dbwriter.remove_unwanted_chars_for_db_dataset(field_name)\r\n    growers = Decagon.open_pickle()\r\n    for g in growers:\r\n        for f in g.fields:\r\n            if f.name == field_name:\r\n                for log in f.loggers:\r\n                    logger_id = log.id\r\n                    for val in column_name_and_type_dict:\r\n                        try:\r\n                            dbwriter.add_new_column_to_table(fieldName, logger_id, val, column_name_and_type_dict[val],\r\n                                                             project=project)\r\n                        except:\r\n                            print(\"Exception\")\r\n\r\n\r\ndef add_column_to_db_logger_table(column_name_and_type_dict: dict, project='stomato'):\r\n    \"\"\"\r\n\r\n    :param column_name_and_type_dict: Dict[column_name] = column_type\r\n    :return:\r\n    \"\"\"\r\n    growers = Decagon.open_pickle()\r\n    for g in growers:\r\n        for f in g.fields:\r\n            fieldName = dbwriter.remove_unwanted_chars_for_db_dataset(f.name)\r\n            print(fieldName)\r\n            for log in f.loggers:\r\n                logger_id = log.id\r\n                for val in column_name_and_type_dict:\r\n                    try:\r\n                        dbwriter.add_new_column_to_table(fieldName, logger_id, val, column_name_and_type_dict[val],\r\n                                                         project=project)\r\n                    except:\r\n                        print(\"Field:\" + fieldName + \" already has ET Hours\")\r\n\r\n\r\ndef add_column_to_db_grower_portal_logger_table(column_name_and_type_dict: dict, project='growers-2024'):\r\n    \"\"\"\r\n\r\n    :param column_name_and_type_dict: Dict[column_name] = column_type\r\n    :return:\r\n    \"\"\"\r\n    growers = Decagon.open_pickle()\r\n    for g in growers:\r\n        # if g.name == 'Surjit Chahal':\r\n            dataset_name = dbwriter.remove_unwanted_chars_for_db_dataset(g.name)\r\n            table_name = 'loggers'\r\n\r\n            print(dataset_name, table_name)\r\n\r\n            for val in column_name_and_type_dict:\r\n                try:\r\n                    dbwriter.add_new_column_to_table(dataset_name, table_name, val, column_name_and_type_dict[val],\r\n                                                     project=project)\r\n                except:\r\n                    print(f'Error when trying to add column to table {dataset_name}.{table_name}')\r\n\r\n\r\n# db_column = {\r\n#     'location': 'STRING'\r\n# }\r\n# add_column_to_db_grower_portal_logger_table(db_column)\r\n\r\ndef remove_duplicate_rows(project, dataset, table, rowName):\r\n    dmlStatement = \"create or replace table `\" + str(project) + \".\" + str(dataset) + \".\" + str(table) + \"` as ( \\\r\n    select * except(row_num) from (SELECT *, ROW_NUMBER() OVER (PARTITION BY \" + str(rowName) + \" ORDER BY \" + str(\r\n        rowName) + \" desc) row_num \\\r\n    FROM `\" + project + \".\" + str(dataset) + \".\" + str(table) + \"`) t \\\r\n    WHERE row_num=1)\"\r\n\r\n    dbwriter.run_dml(dmlStatement, project=project)\r\n\r\n\r\ndef removeDuplicateET():\r\n    cimisStations = Decagon.get_all_current_cimis_stations()\r\n    project = 'stomato-info'\r\n    dataset = \"ET\"\r\n    rowName = \"date\"\r\n    for station in cimisStations:\r\n        print(\"Removing dupes in station \" + str(station))\r\n        remove_duplicate_rows(project, dataset, station, rowName)\r\n\r\n\r\ndef remove_double_data_late_hours(field_name, logger_name):\r\n    field_name = dbwriter.remove_unwanted_chars_for_db_dataset(field_name)\r\n    growers = Decagon.open_pickle()\r\n    dataset_id = \"stomato.\" + field_name + \".\" + logger_name\r\n    for g in growers:\r\n        for f in g.fields:\r\n            if f.name == field_name:\r\n                for log in f.loggers:\r\n                    if logger_name == log.name:\r\n                        project = dbwriter.get_db_project(log.crop_type)\r\n                        dml_statement = \"DELETE `\" + project + \".\" + dataset_id + \"` where time = '11:00 PM' or time = '10:00 PM'\"\r\n                        dbwriter.run_dml(dml_statement, project=project)\r\n\r\n\r\ndef update_kc_values_with_a_max_including_etc_etc_hours(logger):\r\n    if isinstance(logger.irrigation_set_acres, str):\r\n        acres = float(logger.irrigation_set_acres.replace(',', ''))\r\n    elif isinstance(logger.irrigation_set_acres, int):\r\n        acres = float(logger.irrigation_set_acres)\r\n    else:\r\n        acres = 0\r\n    if isinstance(logger.gpm, str):\r\n        gpm = float(logger.gpm.replace(',', ''))\r\n    elif isinstance(logger.gpm, int):\r\n        gpm = float(logger.gpm)\r\n    else:\r\n        gpm = 0\r\n\r\n    if gpm != 0:\r\n        et_hours_pending_etc_mult = ((449 * acres) / (gpm * 0.85))\r\n\r\n        field_name = dbwriter.remove_unwanted_chars_for_db_dataset(logger.field.name)\r\n        project = dbwriter.get_db_project(logger.crop_type)\r\n        dataset_id = project + \".\" + field_name + \".\" + logger.id\r\n        print(\"Updating field: {0} and logger {1}\".format(field_name, logger.id))\r\n        dmlStatement = \"UPDATE `\" + dataset_id + \"` as t \\\r\n        SET t.kc = 1.1, t.etc = t.eto * 1.1, et_hours = ROUND(\" + str(et_hours_pending_etc_mult) + \" * eto * kc) \\\r\n        WHERE t.kc > 1.1\"\r\n        dbwriter.run_dml(dmlStatement, project=project)\r\n\r\n\r\ndef update_kc_values_with_a_max(logger):\r\n    field_name = dbwriter.remove_unwanted_chars_for_db_dataset(logger.field.name)\r\n    project = dbwriter.get_db_project(logger.crop_type)\r\n    dataset_id = project + \".\" + field_name + \".\" + logger.name\r\n    print(\"Updating field: {0} and logger {1}\".format(field_name, logger.name))\r\n    dml_statement = \"UPDATE `\" + dataset_id + \"` as t \\\r\n    SET t.kc = 1.1, t.etc = t.eto * 1.1 \\\r\n    WHERE t.kc > 1.1\"\r\n    dbwriter.run_dml(dml_statement, project=project)\r\n\r\n\r\ndef delete_last_day(project: str, field_name: str, logger_name: str, day=''):\r\n    field_name = dbwriter.remove_unwanted_chars_for_db_dataset(field_name)\r\n    if day == '':\r\n        today = date.today() - timedelta(days=1)\r\n    else:\r\n        today = day\r\n    dataset_id = project + \".\" + field_name + \".\" + logger_name\r\n    dmlStatement = \"Delete `\" + dataset_id + \"` where date = '\" + str(today) + \"'\"\r\n    print(dmlStatement)\r\n    try:\r\n        dbwriter.run_dml(dmlStatement, project=project)\r\n    except:\r\n        print(\"Field Not Found. Please Try With a Different Name\")\r\n\r\n\r\ndef delete_et_day(field, date, date2):\r\n    growers = Decagon.open_pickle()\r\n    for g in growers:\r\n        for f in g.fields:\r\n            if f.name == field:\r\n                for l in f.loggers:\r\n                    logger_name = l.name\r\n                    field_name = dbwriter.remove_unwanted_chars_for_db_dataset(field)\r\n                    project = dbwriter.get_db_project(l.crop_type)\r\n                    dataset_id = project + \".\" + field_name + \".\" + logger_name\r\n                    dmlStatement = \"Update `\" + dataset_id + \"` Set eto = null, etc = null, et_hours = null where date between \" + \\\r\n                                   \"date('\" + date + \"') and date('\" + date2 + \"') \"\r\n                    print(dmlStatement)\r\n                    dbwriter.run_dml(dmlStatement, project=project)\r\n                    # update_missing_et_data(l)\r\n\r\n\r\ndef delete_negative_et(field, date, kc):\r\n    growers = Decagon.open_pickle()\r\n    for g in growers:\r\n        for f in g.fields:\r\n            if f.name == field:\r\n                for l in f.loggers:\r\n                    logger_name = l.name\r\n                    field_name = dbwriter.remove_unwanted_chars_for_db_dataset(field)\r\n                    project = dbwriter.get_db_project(l.crop_type)\r\n                    dataset_id = project + \".\" + field_name + \".\" + logger_name\r\n                    dmlStatement = \"Update `\" + dataset_id + \"` Set eto = null, etc = null, et_hours = null, kc = \" + \\\r\n                                   kc + \" where date = '\" + date + \"'\"\r\n                    print(dmlStatement)\r\n                    dbwriter.run_dml(dmlStatement, project=project)\r\n                    # update_missing_et_data(l)\r\n\r\n\r\ndef get_average_psi_during_growth():\r\n    start_cut = 8\r\n    end_cut = 21\r\n    psi_averages = {}\r\n    datasets = dbwriter.get_datasets()\r\n    for d in datasets[0]:\r\n        if d.dataset_id == 'ET':\r\n            continue\r\n        tables = dbwriter.get_tables(d.dataset_id)\r\n        dataset_psis = []\r\n        for t in tables:\r\n            psi_avg = average_table_psi(d.dataset_id, t.table_id, start_cut, end_cut)\r\n            dataset_psis.append(psi_avg)\r\n        dataset_psi_avg = numpy.mean(dataset_psis)\r\n        psi_averages[d.dataset_id] = dataset_psi_avg\r\n        print()\r\n        print(d.dataset_id, 'psi average:')\r\n        print(dataset_psi_avg)\r\n        print()\r\n\r\n    pprint.pprint(psi_averages)\r\n\r\n\r\ndef all_tables_analysis(crop=None, project='stomato'):\r\n    all_dataset_ats = []\r\n    all_dataset_vpds = []\r\n    all_dataset_rhs = []\r\n    all_dataset_psi = []\r\n\r\n    start_cut = 0\r\n    end_cut = 0\r\n\r\n    field_count = 0\r\n    logger_count = 0\r\n\r\n    if crop is not None:\r\n        fields_with_crop = get_fields_for_crop(crop)\r\n        for field in fields_with_crop:\r\n            field_count = field_count + 1\r\n            for logger in field.loggers:\r\n                logger_count = logger_count + 1\r\n                field_name = dbwriter.remove_unwanted_chars_for_db_dataset(field.name)\r\n                psi_avg = average_table_psi(project, field_name, logger.name, start_cut, end_cut)\r\n                at_avg, vpd_avg, rh_avg = table_analysis(project, field_name, logger.name)\r\n                all_dataset_ats.append(at_avg)\r\n                all_dataset_vpds.append(vpd_avg)\r\n                all_dataset_rhs.append(rh_avg)\r\n                all_dataset_psi.append(psi_avg)\r\n            # time.sleep(5)\r\n    else:\r\n        datasets = dbwriter.get_datasets()\r\n\r\n        for d in datasets[0]:\r\n            if d.dataset_id == 'ET':\r\n                continue\r\n            field_count = field_count + 1\r\n            tables = dbwriter.get_tables(d.dataset_id)\r\n            for t in tables:\r\n                if t.table_id == 'Lat Long Trial':\r\n                    continue\r\n                logger_count = logger_count + 1\r\n                at_avg, vpd_avg, rh_avg = table_analysis(project, d.dataset_id, t.table_id)\r\n                psi_avg = average_table_psi(d.dataset_id, t.table_id, start_cut, end_cut)\r\n                all_dataset_ats.append(at_avg)\r\n                all_dataset_vpds.append(vpd_avg)\r\n                all_dataset_rhs.append(rh_avg)\r\n                all_dataset_psi.append(psi_avg)\r\n\r\n    all_dataset_at_avg = numpy.mean(all_dataset_ats)\r\n    all_dataset_vpd_avg = numpy.mean(all_dataset_vpds)\r\n    all_dataset_rh_avg = numpy.mean(all_dataset_rhs)\r\n    all_dataset_psi_avg = numpy.mean(all_dataset_psi)\r\n    print()\r\n    print('Total fields processed: ', field_count)\r\n    print('Total loggers processed: ', logger_count)\r\n    if crop is not None:\r\n        print('Only showing fields with crop: ', crop)\r\n    print('All Ambient Temp Avg: {}'.format(all_dataset_at_avg))\r\n    print('All VPD Avg: {}'.format(all_dataset_vpd_avg))\r\n    print('All RH Avg: {}'.format(all_dataset_rh_avg))\r\n    print('All PSI Avg: {}'.format(all_dataset_psi_avg))\r\n\r\n\r\ndef get_fields_for_crop(crop):\r\n    count = 0\r\n    fields_with_crop = []\r\n    growers = Decagon.open_pickle()\r\n    for g in growers:\r\n        for f in g.fields:\r\n            if f.loggers[0].crop_type == crop:\r\n                fields_with_crop.append(f)\r\n                count = count + 1\r\n    print('{} total fields for {} found'.format(count, crop))\r\n    return fields_with_crop\r\n\r\n\r\ndef average_table_psi(project, field_name, logger_name, start_cut=0, end_cut=0):\r\n    print('Grabbing psi average: ' + str(field_name) + ' ' + str(logger_name))\r\n    dml = \"SELECT date, psi FROM \" \\\r\n          + \"`\" + project + \".\" + str(field_name) + \".\" + str(logger_name) + \"`\" \\\r\n          + \" WHERE psi is not NULL ORDER BY date ASC\"\r\n    result = dbwriter.run_dml(dml, project=project)\r\n    date_list = []\r\n    psi_list = []\r\n    for e in result:\r\n        date = e[\"date\"]\r\n        psi = e['psi']\r\n\r\n        date_list.append(date)\r\n        psi_list.append(psi)\r\n\r\n    if start_cut != 0 and end_cut != 0:\r\n        new_date_list = date_list[start_cut:-end_cut]\r\n        new_psi_list = psi_list[start_cut:-end_cut]\r\n    else:\r\n        new_date_list = date_list\r\n        new_psi_list = psi_list\r\n    psi_avg = numpy.mean(new_psi_list)\r\n    # print('\\t Logger:',logger)\r\n    # print('\\t Psi avg:',psi_avg)\r\n    print('Analysing: {} - {}'.format(field_name, logger_name))\r\n    print('PSI Avg: ', psi_avg)\r\n\r\n    return psi_avg\r\n\r\n\r\ndef table_analysis(project: str, field: str, logger: str):\r\n    print('Grabbing at, vpd and rh averages: ' + str(field) + ' ' + str(logger))\r\n    dml = \"SELECT date, ambient_temperature, vpd, rh, psi FROM \" \\\r\n          + \"`stomato.\" + str(field) + \".\" + str(logger) + \"`\" \\\r\n          + \" WHERE ambient_temperature is not NULL and vpd is not NULL and rh is not NULL ORDER BY date ASC\"\r\n    result = dbwriter.run_dml(dml, project=project)\r\n    date_list = []\r\n    at_list = []\r\n    vpd_list = []\r\n    rh_list = []\r\n    # psi_list = []\r\n    for e in result:\r\n        date = e[\"date\"]\r\n        at = e['ambient_temperature']\r\n        vpd = e['vpd']\r\n        rh = e['rh']\r\n        # psi = e['psi']\r\n\r\n        date_list.append(date)\r\n        at_list.append(at)\r\n        vpd_list.append(vpd)\r\n        rh_list.append(rh)\r\n        # psi_list.append(psi)\r\n\r\n    at_avg = numpy.mean(at_list)\r\n    vpd_avg = numpy.mean(vpd_list)\r\n    rh_avg = numpy.mean(rh_list)\r\n    # psi_avg = numpy.mean(psi_list)\r\n\r\n    print('Analysing: {} - {}'.format(field, logger))\r\n    print('Ambient Temp Avg: ', at_avg)\r\n    print('VPD Avg: ', vpd_avg)\r\n    print('RH Avg: ', rh_avg)\r\n    # print('PSI Avg: ', psi_avg)\r\n    print()\r\n    # print('\\t Logger:',logger)\r\n    # print('\\t Psi avg:',psi_avg)\r\n    return at_avg, vpd_avg, rh_avg  ##, psi_avg\r\n\r\n\r\ndef get_date_range(year, start_day=1, num_days=365, just_date: bool = False):\r\n    \"\"\" Helper function to generate date range for a given year. \"\"\"\r\n    start_date = datetime(year, 1, 1)\r\n    if just_date:\r\n        start_date = date(year, 1, 1)\r\n    return [start_date + timedelta(days=d) for d in range(start_day - 1, num_days)]\r\n\r\n\r\ndef get_a_logger_for_field(fieldName):\r\n    growers = Decagon.open_pickle()\r\n    for g in growers:\r\n        for f in g.fields:\r\n            if f.name == fieldName:\r\n                for l in f.loggers:\r\n                    return l\r\n\r\n\r\ndef setup_irrigation_scheduling_db(etStation: str, fieldName: str):\r\n    \"\"\"\r\n    Sets up the Irrigation Scheduling Table for a Field in the DB\r\n    :param etStation: ET Statin Number\r\n    :param fieldName: Field Name\r\n    \"\"\"\r\n    print(\"Setting up irrigation scheduling DB table\")\r\n    today = datetime.now()\r\n    previous_year = today.year - 1\r\n    start_date, end_date = datetime(previous_year, 1, 1), datetime(previous_year, 12, 31)\r\n\r\n    # Save Historical ET onto Dict\r\n    field_name = dbwriter.remove_unwanted_chars_for_db_dataset(fieldName)\r\n    historical_et = returnHistoricalETDict(etStation, start_date, end_date)\r\n\r\n    # Load Logger and KC values\r\n    dates_list = get_date_range(previous_year)\r\n    logger = get_a_logger_for_field(fieldName)\r\n    kc_values = logger.get_kc({\"dates\": dates_list})\r\n\r\n    csv_data = {\r\n        \"current_date\": get_date_range(today.year, just_date=True),\r\n        \"historical_eto\": [],\r\n        \"kc\": [],\r\n        \"historical_etc\": [],\r\n        \"historical_hours\": []\r\n    }\r\n\r\n    for index, date in enumerate(dates_list):\r\n        date_only = date.date()  # Convert from (2024, 1, 1, 0, 0) to (2024, 1 ,1)\r\n        eto = historical_et[date_only]\r\n        kc = kc_values['kc'][index]\r\n        if eto is None:\r\n            etc = None\r\n            irrigation_hours = None\r\n        else:\r\n            etc = eto * kc\r\n            irrigation_hours = round((etc * 449 * float(logger.irrigation_set_acres)) / (float(logger.gpm) * 0.85), 0)\r\n\r\n        csv_data[\"historical_eto\"].append(eto)\r\n        csv_data[\"kc\"].append(kc)\r\n        csv_data[\"historical_etc\"].append(etc)\r\n        csv_data[\"historical_hours\"].append(irrigation_hours)\r\n\r\n        # Write CSV and update DB\r\n    Decagon.update_irr_scheduling(field_name + '_Irr_Scheduling', field_name, csv_data, overwrite=True,\r\n                                  logger=logger)\r\n\r\ndef returnHistoricalETDict(etStation: str, start_date: date, end_date: date) -> dict:\r\n    \"\"\"\r\n    #TODO: adjust returnHistoricalETDict for new naming schema of Hist ET tables\r\n    Returns a dictionary with the historical average ET date and value\r\n    :param etStation: ET station\r\n    :param start_date: Start Date\r\n    :param end_date: End Date\r\n    :return: Dictionary of historical average ET date and value\r\n    \"\"\"\r\n    # Returns last year dates and averages of Historical ET into a Dictionary\r\n    project = 'stomato-info'\r\n    et_id = project + \".Historical_ET.\" + etStation\r\n    last_year = \"Year_\" + str(start_date.year)\r\n    dml_statement = \"select \" + last_year + \", Average from \" + et_id + \\\r\n                    \" where \" + last_year + \" between date('\" + str(start_date) + \\\r\n                    \"') and date('\" + str(end_date) + \"') order by \" + last_year\r\n    etValue = dbwriter.return_query_dict(dml_statement, last_year, 'Average', project)\r\n    return etValue\r\n\r\n\r\ndef move_logger_db_info(project:str, field_name:str, new_logger_name:str, old_logger_name:str):\r\n    \"\"\"\r\n    Use if copying data from an old logger to a new logger table\r\n    :param project: Big Query Project\r\n    :param field_name: Field Name\r\n    :param new_logger_name: New Logger Name\r\n    :param old_logger_name: Old Logger Name\r\n    \"\"\"\r\n    field_name = dbwriter.remove_unwanted_chars_for_db_dataset(field_name)\r\n    new_logger_dataset = project + \".\" + field_name + \".\" + new_logger_name\r\n    old_logger_dataset = project + \".\" + field_name + \".\" + old_logger_name\r\n\r\n    print(\"Updating data for table: \" + new_logger_dataset)\r\n    print(\" from\")\r\n    print(\"old logger table: \" + old_logger_dataset)\r\n\r\n    # Use when eto is not NULL\r\n    dml_statement = \"MERGE `\" + new_logger_dataset + \"` T \" \\\r\n                    + \"USING `\" + old_logger_dataset + \"` S \" \\\r\n                    + \"ON T.date = S.date \" \\\r\n                    + \"WHEN NOT MATCHED THEN \" \\\r\n                    + \"INSERT (logger_id, date, time, canopy_temperature, ambient_temperature, vpd, vwc_1, vwc_2, vwc_3, \" \\\r\n                      \"field_capacity, wilting_point,  daily_gallons, daily_switch, daily_hours, daily_pressure, \" \\\r\n                      \"daily_inches, psi, psi_threshold, psi_critical, sdd, rh, eto, kc, etc, et_hours, \" \\\r\n                      \"phase1_adjustment, phase1_adjusted, phase2_adjustment, phase2_adjusted, phase3_adjustment, \" \\\r\n                      \"phase3_adjusted, vwc_1_ec, vwc_2_ec, vwc_3_ec) \" \\\r\n                    + \"Values (S.logger_id, S.date,    S.time, S.canopy_temperature, S.ambient_temperature, S.vpd, S.vwc_1, S.vwc_2, S.vwc_3, \" \\\r\n                      \"S.field_capacity, S.wilting_point,  S.daily_gallons, S.daily_switch, S.daily_hours, S.daily_pressure, \" \\\r\n                      \"S.daily_inches, S.psi, S.psi_threshold, S.psi_critical, S.sdd, S.rh, S.eto, S.kc, S.etc, S.et_hours, \" \\\r\n                      \"S.phase1_adjustment, S.phase1_adjusted, S.phase2_adjustment, S.phase2_adjusted, S.phase3_adjustment, \" \\\r\n                      \"S.phase3_adjusted, S.vwc_1_ec, S.vwc_2_ec, S.vwc_3_ec)\"\r\n\r\n    dbwriter.run_dml(dml_statement, project=project)\r\n\r\n    print(\"Changing logger_id to match new one in old data\")\r\n    dml_statement = (\r\n            \"Update `\" + new_logger_dataset + \"`  set logger_id = '\" + new_logger_name + \"' where logger_id = '\" + old_logger_name + \"'\")\r\n    # print(dml_statement)\r\n    dbwriter.run_dml(dml_statement, project=project)\r\n    print(\"Clean up nulls\")\r\n    dml_statement = (\"Delete `\" + new_logger_dataset + \"` where date is NULL\")\r\n    dbwriter.run_dml(dml_statement, project=project)\r\n\r\n\r\ndef update_field_et(fieldName):\r\n    growers = Decagon.open_pickle()\r\n    for g in growers:\r\n        # if g.name == fieldName:\r\n        for f in g.fields:\r\n            if f.name == fieldName:\r\n                for l in f.loggers:\r\n                    # print(\"doing et\")\r\n                    update_missing_et_data(l)\r\n\r\n\r\ndef update_all_field_et():\r\n    growers = Decagon.open_pickle()\r\n    for g in growers:\r\n        for f in g.fields:\r\n            if f.active:\r\n                for l in f.loggers:\r\n                    try:\r\n                        update_missing_et_data(l)\r\n                    except:\r\n                        print(\"Couldn't update et for field: \", f.name,\r\n                              \"\\n logger: \", l.name)\r\n\r\n\r\ndef update_logger_et(fieldName, loggerName):\r\n    growers = Decagon.open_pickle()\r\n    for g in growers:\r\n        # if g.name == fieldName:\r\n        for f in g.fields:\r\n            if f.name == fieldName:\r\n                for l in f.loggers:\r\n                    if l.name == loggerName:\r\n                        update_missing_et_data(l)\r\n\r\n\r\ndef delete_where_eto_is_null(logger, date1, date2):\r\n    field_name = dbwriter.remove_unwanted_chars_for_db_dataset(logger.field.name)\r\n    project = dbwriter.get_db_project(logger.crop_type)\r\n    dataset = project + \".\" + field_name + \".\" + logger.name\r\n    dml_statement = (\"Delete `\" + dataset + \"` where date between \" + \"date('\" + date1 + \"') and date('\" + date2 + \"') \"\r\n                     + 'and eto is null')\r\n    dbwriter.run_dml(dml_statement, project=project)\r\n    # print(dml_statement)\r\n\r\n\r\ndef update_fc_wp(project, field_name_pickle, logger_name, fc, wp):\r\n    field_name = dbwriter.remove_unwanted_chars_for_db_dataset(field_name_pickle)\r\n    dataset = project + \".\" + field_name + \".\" + logger_name\r\n    dml_statement = (\r\n            \"Update `\" + dataset + \"`  set field_capacity = \" + str(fc) + \", wilting_point = \" + str(wp) +\r\n            \" where date is not null\")\r\n    print(dml_statement)\r\n    dbwriter.run_dml(dml_statement, project=project)\r\n    print('Done updating data studio')\r\n    growers = Decagon.open_pickle()\r\n    for grower in growers:\r\n        for field in grower.fields:\r\n            if field.name == field_name_pickle:\r\n                for logger in field.loggers:\r\n                    if logger.name == logger_name:\r\n                        logger.soil.set_field_capacity_wilting_point(fc, wp)\r\n                        print(\"Updated FC and WP in pickle\")\r\n    Decagon.write_pickle(growers)\r\n\r\n\r\ndef update_portal_image(grower_name, field_name, image_url, portal_year, update_db=True):\r\n    client = DBWriter.grab_bq_client(dbwriter, 'growers-' + portal_year)\r\n    # field.preview_url = imageUrl\r\n    # print(\"Updated image preview in pickle for field: \" + field.name)\r\n    if update_db:\r\n        grower_name_db = DBWriter.remove_unwanted_chars_for_db_dataset(dbwriter, grower_name)\r\n        field_averages_portal_dataset_id = client.project + \".\" + grower_name_db + '.field_averages'\r\n        dml_statement = (\r\n                \"Update `\" + field_averages_portal_dataset_id + \"`  set preview = '\" + image_url +\r\n                \"' where field = '\" + field_name + \"'\")\r\n        print(dml_statement)\r\n        dbwriter.run_dml(dml_statement, project=client.project)\r\n        print('Done updating portal')\r\n    print('Done updating pickle')\r\n\r\n\r\ndef remove_duplicate_data(logger):\r\n    field_name = dbwriter.remove_unwanted_chars_for_db_dataset(logger.field.name)\r\n    project = dbwriter.get_db_project(logger.crop_type)\r\n    dataset = field_name + \".\" + logger.name\r\n    dml_statement = (\"CREATE OR REPLACE TABLE `\" + project + \".\" + dataset +\r\n                     \"`AS SELECT * from ( SELECT *, ROW_NUMBER() OVER (PARTITION BY date order by ambient_temperature DESC) row_number \"\r\n                     \"FROM `\" + dataset + \"`) WHERE row_number = 1 \")\r\n    # print(dml_statement)\r\n    print(\"Removing Duplicate data for field: \", logger.field.name, \"\\n \\t For logger: \", logger.name)\r\n    # After removing duplicates a new column gets added, only way to bypass this is by\r\n    # specifying which columns to select in from clause\r\n    try:\r\n        dbwriter.run_dml(dml_statement, project=project)\r\n        drop_column(project, dataset, \"row_number\")\r\n    except exceptions.BadRequest as err:\r\n        print(\"Bad Request Error: \", err)\r\n\r\n\r\ndef drop_column(project, dataset, column):\r\n    print(\"\\t Dropping extra column\")\r\n    dml_statement = (\"alter table \" + \"`\" + project + \".\" + dataset + \"` drop column \" + column)\r\n    # print(dml_statement)\r\n    try:\r\n        dbwriter.run_dml(dml_statement, project=project)\r\n    except exceptions.BadRequest as err:\r\n        print(\"Bad Request Error: \", err)\r\n\r\n\r\ndef fix_weather_db(field):\r\n    field_name = dbwriter.remove_unwanted_chars_for_db_dataset(field.name)\r\n    project = dbwriter.get_db_project(field.loggers[0].crop_type)\r\n    dataset = field_name + \".weather_forecast`\"\r\n    today = datetime.today() - timedelta(days=1)\r\n    day = str(today.day)\r\n    month = str(today.month)\r\n    year = str(today.year)\r\n    date = year + \"-\" + month + \"-\" + day\r\n    # print(date)\r\n\r\n    dml_statement = (\"Update `\" + project + \".\" + dataset + \" as t\" + \" Set t.order = 99 where t.date < \" + \"'\" + date + \"'\")\r\n    dbwriter.run_dml(dml_statement, project=project)\r\n    print(f\"Finished fixing weather for {field.name}\")\r\n    # print(dml_statement)\r\n\r\n\r\n\r\ndef returnHistoricalData(etStation):\r\n    print(\"Grabbing previous years historical data\")\r\n\r\n    et_id_old = \"stomato.ET.\" + str(etStation)\r\n    etValue = []\r\n    startDate = '2022-01-01'\r\n    endDate = '2022-12-31'\r\n    dml_statement = \"select date, eto from \" + et_id_old + \\\r\n                    \" as t where t.date between date('\" + str(startDate) + \\\r\n                    \"') and date('\" + str(endDate) + \"') order by date asc\"\r\n    etValue2022 = dbwriter.return_query_dict(dml_statement, 'date', 'eto', 'stomato')\r\n    etValue.append(etValue2022)\r\n\r\n    et_id = \"stomato.Historical_ET.\" + str(etStation)\r\n    startDate = '2021-01-01'\r\n    endDate = '2021-12-31'\r\n    dml_statement = \"select Year1, Year1ET from \" + et_id + \\\r\n                    \" where Year1 between date('\" + str(startDate) + \\\r\n                    \"') and date('\" + str(endDate) + \"') order by Year1\"\r\n    etValue2021 = dbwriter.return_query_dict(dml_statement, 'Year1', 'Year1ET', 'stomato')\r\n    etValue.append(etValue2021)\r\n\r\n    startDate = '2020-01-01'\r\n    endDate = '2020-12-31'\r\n    dml_statement = \"select Year2, Year2ET from \" + et_id + \\\r\n                    \" where Year2 between date('\" + str(startDate) + \\\r\n                    \"') and date('\" + str(endDate) + \"') order by Year2\"\r\n    etValue2020 = dbwriter.return_query_dict(dml_statement, 'Year2', 'Year2ET', 'stomato')\r\n    etValue.append(etValue2020)\r\n\r\n    startDate = '2019-01-01'\r\n    endDate = '2019-12-31'\r\n    dml_statement = \"select Year3, Year3ET from \" + et_id + \\\r\n                    \" where Year3 between date('\" + str(startDate) + \\\r\n                    \"') and date('\" + str(endDate) + \"') order by Year3\"\r\n    etValue2019 = dbwriter.return_query_dict(dml_statement, 'Year3', 'Year3ET', 'stomato')\r\n    etValue.append(etValue2019)\r\n\r\n    startDate = '2018-01-01'\r\n    endDate = '2018-12-31'\r\n    dml_statement = \"select Year4, Year4ET from \" + et_id + \\\r\n                    \" where Year4 between date('\" + str(startDate) + \\\r\n                    \"') and date('\" + str(endDate) + \"') order by Year4\"\r\n    etValue2018 = dbwriter.return_query_dict(dml_statement, 'Year4', 'Year4ET', 'stomato')\r\n    etValue.append(etValue2018)\r\n\r\n    # pprint.pprint(etValue[0])\r\n    startDate = '2022-01-01'\r\n    endDate = '2022-12-31'\r\n    start = datetime.strptime(startDate, '%Y-%m-%d').date()\r\n    end = datetime.strptime(endDate, '%Y-%m-%d').date()\r\n    # print(dt)\r\n    # pprint.pprint(etValue[0][start])\r\n    dict2022 = {'Year_2022': [], 'Year_2022_ET': []}\r\n    dict2021 = {'Year_2021': [], 'Year_2021_ET': []}\r\n    dict2020 = {'Year_2020': [], 'Year_2020_ET': []}\r\n    dict2019 = {'Year_2019': [], 'Year_2019_ET': []}\r\n    dict2018 = {'Year_2018': [], 'Year_2018_ET': []}\r\n    dictAverage = {'Average': []}\r\n\r\n    for single_date in daterange(start, end + relativedelta(days=1)):\r\n        etValue2022 = etValue[0][single_date]\r\n        dict2022['Year_2022_ET'].append(etValue2022)\r\n        dict2022['Year_2022'].append(single_date)\r\n        single_date = single_date - relativedelta(years=1)\r\n        etValue2021 = etValue[1][single_date]\r\n        dict2021['Year_2021_ET'].append(etValue2021)\r\n        dict2021['Year_2021'].append(single_date)\r\n        single_date = single_date - relativedelta(years=1)\r\n        etValue2020 = etValue[2][single_date]\r\n        dict2020['Year_2020_ET'].append(etValue2020)\r\n        dict2020['Year_2020'].append(single_date)\r\n        single_date = single_date - relativedelta(years=1)\r\n        etValue2019 = etValue[3][single_date]\r\n        dict2019['Year_2019_ET'].append(etValue2019)\r\n        dict2019['Year_2019'].append(single_date)\r\n        single_date = single_date - relativedelta(years=1)\r\n        etValue2018 = etValue[4][single_date]\r\n        dict2018['Year_2018_ET'].append(etValue2018)\r\n        dict2018['Year_2018'].append(single_date)\r\n        average_list = [etValue2021, etValue2020, etValue2019, etValue2018]\r\n        dictAverage['Average'].append(mean(average_list))\r\n\r\n    return dict2022, dict2021, dict2020, dict2019, dict2018, dictAverage\r\n\r\n\r\ndef return_cimis_data_in_dict(etStation, startDate, endDate, dateKey, valueKey):\r\n    \"\"\"\r\n\r\n    :param etStation: CIMIS ET station number\r\n    :param startDate: start date of cimis data\r\n    :param endDate: end date of cimis data\r\n    :param dateKey: the name of key that will be containing the ET dates\r\n    :param valueKey: the name of the key that will be containing the ET values\r\n    :return:\r\n    \"\"\"\r\n    cimis = CIMIS()\r\n    etos = cimis.get_eto(targets=[etStation], start_date=startDate, end_date=endDate)\r\n\r\n    try:\r\n        if etos is None:\r\n            print(\"ETo is none, Issue with API Call\")\r\n            return None\r\n        elif len(etos['Data']['Providers'][0]['Records']) == 0:\r\n            print('ETo call is returning blank list for values. Usually indicates station is inactive')\r\n            print(f'Station: {etStation}')\r\n            return None\r\n    except Exception as error:\r\n        print('ERROR in grabbing actual eto values from API return')\r\n        print(error)\r\n\r\n    etDict = {dateKey: [],\r\n              valueKey: []}\r\n\r\n    if etos['Data']['Providers'][0]['Records']:\r\n        cimis_et_data = etos['Data']['Providers'][0]['Records']\r\n        for single_et_data_point in cimis_et_data:\r\n            # Need to ignore extra day since it only happens once every 4 years\r\n            if single_et_data_point['Date'][-5:] == '02-29':\r\n                continue\r\n            else:\r\n                etDict[dateKey].append(single_et_data_point['Date'])\r\n                etDict[valueKey].append(single_et_data_point['DayEto']['Value'])\r\n    return etDict\r\n\r\n\r\ndef return_if_et_table_found(etStation):\r\n    \"\"\"\r\n\r\n    :param etStation: CIMIS ET station number\r\n    :return: returns True if ET station was found in Historical_ET dataset\r\n    \"\"\"\r\n    tables = dbwriter.get_tables(\"Historical_ET\", project=\"stomato-info\")\r\n    tableFound = False\r\n\r\n    for table in tables:\r\n        # print(table.table_id)\r\n        if table.table_id == str(etStation):\r\n            # print(table.table_id)\r\n            tableFound = True\r\n\r\n    return tableFound\r\n\r\n\r\ndef fix_irr_inches_for_all_fields(start_date, end_date):\r\n    \"\"\"\r\n\r\n    :param start_date: date you want fixes to start\r\n    :param end_date: date you want fixes to end\r\n    \"\"\"\r\n    growers = Decagon.open_pickle()\r\n    for grower in growers:\r\n        for field in grower.fields:\r\n            if field.active:\r\n                print(f\"Fixing Irrigation for Field: {field.name}\")\r\n                for logger in field.loggers:\r\n                    print(f\"\\tWorking on logger {logger.name}\")\r\n                    select_irr_hours_for_date(logger, start_date, end_date)\r\n\r\n\r\ndef select_irr_hours_for_date(loggers, start_date, end_date):\r\n    \"\"\"\r\n\r\n    :param loggers: logger object from pickle\r\n    :param start_date: date you want fixes to start\r\n    :param end_date: date  you want fixes to end\r\n    \"\"\"\r\n    project = dbwriter.get_db_project(loggers.crop_type)\r\n    field_db = dbwriter.remove_unwanted_chars_for_db_dataset(loggers.field.name)\r\n    dataset = f\"`{project}.{field_db}.{loggers.name}`\"\r\n    dml_statement = \"select date, daily_switch from \" + dataset + \\\r\n                    \" where date between date('\" + str(start_date) + \\\r\n                    \"') and date('\" + str(end_date) + \"')\"\r\n    # print(dml_statement)\r\n    daily_switch_dates_dictionary = dbwriter.return_query_dict(dml_statement, 'date', 'daily_switch', project)\r\n    #  Loop through all dates and update inches for each date\r\n    for date_data in daily_switch_dates_dictionary:\r\n        daily_hours = daily_switch_dates_dictionary[date_data] / 60\r\n        update_irrigation_hours_for_date(project, loggers.field.name, loggers.name, daily_hours, str(date_data))\r\n\r\n\r\ndef fill_missing_dates(dates: list[date], start_date: str, end_date: str) -> tuple[list[str], list[int]]:\r\n    filled_dates = []\r\n    missing_indexes = []\r\n    date_format = \"%Y-%m-%d\"\r\n    start_date = datetime.strptime(start_date, date_format)\r\n    end_date = datetime.strptime(end_date, date_format)\r\n    delta = timedelta(days=1)\r\n    current_date = start_date\r\n    index = 0\r\n\r\n    while current_date <= end_date:\r\n        if current_date.month == 2 and current_date.day == 29:\r\n            current_date += delta\r\n            continue\r\n        elif index < len(dates) and dates[index] == current_date.strftime(date_format):\r\n            filled_dates.append(dates[index])\r\n            index += 1\r\n        else:\r\n            filled_dates.append(current_date.strftime(date_format))\r\n            missing_indexes.append(len(filled_dates) - 1)\r\n        current_date += delta\r\n\r\n    return filled_dates, missing_indexes\r\n\r\n\r\ndef fill_missing_et(et_list: list, index: list[int]):\r\n    for each_index in index:\r\n        et_list.insert(each_index, '0.00')\r\n    return et_list\r\n\r\n\r\ndef copy_missing_data_to_logger_from_other_logger(field_name: str, logger_destination_name: str, logger_source_name: str):\r\n    \"\"\"\r\n    :param field_name: Name of field\r\n    :param logger_destination_name: Name of logger that is missing data\r\n    :param logger_source_name: Name of logger that has data\r\n    \"\"\"\r\n    field = ''\r\n    logger_destination = ''\r\n    logger_source = ''\r\n\r\n    growers = Decagon.open_pickle()\r\n    for grower_pickle in growers:\r\n        for field_pickle in grower_pickle.fields:\r\n            if field_pickle.name == field_name:\r\n                field = field_pickle\r\n                for logger_pickle in field.loggers:\r\n                    if logger_pickle.name == logger_destination_name and logger_pickle.active:\r\n                        logger_destination = logger_pickle\r\n                    elif logger_pickle.name == logger_source_name and logger_pickle.active:\r\n                        logger_source = logger_pickle\r\n\r\n    field_name = dbwriter.remove_unwanted_chars_for_db_dataset(field.name)\r\n    project = dbwriter.get_db_project(logger_destination.crop_type)\r\n\r\n    logger_source_dataset = project + \".\" + field_name + \".\" + logger_source.name\r\n    logger_destination_dataset = project + \".\" + field_name + \".\" + logger_destination.name\r\n\r\n    print(\"Inserting data for table: \" + logger_destination_dataset)\r\n    print(\" from\")\r\n    print(\"Logger table: \" + logger_source_dataset)\r\n\r\n    # Use when eto is not NULL\r\n    dml_statement = \"MERGE `\" + logger_destination_dataset + \"` T \" \\\r\n                    + \"USING `\" + logger_source_dataset + \"` S \" \\\r\n                    + \"ON T.date = S.date \" \\\r\n                    + \"WHEN NOT MATCHED THEN \" \\\r\n                    + \"INSERT (logger_id, date, time, canopy_temperature, ambient_temperature, vpd, vwc_1, vwc_2, vwc_3, \" \\\r\n                      \"field_capacity, wilting_point,  daily_gallons, daily_switch, daily_hours, daily_pressure, \" \\\r\n                      \"daily_inches, psi, psi_threshold, psi_critical, sdd, rh, eto, kc, etc, et_hours, \" \\\r\n                      \"phase1_adjustment, phase1_adjusted, phase2_adjustment, phase2_adjusted, phase3_adjustment, \" \\\r\n                      \"phase3_adjusted, vwc_1_ec, vwc_2_ec, vwc_3_ec) \" \\\r\n                    + \"Values (S.logger_id, S.date,    S.time, S.canopy_temperature, S.ambient_temperature, S.vpd, S.vwc_1, S.vwc_2, S.vwc_3, \" \\\r\n                      \"S.field_capacity, S.wilting_point,  S.daily_gallons, S.daily_switch, S.daily_hours, S.daily_pressure, \" \\\r\n                      \"S.daily_inches, S.psi, S.psi_threshold, S.psi_critical, S.sdd, S.rh, S.eto, S.kc, S.etc, S.et_hours, \" \\\r\n                      \"S.phase1_adjustment, S.phase1_adjusted, S.phase2_adjustment, S.phase2_adjusted, S.phase3_adjustment, \" \\\r\n                      \"S.phase3_adjusted, S.vwc_1_ec, S.vwc_2_ec, S.vwc_3_ec)\"\r\n    # print(dml_statement)\r\n    dbwriter.run_dml(dml_statement, project=project)\r\n\r\n    print(\"Changing logger_id to match new one in old data\")\r\n    dml_statement = (\r\n            \"Update `\" + logger_destination_dataset + \"`  set logger_id = '\" + logger_destination.id + \"' where logger_id = '\" + logger_source.id + \"'\")\r\n    # print(dml_statement)\r\n    dbwriter.run_dml(dml_statement, project=project)\r\n    print(\"Clean up nulls\")\r\n    dml_statement = (\"Delete `\" + logger_destination_dataset + \"` where date is NULL\")\r\n    dbwriter.run_dml(dml_statement, project=project)\r\n\r\n\r\n#Grab dates and psi where first psi was turned on. If possible pull the first 3 days of values. Loop through 2022 year dataset.\r\ndef select_first_psi_for_all_datasets():\r\n    \"\"\"\r\n    Function goes into database project for 2022 and selects the first 3 days of PSI values. It stores the data into a pickle called\r\n    psi_pickle_2022.pickle.\r\n    \"\"\"\r\n    datasets = dbwriter.get_datasets('stomato')\r\n    psi_dict = {'field': [], 'logger': [], 'dates': [], 'psi': []}\r\n    specific_file_path: str = PICKLE_DIRECTORY\r\n    filename = \"psi_pickle_2022.pickle\"\r\n    for dataset in datasets[0]:\r\n        # print(dataset.dataset_id)\r\n        tables_list = dbwriter.get_tables(dataset.dataset_id, 'stomato')\r\n        if dataset.dataset_id == '1_growers_list' or dataset.dataset_id == \"1_technician_portal\" or \\\r\n                dataset.dataset_id == \"1_uninstallation_progress\" or dataset.dataset_id == \"1_users_schema\" or \\\r\n                dataset.dataset_id == \"2022_uninstallation_progress\" or dataset.dataset_id == \"ET\" or dataset.dataset_id == \"Historical_ET\" \\\r\n                or dataset.dataset_id == \"Historical_ET_New\" or dataset.dataset_id == \"Meza\" or \"RnD\" in dataset.dataset_id \\\r\n                or dataset.dataset_id == \"SaulTest\" or dataset.dataset_id == \"YaraAyra\" or dataset.dataset_id == \"TestField\":\r\n            continue\r\n        else:\r\n            for table in tables_list:\r\n                # print(table.table_id)\r\n                if table.table_id == 'weather_forecast' or \"Irr_Scheduling\" in table.table_id or \"temp\" in table.table_id or \"copy\" in table.table_id\\\r\n                        or \"5G\" in table.table_id or \"z6\" in table.table_id:\r\n                    continue\r\n                else:\r\n                    # print(f\"Dataset: {dataset.dataset_id:<20} Table:{table.table_id:<20} Date List:{date_list:<20} PSI List:{psi_list:<20}\")\r\n                    try:\r\n                        date_list, psi_list = select_psi(dataset.dataset_id, table.table_id)\r\n                        psi_dict['field'].append(dataset.dataset_id)\r\n                        psi_dict['logger'].append(table.table_id)\r\n                        psi_dict['dates'].append(date_list)\r\n                        psi_dict['psi'].append(psi_list)\r\n                        print(f\"Working on Field: {dataset.dataset_id} \\n\\t Logger: {table.table_id}\")\r\n\r\n                    except google.api_core.exceptions.BadRequest as e:\r\n                        print(\"Caught BadRequest exception:\", e)\r\n    if path.exists(specific_file_path):\r\n        with open(specific_file_path + filename, \"wb\") as file:\r\n            pickle.dump(psi_dict, file)\r\n\r\n\r\ndef select_psi(field_name: str, logger_name: str) -> tuple[list[date], list[int]]:\r\n    \"\"\"\r\n    Function grabs first 3 values where psi occurs and returns them as a list of dates and a list of psi values\r\n    :param field_name: Dataset Field Name\r\n    :param logger_name: Dataset Logger Name\r\n    :return: date_list: List of Dates of the first 3 psi occurrences\r\n            psi_list: List of PSI of the first 3 psi occurrences\r\n    \"\"\"\r\n    field_db = dbwriter.remove_unwanted_chars_for_db_dataset(field_name)\r\n    dataset = f\"`{'stomato'}.{field_db}.{logger_name}`\"\r\n    dml_statement = f\"select date, psi from {dataset} where psi is not null order by date asc\"\r\n    # print(dml_statement)\r\n    psi_dict = dbwriter.return_query_dict(dml_statement, 'date', 'psi', 'stomato')\r\n    psi_only_three_dict = dict(itertools.islice(psi_dict.items(),3))\r\n    date_list = list(psi_only_three_dict.keys())\r\n    psi_list = list(psi_only_three_dict.values())\r\n    return date_list, psi_list\r\n\r\ndef show_psi_pickle_2022(specific_file_path: str = PICKLE_DIRECTORY, filename: str =\"psi_pickle_2022.pickle\"):\r\n    \"\"\"\r\n    Function opens and returns psi pickle for 2022\r\n\r\n    :param specific_file_path:\r\n    :param filename:\r\n    :return:\r\n    \"\"\"\r\n    with open(specific_file_path + filename, \"rb\") as file:\r\n        data = pickle.load(file)\r\n    return data\r\n\r\n\r\ndef copy_last_day_from_old_date_to_new_date(project: str, field_name: str, logger_name: str, old_date: str, new_date: str):\r\n    \"\"\"\r\n    This function copies the old date's data from a specific logger for a specific field and inserts it into the same logger but using a different\r\n    date. This function is useful for when a logger gets disconnected for a day and we lost data for that day.\r\n    :param project: Project name\r\n    :param field_name: Field Name\r\n    :param logger_name: Logger Name\r\n    :param old_date: Old date you want to copy the information from\r\n    :param new_date: New date you want to copy the information to\r\n    \"\"\"\r\n    # Set up BigQuery client\r\n    client = dbwriter.grab_bq_client(project)\r\n    field_name = dbwriter.remove_unwanted_chars_for_db_dataset(field_name)\r\n    # Define the source and destination table information\r\n    source_table = f\"{project}.{field_name}.{logger_name}\"\r\n    destination_table = f\"{project}.{field_name}.{logger_name}\"\r\n    source_date = old_date  # Date of the source row\r\n    destination_date = new_date  # Date for the new row\r\n\r\n    # Query to select values from the source table for a specific date\r\n    select_query = f\"\"\"\r\n        SELECT *\r\n        FROM `{source_table}`\r\n        WHERE date = '{source_date}'\r\n    \"\"\"\r\n\r\n    query_job = client.query(select_query)\r\n    query_job.result()  # Wait for the query to complete\r\n\r\n    if query_job.errors:\r\n        print(\"Encountered errors while selecting rows.\")\r\n    else:\r\n        print(\"Rows successfully selected into the destination table.\")\r\n        for row in query_job:\r\n            # Convert the Row object to a dictionary\r\n            row_dict = dict(row)\r\n\r\n            # Set the date to the destination date\r\n            row_dict[\"date\"] = destination_date\r\n            # Convert planting date to a string and save that as the planting date. Big query will recongize it as a date time, even if it's\r\n            # inserted as string\r\n            planting_date_string = row_dict[\"planting_date\"].strftime('%Y-%m-%d')\r\n            row_dict[\"planting_date\"] = planting_date_string\r\n\r\n        # print(row_dict)\r\n        values = list(row_dict.values())\r\n\r\n        # Query to insert the selected values into the destination table with a different date\r\n        insert_query = f\"INSERT INTO `{destination_table}` \"\r\n        insert_query += \"VALUES (\"\r\n        insert_query += \", \".join([f\"{'Null' if value is None else value if not isinstance(value, str) else convert_string(value)}\" for\r\n                                   value in values])  # Loop through all the values in row_dict. If your value is None, convert into a string of Null.\r\n        # If your value is a string you want to convert that value into a string by adding a ' to the beginning and end. If not when you loop\r\n        # through the values they get inserted without the ''\r\n        insert_query += \")\"\r\n\r\n        # print(insert_query)\r\n\r\n        # Run the insert query\r\n        query_job = client.query(insert_query)\r\n        query_job.result()  # Wait for the query to complete\r\n\r\n        if query_job.errors:\r\n            print(\"Encountered errors while inserting rows.\")\r\n        else:\r\n            print(\"Rows successfully inserted into the destination table.\")\r\n\r\n\r\ndef convert_string(text):\r\n    return f\"'{text}'\"\r\n\r\ndef find_lowest_psi_fields():\r\n        \"\"\"\r\n        Function finds the lowest psi fields in the database and returns a list of the lowest psi fields.\r\n        \"\"\"\r\n        psi_list = []\r\n        logger_name_list = []\r\n        project = 'stomato-2023'\r\n        # Get list of datasets in database\r\n        client = dbwriter.grab_bq_client(project)\r\n        datasets = dbwriter.get_datasets(project)\r\n        # Loop through all datasets\r\n        for dataset in datasets[0]:\r\n            # print(f\"Working on field: {dataset.dataset_id}\")\r\n            # Get list of tables in dataset\r\n            tables = client.list_tables(dataset.dataset_id)\r\n            # Loop through all tables in dataset\r\n            for table in tables:\r\n                # print(table.table_id)\r\n                # table_id = table.table_id\r\n                if not (\"Irr_Scheduling\" in table.table_id) and not (\"weather_forecast\" in table.table_id):\r\n                    # Check if table is in algorithm list\r\n                    table_id = f\"`{project}.{dataset.dataset_id}.{table.table_id}`\"\r\n                    select_psi = f\"select avg(psi) as average_psi, count(psi) as number_of_data_points from {table_id} where psi is not null\"\r\n                    result = dbwriter.run_dml(select_psi)\r\n                    for row in result:\r\n                        if row.average_psi:\r\n                            # psi_list.append({'field': table_id,'psi':row.average_psi})\r\n                            if row.number_of_data_points > 20:\r\n                                psi_list.append(row.average_psi)\r\n                                logger_name_list.append(table.table_id)\r\n                                print(f\"{table.table_id};{row.average_psi};{row.number_of_data_points}\")\r\n\r\n        # for ind, psi in enumerate(psi_list):\r\n        #     print(f\"{logger_name_list[ind]}: {psi}\")\r\n\r\n\r\ndef generate_invite_code():\r\n    \"\"\"\r\n    Generates a random invite code\r\n    :return: Returns invite code as a 6 letter/number string\r\n    \"\"\"\r\n    invite_code = ''.join(choices(ascii_uppercase + digits, k=6))\r\n    return invite_code\r\n\r\n\r\ndef insert_grower_field(name: str, region: str, tech_assigned: str, stations: str, crop_type: str):\r\n    \"\"\"\r\n    Function inserts the growers field information in the database\r\n    :param crop_type: crop type\r\n    :param name: name of field\r\n    :param region: North or South region\r\n    :param tech_assigned: Technician assigned to field\r\n    :param stations: stations assigned to field\r\n    \"\"\"\r\n    project = 'stomato-info'\r\n    dataset = 'gradient_fields'\r\n    dataset_id = f\"`{project}.{dataset}.all`\"\r\n\r\n    dml_statement = f\"insert into {dataset_id} (name, region, tech_assigned, stations, crop_type) \" \\\r\n                    f\"values ('{name}', '{region}', '{tech_assigned}', '{stations}', '{crop_type}')\"\r\n\r\n    dbwriter.run_dml(dml_statement, project=project)\r\n\r\n\r\ndef insert_grower_loggers(grower: str, field: str, logger_name: str, logger_direction: str, lat: str, long: str, logger_id: str,\r\n                          logger_password: str, crop_type: str):\r\n    \"\"\"\r\n    Function inserts the growers logger information in the database\r\n    :param crop_type: crop type\r\n    :param grower: name of grower\r\n    :param field: name of field\r\n    :param logger_name: name of logger\r\n    :param logger_direction: direction of logger\r\n    :param lat: latitude of logger\r\n    :param long: longitude of logger\r\n    :param logger_id: logger id\r\n    :param logger_password: logger password\r\n\r\n    \"\"\"\r\n    project = 'stomato-info'\r\n    dataset = 'gradient_loggers'\r\n    dataset_id = f\"`{project}.{dataset}.all`\"\r\n\r\n    dml_statement = f\"insert into {dataset_id} (grower, field, logger_name, logger_direction, lat, long, logger_id, logger_password, crop_type) \" \\\r\n                    f\"values ('{grower}', '{field}', '{logger_name}', '{logger_direction}', '{lat}', '{long}', '{logger_id}', '{logger_password}',\" \\\r\n                    f\" '{crop_type}')\"\r\n\r\n    dbwriter.run_dml(dml_statement, project=project)\r\n\r\n\r\ndef add_new_celsius_columns_to_permanent_datasets():\r\n    \"\"\"\r\n    Function adds the canopy_temperature_celsius, ambient_temperature_celsius, lower_ambient_temperature_celsius, sdd_celsius\r\n    columns to the permanent datasets\r\n    \"\"\"\r\n    client = bigquery.Client()\r\n    growers = Decagon.open_pickle()\r\n    for grower in growers:\r\n        for field in grower.fields:\r\n            if (field.crop_type.lower() == 'almonds' or field.crop_type.lower() == 'almonds' or field.crop_type.lower() == 'pistachio'\r\n                    or field.crop_type.lower() == 'pistachios'):\r\n                print(f\"Field: {field.name}\")\r\n                for logger in field.loggers:\r\n                    table_id = f\"stomato-permanents.{dbwriter.remove_unwanted_chars_for_db_dataset(field.name)}.{logger.name}\"\r\n                    # print(f\"Table: {table_id}\")\r\n                    table = client.get_table(table_id)\r\n                    original_schema = table.schema\r\n                    new_schema = original_schema[:]\r\n                    new_schema.append(bigquery.SchemaField(\"canopy_temperature_celsius\", \"FLOAT\"))\r\n                    new_schema.append(bigquery.SchemaField(\"ambient_temperature_celsius\", \"FLOAT\"))\r\n                    new_schema.append(bigquery.SchemaField(\"lowest_ambient_temperature_celsius\", \"FLOAT\"))\r\n                    new_schema.append(bigquery.SchemaField(\"sdd_celsius\", \"FLOAT\"))\r\n\r\n                    table.schema = new_schema\r\n                    try:\r\n                        table = client.update_table(table, [\"schema\"])\r\n                        if len(table.schema) == len(original_schema) + 4 == len(new_schema):\r\n                            print(\"A new column has been added.\")\r\n                        else:\r\n                            print(\"The column has not been added.\")\r\n                    except Exception as e:\r\n                        print(f\"Error updating table schema: {e}\")\r\n\r\n\r\n\r\n\r\ndef update_grower_portal_report_and_images(grower_names: str):\r\n    \"\"\"\r\n    Function updates the grower portal DB reports and images from pickle\r\n    :param grower_names: Grower names\r\n    \"\"\"\r\n    print(f\"Updating reports and previews for {grower_names}\")\r\n    project = 'growers-2024'\r\n    growers = Decagon.open_pickle()\r\n    for g in growers:\r\n        if g.name in grower_names:\r\n            grower_db_name = dbwriter.remove_unwanted_chars_for_db_dataset(g.name)\r\n            base_table_id = f\"{project}.{grower_db_name}\"\r\n            for f in g.fields:\r\n                dml_statement_field = f\"UPDATE `{base_table_id}.field_averages` SET report = '{f.report_url}', preview = '{f.preview_url}' WHERE field = '{f.nickname}'\"\r\n                dml_statement_logger = f\"UPDATE `{base_table_id}.loggers` SET report = '{f.report_url}', crop_image = '{CwsiProcessor().get_crop_image(f.crop_type)}' WHERE field = '{f.nickname}'\"\r\n                for statement in (dml_statement_field, dml_statement_logger):\r\n                    dbwriter.run_dml(statement, project=project)\r\n    print('Done updating reports and previews')\r\n\r\n\r\ndef sum_db_total_for_column_for_list_of_fields(pickle_name: str, pickle_directory: str, db_column: str,\r\n                                               list_of_field_names: list[str]) -> None:\r\n    \"\"\"\r\n    Function that takes in a pickle name, the directory where that pickle is located, a column name from the db,\r\n    and a list of grower field names, and totals up the column from each grower field. This actually totals up the\r\n    column from each logger inside that grower field and then shows the average of all the loggers as well as\r\n    the maximum from the loggers. Useful to total up the irrigation inches from several grower fields\r\n\r\n    :param pickle_name: String of the name of the pickle file including the .pickle extension\r\n    :param pickle_directory: String of the directory where the pickle file is located\r\n    :param db_column: String of the db column name\r\n    :param list_of_field_names: List of strings of the field names we are looking total up\r\n    \"\"\"\r\n    growers = Decagon.open_pickle(pickle_name, pickle_directory)\r\n\r\n    dbw = DBWriter()\r\n    for grower in growers:\r\n        for field in grower.fields:\r\n            if field.name in list_of_field_names:\r\n                print(f'Found {field.name}. Processing...')\r\n                logger_totals = []\r\n                for logger in field.loggers:\r\n                    print(f'\\t{logger.name}')\r\n                    field_name_db = dbw.remove_unwanted_chars_for_db_dataset(field.name)\r\n                    db_project = dbw.get_db_project(field.crop_type)\r\n                    dml = f'SELECT SUM({db_column}) AS total FROM `{db_project}.{field_name_db}.{logger.name}`'\r\n                    result = dbw.run_dml(dml)\r\n                    for row in result:\r\n                        logger_totals.append(row['total'])\r\n                        # print()\r\n                average_total = (sum(logger_totals) / len(logger_totals))\r\n                max_total = (max(logger_totals))\r\n                print(\r\n                    f'\\t\\tAverage Total {db_column} for {len(logger_totals)} loggers = {average_total:.1f}')\r\n                print(\r\n                    f'\\t\\tMax Total {db_column} for {len(logger_totals)} loggers = {max_total:.1f}')\r\n                print()\r\n\r\n\r\ndef change_logger_soil_type(logger_name: str, field_name: str, grower_name: str, new_soil_type: str):\r\n    \"\"\"\r\n    Single function to change the soil type for a logger in both the pickle and the db\r\n\r\n    :param logger_name:\r\n    :param field_name:\r\n    :param grower_name:\r\n    :param new_soil_type:\r\n    \"\"\"\r\n    print(f'Changing soil type for logger: {logger_name} to {new_soil_type}')\r\n\r\n    growers = Decagon.open_pickle()\r\n    dbw = DBWriter()\r\n\r\n    # Change soil type in the pickle\r\n    print('-Changing soil type in the pickle')\r\n    for grower in growers:\r\n        if grower.name == grower_name:\r\n            for field in grower.fields:\r\n                if field.name == field_name:\r\n                    for logger in field.loggers:\r\n                        if logger.name == logger_name:\r\n                            print('\\tFound logger...changing')\r\n                            old_soil_type = logger.soil.soil_type\r\n                            logger.soil.set_soil_type(new_soil_type)\r\n                            field_capacity = logger.soil.field_capacity\r\n                            wilting_point = logger.soil.wilting_point\r\n                            crop_type = logger.crop_type\r\n    Decagon.write_pickle(growers)\r\n    print('\\tDone with pickle')\r\n\r\n    # Change soil type parameters in the DB\r\n    print('-Changing soil type in the db')\r\n    field_name_db = dbw.remove_unwanted_chars_for_db_dataset(field_name)\r\n    db_project = dbw.get_db_project(crop_type)\r\n    dml = (f'UPDATE `{db_project}.{field_name_db}.{logger_name}` '\r\n           f'SET field_capacity = {field_capacity}, wilting_point = {wilting_point} '\r\n           f'WHERE TRUE')\r\n    result = dbw.run_dml(dml)\r\n    print(f'\\tDone with DB')\r\n    print()\r\n    print(f'Soil type for {logger_name} changed from {old_soil_type} to {new_soil_type}')\r\n    print()\r\n\r\n\r\n\r\n\r\n# Decagon.show_pickle()\r\n# growers = Decagon.open_pickle()\r\n# for grower in growers:\r\n#     for field in grower.fields:\r\n#         if field.name == 'Lucero DillardD7':\r\n#             for logger in field.loggers:\r\n#                 change_logger_soil_type(logger.name, logger.field.name, logger.grower.name, 'Sandy Loam')\r\n\r\n\r\n\r\n\r\n\r\n# list_of_grower_fields_to_process = ['Bone Farms LLCF7', 'Fransicioni & Griva8', 'Mike Silva01-MS3', 'Nuss Farms Inc7',\r\n#                                     'Lucero 8 Mile11, 12, 13, 14', 'Lucero Dillard RoadD7', 'Lucero Watermark2,3',\r\n#                                     'Lucero Mandeville15', 'Greenfield Tyler Island1, 3', 'Mumma Bros11,12',\r\n#                                     'Matteoli BrothersN3', 'CM OchoaA8']\r\n# sum_db_total_for_column_for_list_of_fields('2023_pickle.pickle', \"H:\\\\Shared drives\\\\Stomato\\\\2023\\\\Pickle\\\\\", 'daily_inches', list_of_grower_fields_to_process)\r\n\r\n\r\n# update_portal_reports('RKB Farms')\r\n# IMPORTANT: This function updates historical et for all tables or a specific one if specified\r\n# add_new_year_to_historical_et()\r\n# add_new_year_to_historical_et(\"148\")\r\n\r\n\r\n\r\n\r\n# add_new_celsius_columns_to_permanent_datasets()\r\n# add_new_year_to_historical_et()\r\n# cimisStation = CimisStation()\r\n# cimisStation.showCimisStations()\r\n# cimisStation = cimisStation.open_cimis_station_pickle()\r\n# for station in cimisStation:\r\n    # print(f\"{station.station_number} : {station.latest_eto_value}\")\r\n    # print(f\"{station.station_number} : {station.active}\")\r\n# Dict with keys being Field, Logger, Days as a List, PSI as a List\r\n# copy_missing_data_to_logger_from_other_logger('Lucero Dillard RoadD1', 'DI-D1-NE', 'DI-D3-NW')\r\n# select_first_psi_for_all_datasets()\r\n# find_lowest_psi_fields()\r\n# psi_pickle = show_psi_pickle()\r\n# # print(psi_pickle)\r\n# for ind, dataset in enumerate(psi_pickle):\r\n#     print(dataset[ind]['field'])\r\n#     print(dataset[ind]['logger'])\r\n#     print(dataset[ind]['dates'])\r\n#     print(dataset[ind]['psi'])\r\n    # print(f\"Field: {dataset['field']:20} Logger: {dataset['logger']:20} \\n Dates: \\n{' '.join(str(date) for date in dataset['dates']):20}\"\r\n    #       f\" \\nPSI: \\n{' '.join(str(date) for date in dataset['psi']):20}\")\r\n# fix_irr_inches_for_all_fields('2023-04-01', '2023-04-06')\r\n# copy_last_day_from_old_date_to_new_date('stomato-2023', 'Lucero Dillard RoadD1', \"DI-D1-NE\", '2023-06-26', '2023-06-27')\r\n# update_fc_wp('stomato-2023', 'Lucero Dillard RoadD 8, 11', 'DI-D8-C', 36, 22)\r\n# grab_historical_data('250')\r\n# southCount = 0\r\n# northCount = 0\r\n# update_fc_wp('stomato-2023', 'Lucero Dillard RoadD2', 'DI-D2-W', 36, 22)\r\n# update_fc_wp('stomato-2023', 'Turlock Fruit Co1250', 'TU-1250-SE', 28, 14)\r\n# Decagon.show_pickle()\r\n\r\n# growers = Decagon.open_pickle()\r\n# for grower in growers:\r\n#     for field in grower.fields:\r\n#         if field.name == 'Lucero Rio Vista1':\r\n#             Decagon.remove_field(grower.name, field.name)\r\n#             print(f\"{field.name}\")\r\n#             forecast = field.get_weather_forecast()\r\n#             pprint.pprint(forecast)\r\n# removeDuplicateET()\r\n# update_field_et('S&S Ranch33-3')\r\n# growers = Decagon.open_pickle()\r\n# for grower in growers:\r\n#     for field in grower.fields:\r\n#         if field.crop_type.lower() == 'almonds' or field.crop_type.lower() == 'almond':\r\n#             # field.name = 'JJB FarmsGI 19, 21'\r\n#             # field.nickname  = 'GI 19, 21'\r\n#             # print(field.name)\r\n#             # print(field.nickname)\r\n#             for logger in field.loggers:\r\n#                 remove_psi(field.name, '2023-11-1', '2023-12-15', '2023')\r\n                # print(logger.name)\r\n                # print(f\"Old Logger Active: {logger.ir_active}\")\r\n                # logger.ir_active = False\r\n                # print(f\"New Logger Active: {logger.ir_active}\")\r\n#                 if logger.active and logger.name == 'SS-333-SW':\r\n#\r\n# #                     print(logger.id)\r\n# #                     logger.ir_active = True\r\n# #                     print(f\"{field.name}:\\n\\t{logger.name}:{logger.consecutive_ir_values}:{logger.ir_active}\")\r\n# Decagon.write_pickle(growers)\r\n# # # #                 if logger.name == 'BF-57-SW' and logger.active:\r\n# # # #                     project = dbwriter.get_db_project(logger.crop_type)\r\n# # # #                     update_irr_hours_for_date(project, field.name, logger.name, 1.3, '2023-06-16')\r\n#                     remove_duplicate_data(logger)\r\n#                     update_missing_et_data(logger)\r\n#         if field.name == 'S&S Ranch33-3' or field.name == \"S&S Ranch33-1\":\r\n#             for logger in field.loggers:\r\n#                 print(f\"{logger.name}: {logger.id}: {logger.field_capacity} : {logger.wilting_point}\")\r\n# if logger.name == 'SS-331-SW':\r\n#     update_fc_wp(project,field.name,logger.name, 36, 22)\r\n\r\n# Decagon.show_pickle()\r\n\r\n# growers = Decagon.open_pickle()\r\n# for grower in growers:\r\n#     for field in grower.fields:\r\n#         if field.name == 'Lucero Dillard RoadD4':\r\n#             for logger in field.loggers:\r\n#                 if logger.name == 'DI-D4-W':\r\n#                     update_irr_hours_for_date_range('stomato-2023', logger.field.name, logger.name, 0, '2023-06-30', '2023-07-5')\r\n\r\n# growers = Decagon.open_pickle()\r\n# for g in growers:\r\n#     for f in g.fields:\r\n#         if f.name == 'Kruppa Enterprises LLCRokpileb':\r\n#             for l in f.loggers:\r\n# # #                 # update_fc_wp('stomato-2023', f.name, l.name, 36, 22)\r\n#                 if l.logger_direction == 'W':\r\n#                     project = dbwriter.get_db_project(l.crop_type)\r\n#                     update_irr_hours_for_date_range(project, f.name, l.name, 11.2, '2023-06-18', '2023-06-18')\r\n#                     remove_psi_specific(project, f.name, l.name, '2023-05-15', '2023-05-15')\r\n#                     delete_repeat_data(project, f.name, l.name)\r\n#             print(l)\r\n#             if not l.rnd:\r\n#                 if g.region == 'South' or g.region == 'south':\r\n#                     southCount = southCount + 1\r\n#\r\n#                 if g.region == 'North' or g.region == 'north':\r\n#                     northCount = northCount + 1\r\n# print(\"South\\n\\t\" + str(southCount))\r\n# print(\"North\\n\\t\" + str(northCount))\r\n# print(\"Total\\n\\t\" + str(southCount+northCount))\r\n# growers = Decagon.open_pickle()\r\n# for g in growers:\r\n#     for f in g.fields:\r\n#         if f.name == 'Mike Silva01-MS3':\r\n#             for logger in f.loggers:\r\n#                 if logger.name == 'MS-01MS3-NE':\r\n#                     project = dbwriter.get_db_project(logger.crop_type)\r\n#                     update_irr_hours_for_date(project, f.name, logger.name, 4.5, '2023-5-6')\r\n#         update_fc_wp(project, f.name, logger.name, 18, 8)\r\n#     # print(project)\r\n#     print(logger.name)\r\n# remove_duplicate_data(logger)\r\n# growers = Decagon.open_pickle()\r\n# for g in growers:\r\n#     for f in g.fields:\r\n#         if f.active:\r\n#             print(\"Erasing weather issue for field: \", f.name)\r\n# fixWeatherDB(f.name)\r\n# start_date = date(2022, 8, 15)\r\n# end_date = date(2022, 8, 16)\r\n# for single_date in daterange(start_date, end_date):\r\n#     copy_vp4_vals_from_table_to_table('Lucero Rio Vista2', 'RV-02-NW', 'RV-02-S', single_date.strftime(\"%Y-%m-%d\"))\r\n# copy_gdd_values_from_temp_table_to_table()\r\n# delete_repeat_data('Meza', 'Development-C')\r\n# update_portal_image('DCB', 'F3_F4', 'https://i.imgur.com/C6ePFHh.png', True)\r\n\r\n# copy_gdd_values_from_temp_table_to_table('Barrios_Farms22', 'BF-22-SE', 'BF-22-SE_temp')\r\n\r\n# update_FC_WP('Bullseye FarmsRG42', 'Bull-RG42-C', 31, 11)\r\n# move_logger_DB_info('DCBD_T', 'BR-DT1-NE', 'BR-DT1-NE_copy')\r\n# move_logger_DB_info('Lucero_Sun_Pacific31', 'LU-31WM-SE', 'LU-31-SE')\r\n# copy_vp4_vals_from_table_to_table('LemonicaTango K', 'TAG-LE-SE', 'TAG-WM-SE', '2022-04-18')\r\n# update_FC_WP('Carvalho308A', 'KC-308A-S', 36, 22)\r\n# update_logger_et('La QuintaDates Block 36 DB', 'DAT-Blk36DB-NW')\r\n# deleteWhereEToIsNull('OPC3-2', 'OP-AL-NW', '2022-04-02', '2022-04-03')\r\n# removeDuplicateET()\r\n# update_all_field_et()\r\n# update_field_et(\"OPC15-4\")\r\n# delete_et_day('CM OchoaA8', '2023-05-06', '2023-05-09')\r\n# update_field_et(\"CM OchoaA8\")\r\n# update_field_et(\"Andrew3104\")\r\n# update_field_et(\"Andrew3101-3103\")\r\n# update_field_et('DCBWarren')\r\n# growerSucks = 0\r\n# fieldCount = 0\r\n# stationCount = 0\r\n# count = 0\r\n# growers = Decagon.open_pickle()\r\n# for g in growers:\r\n#     for f in g.fields:\r\n#         if f.name == 'CM OchoaA8':\r\n#             delete_et_day(f, '2023-05-06', '2023-05-09')\r\n\r\n# for l in f.loggers:\r\n#             print(g.name,\";\", f.nickname,\";\", l.rnd)\r\n#             if l.rnd:\r\n#                 count += 1\r\n# print(count)\r\n\r\n# print(\"Grower fields: \", growerSucks)\r\n# print(\"Field Count: \", fieldCount)\r\n# print(\"Station Count: \", stationCount)\r\n\r\n# if g.name != 'RnD':\r\n#             for l in f.loggers:\r\n#                 if l.cropType == 'Tomatoes':\r\n#                     print(g.name, ';', f.nickname, ';', l.name, ';')\r\n#         if f.name == 'DCBVerway':\r\n#             previewImage = 'https://i.imgur.com/ROE4By8.png'\r\n#             update_portal_image(g, f, previewImage)\r\n# for l in f.loggers:\r\n#             update_missing_et_data(l)\r\n# update_field_et('LemonicaTango K')\r\n# update_field_et(\"F&SAirport 3 \")\r\n# update_all_field_et()\r\n# removePSISpecific('Hughes303', 'HU-303-NE', '2022-05-27', '2022-05-28')\r\n# remove_psi('Bone Farms LLCN42 N43', '2023-05-16', '2023-05-17', '2023')\r\n# startDate = date(datetime.now().year, 1, 1)\r\n# print(startDate)\r\n# endDate = date(2021, 12, 31)\r\n# setupIrrigationSchedulingDB(105, \"Andrew3125\", startDate, endDate)\r\n# Decagon.show_pickle()\r\n# all_tables_analysis()\r\n# get_average_psi_during_growth()\r\n\r\n# delete_all_null_rows()\r\n# update_missing_et_data('z6-01995')\r\n# delete_repeat_data('OPC3-3', 'OP-33-NE')\r\n# remove_duplicate_data(\"RnD77\", 'LH-77Y-N')\r\n\r\n# deleteETDay('DCBChapman', '2021-08-16')\r\n# deleteLastDay('Hughes234-2', 'z6-07156', '2021-08-18')\r\n\r\n# delete_null_rows('Ryan_JonesFirebaugh_B2', 'z6-01956', 'eto', '2022-1-1', '2022-1-13')\r\n# delete_all_null_rows('eto', '2022-1-1', '2022-1-13')\r\n\r\n# removePSISpecific('DCBSilva 5', 'z6-05978', '2021-08-17', '2021-08-17')\r\n\r\n# range(field, logger, dailyHours, startDate, endDate):\r\n# update_irr_hours_for_date('Barrios Farms84', 'BF-84-SW', 18.9, '2022-04-13')\r\n\r\n\r\n# columns_to_add = {}\r\n# columns_to_add['vwc_1_ec'] = 'FLOAT'\r\n# columns_to_add['vwc_2_ec'] = 'FLOAT'\r\n# columns_to_add['vwc_3_ec'] = 'FLOAT'\r\n# add_column_to_db_specific_field('Meza', columns_to_add)\r\n# add_column_to_db(columns_to_add)\r\n# Decagon.show_pickle()\r\n# removeDuplicateET()\r\n# update_field_et('WestSide RanchSEQ23')\r\n# update_field_et('Lucero BakersfieldTowerline')\r\n# update_all_field_et()\r\n# loggerSetups.removeField('Maricopa Orchards', 'Maricopa Orchards1831Delete')\r\n# cropType = ''\r\n# Decagon.remove_field('Maricopa Orchards', 'Maricopa Orchards1831')\r\n# growers = Decagon.open_pickle()\r\n# for g in growers:\r\n#     for f in g.fields:\r\n#         if f.name == 'Bone Farms LLCR12-13':\r\n# print(f.active)\r\n# f.active = True\r\n# print(f.active)\r\n# Decagon.write_pickle(growers)\r\n#                 print(f.preview_url)\r\n# if g.name == 'Shiraz Ranch':\r\n#     print(g.technician.name)\r\n# for l in f.loggers:\r\n#                 if l.name == 'TO-Green-N':\r\n#                     print('old id: ', l.password)\r\n#                     l.password = '78722-13527'\r\n#                     print('new id: ', l.password)\r\n#                     l.name = \"LH-77-SE\"\r\n#                 l.prev_day_switch = 120\r\n#                 print(l.name)\r\n#                 print(\"\\t\" + str(l.active))\r\n# changeName = input('Name Change: ')\r\n# f.name = changeName\r\n# Decagon.write_pickle(growers)\r\n#                 if l.name == 'MA-BEPI-NE' or l.name == 'MA-YEPI-NE':\r\n#                     l.active = False\r\n#                     print(l.name)\r\n#                     print(\"\\t\" + str(l.active))\r\n# loggerSetups.removeLoggerIDFromPickle(l.id)\r\n\r\n# cropType = l.cropType\r\n# print(f.name)\r\n# print(\"\\t\" + cropType)\r\n#             update_missing_et_data(l)\r\n# for g in growers:\r\n#     if g.active:\r\n#         for f in g.fields:\r\n#             if f.active:\r\n#                 for l in f.loggers:\r\n#                     update_missing_et_data(l)\r\n# for ind, g in enumerate(growers):\r\n#     if g.name == 'DCB':\r\n#         for f in g.fields:\r\n#             if f.name == 'DCBRoggero Almonds':\r\n#                 for l in f.loggers:\r\n#                     update_missing_et_data(l)\r\n#     updateKcValuesWithAMaxIncludingEtcEtcHours(l)\r\n#     updateKcValuesWithAMaxI(l)\r\n\r\n# if g.name == \"DCB\" or g.name == \"David Santos\" or g.name == \"Matteoli\" or g.name == \"Barrios\"\r\n\r\n# growers = Decagon.open_pickle()\r\n# for ind, g in enumerate(growers):\r\n#     if g.name == 'La Quinta':\r\n#         for f in g.fields:\r\n#             # if f.name == 'TPMM3':\r\n#             for l in f.loggers:\r\n#                 update_missing_et_data(l)\r\n# Decagon.write_pickle(growers)\r\n\r\n# removeDoubleDataLateHours(\"DCBNees 7-8\", \"z6-12427\")\r\n# deleteET(146)\r\n# Decagon.setPlantingDate(\"David Santos\", \"David SantosFA3\", 2021, 3, 8)\r\n# addETHoursCol()\r\n# addETHoursColSpecific(\"BoneFarms\")\r\n# dbwriter.add_new_column_to_table(\"Bullseye_FarmsFD85D\", \"z6-11506\", \"et_hours\", \"FLOAT\")\r\n# removeDuplicateET()\r\n\r\n\r\n# field = \"Zuckerman Farms7B-7K\"\r\n# logger = \"z6-11562\"\r\n# addETHoursColSpecific(\"DCBDF7-8\")\r\n# Decagon.show_pickle()\r\n# growers = Decagon.open_pickle()\r\n# for g in growers:\r\n#     for f in g.fields:\r\n#         if f.active:\r\n#             for l in f.loggers:\r\n#                 # if f.name == \"LemonicaLemon E\":\r\n#                 if l.active:\r\n#                     remove_duplicate_data(f.name, l.name)\r\n#             print(f.name)\r\n# growers = Decagon.open_pickle()\r\n# for g in growers:\r\n#     for f in g.fields:\r\n#         for l in f.loggers:\r\n#             if l.active and not f.active:\r\n# removeField = input(\"Remove field: \" + f.name + \"\\n \\t Remove Logger: \" + l.name + \"\\n\")\r\n# if removeField == 'y':\r\n# Decagon.removeLogger(g.name, f.name, l.id)\r\n#         if f.name == 'La QuintaDates':\r\n#             print(f.preview_url)\r\n# f.preview_url = 'https://i.imgur.com/jsRPSad.png'\r\n# print(f.preview_url)\r\n# for l in f.loggers:\r\n# if f.name == \"LemonicaLemon E\":\r\n# if l.active:\r\n#     remove_duplicate_data(f.name, l.name)\r\n\r\n\r\n#         f.CimisStation = CimisStation()\r\n# for ind, g in enumerate(growers):\r\n#     if g.name == 'Andrew':\r\n#         for f in g.fields:\r\n#             if f.name == 'Andrew3125':\r\n#                 for l in f.loggers:\r\n#                     update_missing_et_data(l)\r\n# Decagon.write_pickle(growers)\r\n# update_missing_et_data('z6-07275')\r\n\r\n\r\n# field = \"Dougherty BrosSY\"\r\n# growers = Decagon.open_pickle()\r\n# for ind, g in enumerate(growers):\r\n#     for f in g.fields:\r\n#         if f.name == field:\r\n#             for l in f.loggers:\r\n#                 update_missing_et_data(l)\r\n\r\n\r\n# Decagon.show_pickle()\r\n# deleteAndUpdateDB(\"DCB\", \"DCBMontague \", '2021-04-22', '2021-04-23')\r\n#\r\n# deleteAndUpdateDBGrower(\"Carvalho\", \"2021-05-15\", '2021-05-16')\r\n# removePSISpecific(\"JJB FarmsGrand Island 1-4-10\", \"z6-12365\", '2021-07-08', '2021-07-24')\r\n# remove_psi(\"Bone Farms LLCR12-13\", '2023-04-01', '2023-04-11', '2023')\r\n# growers = Decagon.open_pickle()\r\n# for grower in growers:\r\n#     for field in grower.fields:\r\n#         if field.name == 'Bone Farms LLCR12-13':\r\n#             fix_weather_db(field)\r\n\r\n#\r\n# copy_values_from_table_to_table()\r\n#\r\n\r\n#\r\n#\r\n# update_eto_etc('Andrew3104', 'z6-12421', [0.34, 0.31], '2021-05-03', '2021-05-04')\r\n#\r\n# copy_vp4_vals_from_table_to_table()\r\n#\r\n\r\n# delete_repeat_data('JK_VineyardsGemmer_North', 'z6-02143')\r\n# delete_repeat_data('Dougherty_BrosT4', 'z6-11510')\r\n# delete_repeat_data('DCBMaricopa_3', 'z6-11933')\r\n# delete_repeat_data('Andrew3125', 'z6-07261')\r\n# delete_repeat_data('Bullseye FarmsRG28', 'Bull-RG28-NW')\r\n\r\n#\r\n# delete_all_repeat_data()\r\n\r\n# Decagon.show_pickle()\r\n\r\n\r\n# delete_all_null_rows()\r\n# remove_unwanted_chars_for_db('OPC4-1')\r\n# update_field_et('WestSide RanchSWQ15')\r\n# Decagon.show_pickle()\r\n# update_irr_hours_for_date('stomato-2023', 'Lucero BakersfieldTowerline', 'BA-Purple-S', 0, '2023-04-17')\r\n# update_irr_hours_for_date_range('stomato-2023', 'Bone Farms LLCF7', 'BO-F7-NE', 0, '2023-04-05', '2023-04-11')\r\n# updateIrrHours('DCBNees 7-8', 'z6-12427', 8, '2021-07-31')\r\n# updateIrrHours('DCBNees 7-8', 'z6-12427', 2.6, '2021-08-01')\r\n# updateIrrHours('DCBNees 7-8', 'z6-12427', 5.2, '2021-08-02')\r\n# updateIrrHours('DCBNees 7-8', 'z6-12427', 10.7, '2021-08-03')\r\n\r\n# deleteLastDay('JHPBase5', 'z6-07262')\r\n# dt = date.today() - timedelta(days=5)\r\n# while str(dt) != '2021-07-22':\r\n#     print(dt)\r\n#     updateIrrHours('DCBNees 7-8', 'z6-12427', 12, str(dt))\r\n#     dt = dt - timedelta(days=1)\r\n#     time.sleep(1)\r\n\r\n# Decagon.show_pickle()\r\n# update_all_field_et()
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/SQLScripts.py b/SQLScripts.py
--- a/SQLScripts.py	(revision 5cf7d78a1d94023d00661edf5517badacc064130)
+++ b/SQLScripts.py	(date 1720741670686)
@@ -22,6 +22,7 @@
 DIRECTORY_YEAR = "2023"
 PICKLE_DIRECTORY = "H:\\Shared drives\\Stomato\\" + DIRECTORY_YEAR + "\\Pickle\\"
 
+
 def update_value_for_date(project, field_name, logger_name, date, value_name, value):
     dml = 'UPDATE `' + str(project) + '.' + str(field_name) + '.' + str(logger_name) + '`' \
           + ' SET ' + str(value_name) + ' = ' + str(value) \
@@ -82,6 +83,7 @@
     dbwriter.run_dml(dml, project=project)
     print("Done Updating Irr. Hours")
 
+
 # update_irrigation_hours_for_date('stomato-2023', 'Lucero Mandeville17 I J', 'LM-17J-S', 4.2, '2023-08-08')
 # update_irrigation_hours_for_date('stomato-2023', 'Lucero Mandeville17 I J', 'LM-17J-S', 5.1, '2023-08-10')
 # update_irrigation_hours_for_date('stomato-2023', 'Lucero Mandeville17 I J', 'LM-17J-S', 3.5, '2023-08-12')
@@ -102,6 +104,8 @@
     # print(dml)
     dbwriter.run_dml(dml, project=project)
     print("Done Updating Irr. Inches")
+
+
 # update_irr_inches_for_date('stomato-2023', 'Lucero Dillard RoadD4', 'DI-D4-W')
 
 
@@ -128,6 +132,7 @@
     dbwriter.run_dml(dml, project=project)
     print("Done Updating Irr. Hours")
 
+
 def update_eto_etc(project, field_name, logger_name, list_etos, start_date, end_date):
     field_name = dbwriter.remove_unwanted_chars_for_db_dataset(field_name)
     list_etos = list_etos
@@ -246,7 +251,6 @@
             hours = e["daily_hours"]
             switch = e["daily_switch"]
 
-
             dml = "UPDATE `stomato-permanents.Riley_Chaney_Farms16.RC-16-W` " \
                   + " SET daily_switch = " + str(switch) + \
                   ", daily_hours = " + str(hours) + \
@@ -309,26 +313,26 @@
     # date, daily_inches, daily_hours, daily_switch, canopy_temperature, ambient_temperature, vpd, psi, sdd, rh, lowest_ambient_temperature
 
     dml = "MERGE `" + main_dataset_id + "` T " \
-        + "USING `" + merge_dataset_id + "` S " \
-        + "ON (t.date = s.date AND T.date > '2024-01-01') " \
-        + "WHEN MATCHED THEN " \
-        + "UPDATE SET " \
-          "daily_inches = s.daily_inches, " \
-          "daily_hours = s.daily_hours," \
-          "daily_switch = s.daily_switch, " \
-          "canopy_temperature = s.canopy_temperature, " \
-          "ambient_temperature = s.ambient_temperature, " \
-          "vpd = s.vpd, " \
-          "psi = s.psi, " \
-          "sdd = s.sdd, " \
-          "rh = s.rh, " \
-          "lowest_ambient_temperature = s.lowest_ambient_temperature"
+          + "USING `" + merge_dataset_id + "` S " \
+          + "ON (t.date = s.date AND T.date > '2024-01-01') " \
+          + "WHEN MATCHED THEN " \
+          + "UPDATE SET " \
+            "daily_inches = s.daily_inches, " \
+            "daily_hours = s.daily_hours," \
+            "daily_switch = s.daily_switch, " \
+            "canopy_temperature = s.canopy_temperature, " \
+            "ambient_temperature = s.ambient_temperature, " \
+            "vpd = s.vpd, " \
+            "psi = s.psi, " \
+            "sdd = s.sdd, " \
+            "rh = s.rh, " \
+            "lowest_ambient_temperature = s.lowest_ambient_temperature"
     result = dbwriter.run_dml(dml, project='stomato-permanents')
 
+
 # merge_table_into_table_updating_some_values('stomato-permanents.Barrios_Farms22.BF-22-NW2', 'stomato-permanents.Barrios_Farms22.BF-22-NW')
 
 
-
 def daterange(start_date, end_date):
     for n in range(int((end_date - start_date).days)):
         yield start_date + timedelta(n)
@@ -549,17 +553,17 @@
     growers = Decagon.open_pickle()
     for g in growers:
         # if g.name == 'Surjit Chahal':
-            dataset_name = dbwriter.remove_unwanted_chars_for_db_dataset(g.name)
-            table_name = 'loggers'
+        dataset_name = dbwriter.remove_unwanted_chars_for_db_dataset(g.name)
+        table_name = 'loggers'
 
-            print(dataset_name, table_name)
+        print(dataset_name, table_name)
 
-            for val in column_name_and_type_dict:
-                try:
-                    dbwriter.add_new_column_to_table(dataset_name, table_name, val, column_name_and_type_dict[val],
-                                                     project=project)
-                except:
-                    print(f'Error when trying to add column to table {dataset_name}.{table_name}')
+        for val in column_name_and_type_dict:
+            try:
+                dbwriter.add_new_column_to_table(dataset_name, table_name, val, column_name_and_type_dict[val],
+                                                 project=project)
+            except:
+                print(f'Error when trying to add column to table {dataset_name}.{table_name}')
 
 
 # db_column = {
@@ -919,6 +923,7 @@
     Decagon.update_irr_scheduling(field_name + '_Irr_Scheduling', field_name, csv_data, overwrite=True,
                                   logger=logger)
 
+
 def returnHistoricalETDict(etStation: str, start_date: date, end_date: date) -> dict:
     """
     #TODO: adjust returnHistoricalETDict for new naming schema of Hist ET tables
@@ -939,7 +944,7 @@
     return etValue
 
 
-def move_logger_db_info(project:str, field_name:str, new_logger_name:str, old_logger_name:str):
+def move_logger_db_info(project: str, field_name: str, new_logger_name: str, old_logger_name: str):
     """
     Use if copying data from an old logger to a new logger table
     :param project: Big Query Project
@@ -1103,13 +1108,13 @@
     date = year + "-" + month + "-" + day
     # print(date)
 
-    dml_statement = ("Update `" + project + "." + dataset + " as t" + " Set t.order = 99 where t.date < " + "'" + date + "'")
+    dml_statement = (
+                "Update `" + project + "." + dataset + " as t" + " Set t.order = 99 where t.date < " + "'" + date + "'")
     dbwriter.run_dml(dml_statement, project=project)
     print(f"Finished fixing weather for {field.name}")
     # print(dml_statement)
 
 
-
 def returnHistoricalData(etStation):
     print("Grabbing previous years historical data")
 
@@ -1322,7 +1327,8 @@
     return et_list
 
 
-def copy_missing_data_to_logger_from_other_logger(field_name: str, logger_destination_name: str, logger_source_name: str):
+def copy_missing_data_to_logger_from_other_logger(field_name: str, logger_destination_name: str,
+                                                  logger_source_name: str):
     """
     :param field_name: Name of field
     :param logger_destination_name: Name of logger that is missing data
@@ -1403,7 +1409,7 @@
         else:
             for table in tables_list:
                 # print(table.table_id)
-                if table.table_id == 'weather_forecast' or "Irr_Scheduling" in table.table_id or "temp" in table.table_id or "copy" in table.table_id\
+                if table.table_id == 'weather_forecast' or "Irr_Scheduling" in table.table_id or "temp" in table.table_id or "copy" in table.table_id \
                         or "5G" in table.table_id or "z6" in table.table_id:
                     continue
                 else:
@@ -1436,12 +1442,13 @@
     dml_statement = f"select date, psi from {dataset} where psi is not null order by date asc"
     # print(dml_statement)
     psi_dict = dbwriter.return_query_dict(dml_statement, 'date', 'psi', 'stomato')
-    psi_only_three_dict = dict(itertools.islice(psi_dict.items(),3))
+    psi_only_three_dict = dict(itertools.islice(psi_dict.items(), 3))
     date_list = list(psi_only_three_dict.keys())
     psi_list = list(psi_only_three_dict.values())
     return date_list, psi_list
 
-def show_psi_pickle_2022(specific_file_path: str = PICKLE_DIRECTORY, filename: str ="psi_pickle_2022.pickle"):
+
+def show_psi_pickle_2022(specific_file_path: str = PICKLE_DIRECTORY, filename: str = "psi_pickle_2022.pickle"):
     """
     Function opens and returns psi pickle for 2022
 
@@ -1454,7 +1461,8 @@
     return data
 
 
-def copy_last_day_from_old_date_to_new_date(project: str, field_name: str, logger_name: str, old_date: str, new_date: str):
+def copy_last_day_from_old_date_to_new_date(project: str, field_name: str, logger_name: str, old_date: str,
+                                            new_date: str):
     """
     This function copies the old date's data from a specific logger for a specific field and inserts it into the same logger but using a different
     date. This function is useful for when a logger gets disconnected for a day and we lost data for that day.
@@ -1504,8 +1512,10 @@
         # Query to insert the selected values into the destination table with a different date
         insert_query = f"INSERT INTO `{destination_table}` "
         insert_query += "VALUES ("
-        insert_query += ", ".join([f"{'Null' if value is None else value if not isinstance(value, str) else convert_string(value)}" for
-                                   value in values])  # Loop through all the values in row_dict. If your value is None, convert into a string of Null.
+        insert_query += ", ".join(
+            [f"{'Null' if value is None else value if not isinstance(value, str) else convert_string(value)}" for
+             value in
+             values])  # Loop through all the values in row_dict. If your value is None, convert into a string of Null.
         # If your value is a string you want to convert that value into a string by adding a ' to the beginning and end. If not when you loop
         # through the values they get inserted without the ''
         insert_query += ")"
@@ -1525,40 +1535,41 @@
 def convert_string(text):
     return f"'{text}'"
 
+
 def find_lowest_psi_fields():
-        """
+    """
         Function finds the lowest psi fields in the database and returns a list of the lowest psi fields.
         """
-        psi_list = []
-        logger_name_list = []
-        project = 'stomato-2023'
-        # Get list of datasets in database
-        client = dbwriter.grab_bq_client(project)
-        datasets = dbwriter.get_datasets(project)
-        # Loop through all datasets
-        for dataset in datasets[0]:
-            # print(f"Working on field: {dataset.dataset_id}")
-            # Get list of tables in dataset
-            tables = client.list_tables(dataset.dataset_id)
-            # Loop through all tables in dataset
-            for table in tables:
-                # print(table.table_id)
-                # table_id = table.table_id
-                if not ("Irr_Scheduling" in table.table_id) and not ("weather_forecast" in table.table_id):
-                    # Check if table is in algorithm list
-                    table_id = f"`{project}.{dataset.dataset_id}.{table.table_id}`"
-                    select_psi = f"select avg(psi) as average_psi, count(psi) as number_of_data_points from {table_id} where psi is not null"
-                    result = dbwriter.run_dml(select_psi)
-                    for row in result:
-                        if row.average_psi:
-                            # psi_list.append({'field': table_id,'psi':row.average_psi})
-                            if row.number_of_data_points > 20:
-                                psi_list.append(row.average_psi)
-                                logger_name_list.append(table.table_id)
-                                print(f"{table.table_id};{row.average_psi};{row.number_of_data_points}")
+    psi_list = []
+    logger_name_list = []
+    project = 'stomato-2023'
+    # Get list of datasets in database
+    client = dbwriter.grab_bq_client(project)
+    datasets = dbwriter.get_datasets(project)
+    # Loop through all datasets
+    for dataset in datasets[0]:
+        # print(f"Working on field: {dataset.dataset_id}")
+        # Get list of tables in dataset
+        tables = client.list_tables(dataset.dataset_id)
+        # Loop through all tables in dataset
+        for table in tables:
+            # print(table.table_id)
+            # table_id = table.table_id
+            if not ("Irr_Scheduling" in table.table_id) and not ("weather_forecast" in table.table_id):
+                # Check if table is in algorithm list
+                table_id = f"`{project}.{dataset.dataset_id}.{table.table_id}`"
+                select_psi = f"select avg(psi) as average_psi, count(psi) as number_of_data_points from {table_id} where psi is not null"
+                result = dbwriter.run_dml(select_psi)
+                for row in result:
+                    if row.average_psi:
+                        # psi_list.append({'field': table_id,'psi':row.average_psi})
+                        if row.number_of_data_points > 20:
+                            psi_list.append(row.average_psi)
+                            logger_name_list.append(table.table_id)
+                            print(f"{table.table_id};{row.average_psi};{row.number_of_data_points}")
 
-        # for ind, psi in enumerate(psi_list):
-        #     print(f"{logger_name_list[ind]}: {psi}")
+    # for ind, psi in enumerate(psi_list):
+    #     print(f"{logger_name_list[ind]}: {psi}")
 
 
 def generate_invite_code():
@@ -1589,7 +1600,8 @@
     dbwriter.run_dml(dml_statement, project=project)
 
 
-def insert_grower_loggers(grower: str, field: str, logger_name: str, logger_direction: str, lat: str, long: str, logger_id: str,
+def insert_grower_loggers(grower: str, field: str, logger_name: str, logger_direction: str, lat: str, long: str,
+                          logger_id: str,
                           logger_password: str, crop_type: str):
     """
     Function inserts the growers logger information in the database
@@ -1624,7 +1636,8 @@
     growers = Decagon.open_pickle()
     for grower in growers:
         for field in grower.fields:
-            if (field.crop_type.lower() == 'almonds' or field.crop_type.lower() == 'almonds' or field.crop_type.lower() == 'pistachio'
+            if (
+                    field.crop_type.lower() == 'almonds' or field.crop_type.lower() == 'almonds' or field.crop_type.lower() == 'pistachio'
                     or field.crop_type.lower() == 'pistachios'):
                 print(f"Field: {field.name}")
                 for logger in field.loggers:
@@ -1649,9 +1662,7 @@
                         print(f"Error updating table schema: {e}")
 
 
-
-
-def update_grower_portal_report_and_images(grower_names: str):
+def update_growers_portal_report_and_images(grower_names: str):
     """
     Function updates the grower portal DB reports and images from pickle
     :param grower_names: Grower names
@@ -1668,6 +1679,39 @@
                 dml_statement_logger = f"UPDATE `{base_table_id}.loggers` SET report = '{f.report_url}', crop_image = '{CwsiProcessor().get_crop_image(f.crop_type)}' WHERE field = '{f.nickname}'"
                 for statement in (dml_statement_field, dml_statement_logger):
                     dbwriter.run_dml(statement, project=project)
+                    print(statement)
+    print('Done updating reports and previews')
+
+
+def update_field_portal_report_and_images(field_name: str, report_url: str = None, preview_url: str = None):
+    """
+    Function updates the fields pickle report and preview and grower portal DB reports and images
+    :param field_name: Field name
+    :param report_url: Pulls from pickle if none given
+    :param preview_url: Pulls from pickle if none given
+    """
+
+    print(f"Updating reports and previews for {field_name}")
+    project = 'growers-2024'
+    growers = Decagon.open_pickle()
+    for g in growers:
+        for f in g.fields:
+            if f.name == field_name:
+                grower_db_name = dbwriter.remove_unwanted_chars_for_db_dataset(g.name)
+                base_table_id = f"{project}.{grower_db_name}"
+                # update pickle
+                if report_url:
+                    f.report_url = report_url
+                if preview_url:
+                    f.preview_url = preview_url
+
+                # update DB
+                dml_statement_field = f"UPDATE `{base_table_id}.field_averages` SET report = '{f.report_url}', preview = '{f.preview_url}' WHERE field = '{f.nickname}'"
+                dml_statement_logger = f"UPDATE `{base_table_id}.loggers` SET report = '{f.report_url}', crop_image = '{CwsiProcessor().get_crop_image(f.crop_type)}' WHERE field = '{f.nickname}'"
+                for statement in (dml_statement_field, dml_statement_logger):
+                    dbwriter.run_dml(statement, project=project)
+                    print(statement)
+    Decagon.write_pickle(growers)
     print('Done updating reports and previews')
 
 
@@ -1754,9 +1798,6 @@
     print(f'Soil type for {logger_name} changed from {old_soil_type} to {new_soil_type}')
     print()
 
-
-
-
 # Decagon.show_pickle()
 # growers = Decagon.open_pickle()
 # for grower in growers:
@@ -1766,9 +1807,6 @@
 #                 change_logger_soil_type(logger.name, logger.field.name, logger.grower.name, 'Sandy Loam')
 
 
-
-
-
 # list_of_grower_fields_to_process = ['Bone Farms LLCF7', 'Fransicioni & Griva8', 'Mike Silva01-MS3', 'Nuss Farms Inc7',
 #                                     'Lucero 8 Mile11, 12, 13, 14', 'Lucero Dillard RoadD7', 'Lucero Watermark2,3',
 #                                     'Lucero Mandeville15', 'Greenfield Tyler Island1, 3', 'Mumma Bros11,12',
@@ -1782,16 +1820,14 @@
 # add_new_year_to_historical_et("148")
 
 
-
-
 # add_new_celsius_columns_to_permanent_datasets()
 # add_new_year_to_historical_et()
 # cimisStation = CimisStation()
 # cimisStation.showCimisStations()
 # cimisStation = cimisStation.open_cimis_station_pickle()
 # for station in cimisStation:
-    # print(f"{station.station_number} : {station.latest_eto_value}")
-    # print(f"{station.station_number} : {station.active}")
+# print(f"{station.station_number} : {station.latest_eto_value}")
+# print(f"{station.station_number} : {station.active}")
 # Dict with keys being Field, Logger, Days as a List, PSI as a List
 # copy_missing_data_to_logger_from_other_logger('Lucero Dillard RoadD1', 'DI-D1-NE', 'DI-D3-NW')
 # select_first_psi_for_all_datasets()
@@ -1803,8 +1839,8 @@
 #     print(dataset[ind]['logger'])
 #     print(dataset[ind]['dates'])
 #     print(dataset[ind]['psi'])
-    # print(f"Field: {dataset['field']:20} Logger: {dataset['logger']:20} \n Dates: \n{' '.join(str(date) for date in dataset['dates']):20}"
-    #       f" \nPSI: \n{' '.join(str(date) for date in dataset['psi']):20}")
+# print(f"Field: {dataset['field']:20} Logger: {dataset['logger']:20} \n Dates: \n{' '.join(str(date) for date in dataset['dates']):20}"
+#       f" \nPSI: \n{' '.join(str(date) for date in dataset['psi']):20}")
 # fix_irr_inches_for_all_fields('2023-04-01', '2023-04-06')
 # copy_last_day_from_old_date_to_new_date('stomato-2023', 'Lucero Dillard RoadD1', "DI-D1-NE", '2023-06-26', '2023-06-27')
 # update_fc_wp('stomato-2023', 'Lucero Dillard RoadD 8, 11', 'DI-D8-C', 36, 22)
@@ -1835,10 +1871,10 @@
 #             # print(field.nickname)
 #             for logger in field.loggers:
 #                 remove_psi(field.name, '2023-11-1', '2023-12-15', '2023')
-                # print(logger.name)
-                # print(f"Old Logger Active: {logger.ir_active}")
-                # logger.ir_active = False
-                # print(f"New Logger Active: {logger.ir_active}")
+# print(logger.name)
+# print(f"Old Logger Active: {logger.ir_active}")
+# logger.ir_active = False
+# print(f"New Logger Active: {logger.ir_active}")
 #                 if logger.active and logger.name == 'SS-333-SW':
 #
 # #                     print(logger.id)
@@ -2177,4 +2213,9 @@
 #     time.sleep(1)
 
 # Decagon.show_pickle()
-# update_all_field_et()
\ No newline at end of file
+# update_all_field_et()
+# change_logger_soil_type('DF-E1-NW', 'Danna FarmsE1', 'Danna Farms', 'Silt Loam')
+# change_logger_soil_type('LL-L2 5-E', 'Lucero SE LafayetteL2 L5', 'Lucero SE Lafayette', 'Loam')
+# update_growers_portal_report_and_images('Lucero Rio Vista')
+# copy_last_day_from_old_date_to_new_date('stomato-2024', 'Bays Ranch38', 'BR-38-N', '2024-06-27', '2024-06-30')
+# update_irrigation_inches_for_whole_table('stomato-2024', 'Lucero NeesField 10', 'LN-10-N')
\ No newline at end of file
Index: Decagon.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>\"\"\"\r\nMain worker file\r\n\r\n\"\"\"\r\nimport csv\r\nimport datetime\r\nimport math\r\nimport pickle\r\nimport time\r\nfrom datetime import date, timedelta\r\nfrom datetime import datetime\r\nfrom itertools import zip_longest\r\nfrom os import path\r\nfrom pathlib import Path\r\nfrom shutil import copyfile\r\n\r\nimport matplotlib.pyplot as plt\r\nimport pandas as pd\r\nfrom google.cloud import bigquery\r\n\r\nimport Technician\r\nfrom AIGameData import AIGameData\r\nfrom CIMIS import CIMIS\r\nfrom CimisStation import CimisStation\r\nfrom CwsiProcessor import CwsiProcessor\r\nfrom DBWriter import DBWriter\r\nfrom Field import Field\r\nfrom Grower import Grower\r\nfrom IrrigationRecommendationExpert import IrrigationRecommendationExpert\r\nfrom Logger import Logger\r\nfrom Saulisms import Saulisms\r\nfrom SwitchTestCase import SwitchTestCase\r\nfrom Technician import Technician\r\nfrom WeatherStation import WeatherStation\r\n\r\nDIRECTORY_YEAR = \"2024\"\r\nPICKLE_DIRECTORY = \"H:\\\\Shared drives\\\\Stomato\\\\\" + DIRECTORY_YEAR + \"\\\\Pickle\\\\\"\r\nBACKUP_PICKLE_DIRECTORY = \"H:\\\\Shared drives\\\\Stomato\\\\\" + DIRECTORY_YEAR + \"\\\\Pickle\\\\Backup\\\\\"\r\nPICKLE_NAME = DIRECTORY_YEAR + \"_pickle.pickle\"\r\nPICKLE_PATH = PICKLE_DIRECTORY + PICKLE_NAME\r\n\r\nNOTIFICATIONS_DIRECTORY = \"H:\\\\Shared drives\\\\Stomato\\\\\" + DIRECTORY_YEAR + \"\\\\Notifications\"\r\n\r\n\r\ndef open_pickle(filename: str = PICKLE_NAME, specific_file_path: str = PICKLE_DIRECTORY):\r\n    \"\"\"\r\n    Function to open a pickle and return its contents.\r\n\r\n    :return:\r\n        List fields\r\n    \"\"\"\r\n\r\n    if path.exists(specific_file_path + filename):\r\n        with open(specific_file_path + filename, 'rb') as f:\r\n            content = pickle.load(f)\r\n        return content\r\n\r\n\r\ndef write_pickle(data, filename: str = PICKLE_NAME, specific_file_path: str = PICKLE_DIRECTORY):\r\n    \"\"\"\r\n    Function to write to a pickle.\r\n\r\n    A pickle is a form of permanent storage used to store any data structure. In this case, it's storing\r\n    the list of fields.\r\n\r\n    :param specific_file_path:\r\n    :param filename:\r\n    :param data: List that you want to have writen\r\n    :return:\r\n    \"\"\"\r\n\r\n    # Backup the old pickle before writing to it\r\n    # backup_pickle()\r\n\r\n    if path.exists(specific_file_path):\r\n        with open(specific_file_path + filename, 'wb') as f:\r\n            pickle.dump(data, f)\r\n\r\n\r\ndef backup_pickle(specific_name=None):\r\n    \"\"\"\r\n\r\n    :return:\r\n    \"\"\"\r\n    now = datetime.today()\r\n    if specific_name is not None:\r\n        file_name = specific_name + \"_pickle_backup_\" + str(now.strftime(\"%m-%d-%y  %I_%M_%S %p\")) + \".pickle\"\r\n    else:\r\n        file_name = \"pickle_backup_\" + str(now.strftime(\"%m-%d-%y  %I_%M_%S %p\")) + \".pickle\"\r\n\r\n    print('Backing up Pickle...')\r\n\r\n    # Check if the pickle we want to copy exists and if it does, copy it\r\n    if path.exists(PICKLE_PATH):\r\n        copyfile(\r\n            PICKLE_PATH,\r\n            BACKUP_PICKLE_DIRECTORY + file_name\r\n        )\r\n\r\n    print('Pickle Backed Up - ', file_name)\r\n\r\n\r\ndef reset_notifications(technicians: list[Technician]):\r\n    \"\"\"\r\n    Resets notifications for each technician\r\n\r\n    :param technicians: List of Technician\r\n    :return: None\r\n    \"\"\"\r\n    for tech in technicians:\r\n        # tech.all_notifications = AllNotifications()\r\n        tech.all_notifications.clear_all_notifications()\r\n\r\n\r\ndef get_all_technicians(growers: list[Grower]) -> list[Technician]:\r\n    \"\"\"\r\n    Get a list of all technicians\r\n\r\n    :param growers: List of Growers\r\n    :return: List of Technicians\r\n    \"\"\"\r\n    all_technicians = []\r\n    for grower in growers:\r\n        if grower.technician not in all_technicians and grower.technician is not None:\r\n            all_technicians.append(grower.technician)\r\n    return all_technicians\r\n\r\n\r\ndef update_et_information(\r\n        get_et: bool = False,\r\n        write_to_db: bool = False,\r\n        start_date=None,\r\n        end_date=None,\r\n        window: int = 10\r\n):\r\n    if get_et:\r\n        yesterdayRaw = date.today() - timedelta(1)\r\n        if start_date is None:\r\n            start_date = yesterdayRaw\r\n        if end_date is None:\r\n            end_date = yesterdayRaw\r\n        start_date = start_date - timedelta(days=window)\r\n        try:\r\n            all_et_data_dicts = pull_all_et_values(str(start_date), str(end_date))\r\n        except Exception as error:\r\n            print('ERROR in get_et')\r\n            print(error)\r\n    if get_et and write_to_db:\r\n        try:\r\n            write_all_et_values_to_db(all_et_data_dicts)\r\n        except Exception as error:\r\n            print('ERROR in write_et_to_db')\r\n            print(error)\r\n\r\n\r\ndef update_information(get_weather: bool = False, get_data: bool = False, write_to_portal: bool = False,\r\n                       write_to_db: bool = False, check_for_notifications: bool = False,\r\n                       email_notifications: bool = False, all_params: bool = False):\r\n    \"\"\"\r\n    Function to update information from each field with a trial.\r\n\r\n    :return:\r\n    \"\"\"\r\n\r\n    # OLD WAY OF SETTING THE CREDENTIALS FOR GOOGLE CLOUD\r\n    # if os.path.exists('C:\\\\Users\\\\javie\\\\Projects\\\\S-TOMAto\\\\credentials.json'):\r\n    #     os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"C:/Users/javie/Projects/S-TOMAto/credentials.json\"\r\n    # elif os.path.exists('C:\\\\Users\\\\javie\\\\PycharmProjects\\\\Stomato\\\\credentials.json'):\r\n    #     os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"C:/Users/javie/PycharmProjects/Stomato/credentials.json\"\r\n    # elif os.path.exists('C:\\\\Users\\\\jsalcedo\\\\PycharmProjects\\\\Stomato\\\\credentials.json'):\r\n    #     os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"C:/Users/jsalcedo/PycharmProjects/Stomato/credentials.json\"\r\n    # elif os.path.exists('C:\\\\Users\\\\jesus\\\\PycharmProjects\\\\Stomato\\\\credentials.json'):\r\n    #     os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"C:/Users/jesus/PycharmProjects/Stomato/credentials.json\"\r\n\r\n    now = datetime.today()\r\n    print(\">>>>>>>>>>>>>>>>>>>S-TOMAto Program<<<<<<<<<<<<<<<<<<<\")\r\n    print(\"                                 - \" + now.strftime(\"%m/%d/%y  %I:%M %p\"))\r\n    print()\r\n    print()\r\n\r\n    update_information_start_time = time.time()\r\n\r\n    if all_params:\r\n        get_data = True\r\n        get_weather = True\r\n        write_to_portal = True\r\n        write_to_db = True\r\n        check_for_notifications = True\r\n        email_notifications = True\r\n\r\n    # Get growers from pickle\r\n    growers = open_pickle()\r\n\r\n    # Backup current pickle\r\n    backup_pickle()\r\n    print()\r\n\r\n    # Grab the cimis stations pickle to pass in to Logger for ET data updating\r\n    cimis_stations_pickle = open_pickle(filename=\"cimisStation.pickle\")\r\n\r\n    # Iterate through growers and call update on each field which will in turn\r\n    #  call update on each logger in each field\r\n    for ind, grower in enumerate(growers):\r\n        # print(ind)\r\n        # try:\r\n        grower.update(cimis_stations_pickle, get_weather=get_weather, get_data=get_data,\r\n                      write_to_portal=write_to_portal, write_to_db=write_to_db,\r\n                      check_for_notifications=check_for_notifications)\r\n        # except Exception as e:\r\n        # Adding so any\r\n        # print(\"Error in \" + str(g.name) + \" . update\")\r\n        # print(\"Error type: \" + str(e))\r\n        # print(\"Writing growers\")\r\n        write_pickle(growers)\r\n\r\n        if check_for_notifications:\r\n            if hasattr(grower, 'technician'):\r\n                technician = grower.technician\r\n                # technician.all_notifications.write_all_notifications_to_txt(technician.name, g.name)\r\n                # technician.all_notifications.write_all_notifications_to_html(technician.name, g.name)\r\n                technician.all_notifications.write_all_notifications_to_html_v2(technician.name, grower.name)\r\n                technician.all_notifications.clear_all_notifications()\r\n\r\n    if check_for_notifications and email_notifications:\r\n        all_technicians = get_all_technicians(growers)\r\n        for tech in all_technicians:\r\n            # list_of_notifcation_files.append(tech.notification_file_path)\r\n            tech.all_notifications.email_all_notifications(tech.name, tech.email, file_type='html')\r\n\r\n    # Write pickle with updated information after update\r\n    print('Writing data to pickle-')\r\n    write_pickle(growers)\r\n\r\n    update_information_end_time = time.time()\r\n    print(\"----------FINISHED----------\")\r\n    update_information_elapsed_time_seconds = update_information_end_time - update_information_start_time\r\n\r\n    update_information_elapsed_time_hours = int(update_information_elapsed_time_seconds // 3600)\r\n    update_information_elapsed_time_minutes = int((update_information_elapsed_time_seconds % 3600) // 60)\r\n    update_information_elapsed_time_seconds = int(update_information_elapsed_time_seconds % 60)\r\n\r\n    print(f\"Update Information execution time: {update_information_elapsed_time_hours}:\"\r\n            + f\"{update_information_elapsed_time_minutes}:\"\r\n            + f\"{update_information_elapsed_time_seconds} (hours:minutes:seconds)\")\r\n    print()\r\n    print()\r\n\r\n\r\ndef notifications_setup(growers, technicians, file_type='txt'):\r\n    \"\"\"\r\n    Setup notifications files for each technician\r\n\r\n    :param growers: List of growers\r\n    :param technicians: List of technicians\r\n    :return:\r\n    \"\"\"\r\n    now = datetime.today()\r\n    # Clear previous notifications\r\n    growers[0].all_notifications.clear_all_notifications()\r\n\r\n    notif_folder = Path(NOTIFICATIONS_DIRECTORY)\r\n\r\n    saulisms = Saulisms()\r\n\r\n    # Setup Tech Notifications\r\n    for tech in technicians:\r\n        technician_name = tech.name\r\n        saying, saying_date = saulisms.get_random_saulism()\r\n\r\n        # SENSOR ERROR\r\n        sensor_error_notif_folder = Path.joinpath(notif_folder, 'Sensor Error')\r\n        sensor_error_file_name = technician_name + \"_sensor_error_notifications_\" + str(\r\n            now.strftime(\"%m-%d-%y\")\r\n        ) + \".\" + file_type\r\n        sensor_error_file_path = sensor_error_notif_folder / sensor_error_file_name\r\n\r\n        if file_type == 'txt':\r\n            with open(sensor_error_file_path, 'a') as the_file:\r\n                the_file.write(\"\\n\")\r\n                the_file.write(\r\n                    \">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>  SENSOR ERRORS  <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\\n\"\r\n                )\r\n                the_file.write(\r\n                    \"                                                      \" + now.strftime(\"%m/%d/%y  %I:%M %p\")\r\n                )\r\n        elif file_type == 'html':\r\n            with open(sensor_error_file_path, 'a') as the_file:\r\n                the_file.write(\"<!DOCTYPE html>\\n\")\r\n                the_file.write(\"<html>\\n\")\r\n                the_file.write(\"<head>\\n\")\r\n                the_file.write(\"<title>Sensor Error Notifications</title>\\n\")\r\n                the_file.write(\"</head>\\n\")\r\n                the_file.write(\"<body>\\n\")\r\n                the_file.write(\"<style>\\n\")\r\n                the_file.write(\"table { table-layout: fixed; }\\n\")\r\n                the_file.write(\"table, th, td {border: 1px solid black; border-collapse: collapse;}\\n\")\r\n                the_file.write(\"th, td {padding: 15px;}\\n\")\r\n                the_file.write(\"tr:nth-child(even) {background-color: #F0F8FF;}\\n\")\r\n                the_file.write(\"</style>\\n\")\r\n                the_file.write(\"<h2>SENSOR ERRORS</h2>\\n\")\r\n                the_file.write(f\"<h2>{now.strftime('%m/%d/%y  %I:%M %p')}</h2>\\n\")\r\n                the_file.write(f\"<h2 style='font-style: italic; font-size: 150%;'>\\\"{saying}\\\", {saying_date}</h2>\\n\")\r\n                the_file.write(\"<hr>\\n\")\r\n\r\n        # TECH WARNINGS\r\n        # Disabling for the time being\r\n        # tech_warning_notif_folder = Path.joinpath(notif_folder, 'Tech Warning')\r\n        # tech_warning_file_name = technician_name + \"_tech_warning_notifications_\" + str(\r\n        #     now.strftime(\"%m-%d-%y\")\r\n        # ) + \".txt\"\r\n        # tech_warning_file_path = tech_warning_notif_folder / tech_warning_file_name\r\n        #\r\n        # with open(tech_warning_file_path, 'a') as the_file:\r\n        #     the_file.write(\"\\n\")\r\n        #     the_file.write(\r\n        #         \">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>  TECHNICIAN WARNINGS  <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\\n\"\r\n        #     )\r\n        #     the_file.write(\r\n        #         \"                                                      \" + now.strftime(\"%m/%d/%y  %I:%M %p\")\r\n        #     )\r\n\r\n        # LOGGER SETUPS\r\n        logger_setups_notif_folder = Path.joinpath(notif_folder, 'Logger Setups')\r\n        logger_setups_file_name = technician_name + \"_logger_setups_notifications_\" + str(\r\n            now.strftime(\"%m-%d-%y\")\r\n        ) + \".\" + file_type\r\n        logger_setups_file_path = logger_setups_notif_folder / logger_setups_file_name\r\n\r\n        if file_type == 'txt':\r\n            with open(logger_setups_file_path, 'a') as the_file:\r\n                the_file.write(\"\\n\")\r\n                the_file.write(\r\n                    \">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>  NEW FIELDS CREATED  <<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<\\n\"\r\n                )\r\n                the_file.write(\r\n                    \"                                                      \" + now.strftime(\"%m/%d/%y  %I:%M %p\")\r\n                )\r\n        elif file_type == 'html':\r\n            with open(logger_setups_file_path, 'a') as the_file:\r\n                the_file.write(\"<!DOCTYPE html>\\n\")\r\n                the_file.write(\"<html>\\n\")\r\n                the_file.write(\"<head>\\n\")\r\n                the_file.write(\"<title>Logger Setups Notifications</title>\\n\")\r\n                the_file.write(\"</head>\\n\")\r\n                the_file.write(\"<body>\\n\")\r\n                the_file.write(\"<style>\\n\")\r\n                the_file.write(\"table { table-layout: fixed; }\\n\")\r\n                the_file.write(\"table, th, td {border: 1px solid black; border-collapse: collapse;}\\n\")\r\n                the_file.write(\"th, td {padding: 15px;}\\n\")\r\n                the_file.write(\"tr:nth-child(even) {background-color: #F0F8FF;}\\n\")\r\n                the_file.write(\"</style>\\n\")\r\n                the_file.write(\"<h2>LOGGER SETUPS</h2>\\n\")\r\n                the_file.write(f\"<h2>{now.strftime('%m/%d/%y  %I:%M %p')}</h2>\\n\")\r\n                the_file.write(f\"<h2 style='font-style: italic; font-size: 150%;'>\\\"{saying}\\\", {saying_date}</h2>\\n\")\r\n\r\n\r\ndef add_field_to_grower(growerName, field):\r\n    \"\"\"\r\n\r\n\r\n    :param growerName:\r\n    :param field:\r\n    :return:\r\n    \"\"\"\r\n    growers = open_pickle()\r\n    grower = None\r\n    for item in growers:\r\n        if item.name == growerName:\r\n            grower = item\r\n            break\r\n        else:\r\n            grower = None\r\n\r\n    field.grower = grower\r\n    for logger in field.loggers:\r\n        if logger.grower is None:\r\n            logger.grower = grower\r\n        if logger.field is None:\r\n            logger.field = field\r\n    grower.fields.append(field)\r\n    write_pickle(growers)\r\n\r\n\r\ndef remove_field(grower_name, field_name, avoid_user_input=False):\r\n    \"\"\"\r\n    Function to remove field from a grower\r\n\r\n    :param grower_name:\r\n    :param field_name:\r\n    :param avoid_user_input:\r\n    \"\"\"\r\n    growers = open_pickle()\r\n    for g in growers:\r\n        if g.name == grower_name:\r\n            for ind, f in enumerate(g.fields):\r\n                if f.name == field_name:\r\n                    print('Field found:')\r\n                    f.to_string()\r\n                    print()\r\n                    print('About to remove ' + g.name + ' - ' + f.name + ' at index ' + str(ind))\r\n\r\n                    if avoid_user_input:\r\n                        confirm = 'y'\r\n                    else:\r\n                        confirm = input('Confirm? (Y/N) ').lower().strip()\r\n\r\n                    if confirm[:1] == 'y':\r\n                        print('Confirmed - Field removed')\r\n                        del g.fields[ind]\r\n                        write_pickle(growers)\r\n                    elif confirm[:1] == 'n':\r\n                        print('Canceled')\r\n                    else:\r\n                        print('Invalid Input')\r\n                # else:\r\n                # print('Field not found')\r\n\r\n\r\ndef deactivate_grower(grower_name: str) -> bool:\r\n    \"\"\"\r\n    Function to deactivate a grower to ensure his fields do not get updated\r\n\r\n    :param grower_name:\r\n    :return:\r\n    \"\"\"\r\n    success = False\r\n    growers = open_pickle()\r\n    for g in growers:\r\n        if g.name == grower_name:\r\n            print('Grower {} found:'.format(g.name))\r\n            if g.active:\r\n                g.deactivate()\r\n                success = True\r\n    write_pickle(growers)\r\n    return success\r\n\r\n\r\ndef deactivate_field(grower_name: str, field_name: str) -> bool:\r\n    \"\"\"\r\n    Function to deactivate a field so that the loggers in it no longer update\r\n\r\n    :param grower_name: String of the grower name\r\n    :param field_name: String of the field name\r\n    :return: success - Boolean of whether a field was successfully deactivate or not\r\n    \"\"\"\r\n    success = False\r\n    growers = open_pickle()\r\n    for g in growers:\r\n        if g.name == grower_name:\r\n            for f in g.fields:\r\n                if f.name == field_name:\r\n                    print('Field {} found:'.format(f.name))\r\n                    if f.active:\r\n                        f.deactivate()\r\n                        success = True\r\n    write_pickle(growers)\r\n    return success\r\n\r\n\r\ndef deactivate_logger(grower_name: str, field_name: str, logger_id: str) -> bool:\r\n    \"\"\"\r\n    Function to deactivate a logger so its data no longer gets processed\r\n\r\n    :param grower_name:\r\n    :param field_name:\r\n    :param logger_id:\r\n    :return: success - Boolean of whether a logger was successfully deactivated or not\r\n    \"\"\"\r\n    success = False\r\n    growers = open_pickle()\r\n    for g in growers:\r\n        if g.name == grower_name:\r\n            for f in g.fields:\r\n                if f.name == field_name:\r\n                    for logger in f.loggers:\r\n                        if logger.id == logger_id:\r\n                            print('Logger {} found:'.format(logger.id))\r\n                            if logger.active:\r\n                                logger.deactivate()\r\n                                success = True\r\n    write_pickle(growers)\r\n    return success\r\n\r\n\r\ndef remove_grower(grower_name: str) -> None:\r\n    \"\"\"\r\n    Deprecated function. We now 'deactivate' the grower instead to leave them in the pickle\r\n    Function to remove a grower from the pickle\r\n\r\n    :param grower_name: String of the grower name\r\n    \"\"\"\r\n    growers = open_pickle()\r\n    for ind, g in enumerate(growers):\r\n        if g.name == grower_name:\r\n            print('Grower found:')\r\n            g.to_string()\r\n            print()\r\n            print('About to remove ' + g.name + ' at index ' + str(ind))\r\n\r\n            confirm = input('Confirm? (Y/N) ').lower().strip()\r\n            if confirm[:1] == 'y':\r\n                print('Confirmed - Grower removed')\r\n                del growers[ind]\r\n                write_pickle(growers)\r\n            elif confirm[:1] == 'n':\r\n                print('Canceled')\r\n            else:\r\n                print('Invalid Input')\r\n    print('Growers remaining:')\r\n    for g in growers:\r\n        print(g.name)\r\n\r\n\r\ndef remove_last_grower():\r\n    \"\"\"\r\n    Function to remove the last grower in the pickle.\r\n    Typically, used to remove a grower that was added by mistake\r\n\r\n    \"\"\"\r\n    growers = open_pickle()\r\n    del growers[-1]\r\n    write_pickle(growers)\r\n\r\n\r\ndef removeLogger(growerName: str, fieldName: str, loggerID: str) -> None:\r\n    \"\"\"\r\n    Function to remove a logger from a grower's field\r\n\r\n    :param growerName: String of the grower name\r\n    :param fieldName: String of the field name\r\n    :param loggerID: String of the logger id\r\n    \"\"\"\r\n    growers = open_pickle()\r\n    for g in growers:\r\n        if g.name == growerName:\r\n            for f in g.fields:\r\n                if f.name == fieldName:\r\n                    for ind, l in enumerate(f.loggers):\r\n                        if l.id == loggerID:\r\n                            print('Logger found:')\r\n                            l.to_string()\r\n                            print()\r\n                            print(\r\n                                'About to remove ' + g.name + ' - ' + f.name + ' - ' + l.id + ' at index ' + str(\r\n                                    ind\r\n                                )\r\n                            )\r\n\r\n                            confirm = input('Confirm? (Y/N) ').lower().strip()\r\n                            if confirm[:1] == 'y':\r\n                                print('Confirmed - Field removed')\r\n                                del f.loggers[ind]\r\n                                write_pickle(growers)\r\n                            elif confirm[:1] == 'n':\r\n                                print('Canceled')\r\n                            else:\r\n                                print('Invalid Input')\r\n\r\n\r\ndef addGrowerToGrowers(grower: Grower) -> None:\r\n    \"\"\"\r\n    Function to add a grower to the growers in the pickle\r\n\r\n    :param grower:\r\n    \"\"\"\r\n    growers = open_pickle()\r\n\r\n    growers.append(grower)\r\n\r\n    write_pickle(growers)\r\n\r\n\r\ndef show_pickle(filename: str = PICKLE_NAME, specific_file_path: str = PICKLE_DIRECTORY):\r\n    \"\"\"\r\n        Function to print out the contents of the pickle.\r\n\r\n        :return:\r\n    \"\"\"\r\n    data = open_pickle(filename=filename, specific_file_path=specific_file_path)\r\n    print(\"PICKLE CONTENTS\")\r\n    for d in data:\r\n        d.to_string()\r\n\r\n\r\ndef only_certain_growers_update(\r\n        growerNames: list[str],\r\n        get_weather: bool = False,\r\n        get_data: bool = False,\r\n        write_to_portal: bool = False,\r\n        write_to_db: bool = False,\r\n        check_for_notifications: bool = False,\r\n        subtract_from_mrid: int = 0\r\n) -> None:\r\n    \"\"\"\r\n    Function to only update certain growers\r\n\r\n    :param subtract_from_mrid: Int used to subtract a specific amount from the logger MRIDs for API calls\r\n    :param growerNames: List of grower names in string form\r\n    :param get_et: Boolean, True if you want to get ET, False otherwise\r\n    :param get_weather: Boolean, True if you want to get Weather, False otherwise\r\n    :param get_data: Boolean, True if you want to get Data, False otherwise\r\n    :param write_to_portal: Boolean, True if you want to write data to grower portal, False otherwise\r\n    :param write_to_db: Boolean, True if you want to write to DB, False otherwise\r\n    :param check_for_notifications: Boolean, True if you want to check for notifications, False otherwise\r\n    \"\"\"\r\n    # Grab the cimis stations pickle to pass in to Logger for ET data updating\r\n    cimis_stations_pickle = open_pickle(filename=\"cimisStation.pickle\")\r\n\r\n    print(\"Only updating: \" + str(growerNames))\r\n    allGrowers = open_pickle()\r\n    for g in allGrowers:\r\n        if g.name in growerNames:\r\n            g.updated = False\r\n            for field in g.fields:\r\n                field.updated = False\r\n                for logger in field.loggers:\r\n                    logger.updated = False\r\n            g.update(\r\n                cimis_stations_pickle,\r\n                get_weather=get_weather,\r\n                get_data=get_data,\r\n                write_to_portal=write_to_portal,\r\n                write_to_db=write_to_db,\r\n                check_for_notifications=check_for_notifications,\r\n                subtract_from_mrid=subtract_from_mrid\r\n            )\r\n    write_pickle(allGrowers)\r\n\r\n\r\ndef only_certain_growers_field_update(\r\n        grower_name: str,\r\n        field_name: str,\r\n        get_weather: bool = False,\r\n        get_data: bool = False,\r\n        write_to_portal: bool = False,\r\n        write_to_db: bool = False,\r\n        check_for_notifications: bool = False,\r\n        subtract_from_mrid: int = 0\r\n) -> None:\r\n    \"\"\"\r\n    Function to only update a certain field for a grower\r\n\r\n    :param subtract_from_mrid:\r\n    :param grower_name: String of grower name\r\n    :param field_name: String of field name\r\n    :param get_et: Boolean, True if you want to get ET, False otherwise\r\n    :param get_weather: Boolean, True if you want to get Weather, False otherwise\r\n    :param get_data: Boolean, True if you want to get Data, False otherwise\r\n    :param write_to_portal: Boolean, True if you want to write data to grower portal sheet, False otherwise\r\n    :param write_to_db: Boolean, True if you want to write to DB, False otherwise\r\n    :param check_for_notifications: Boolean, True if you want to check for notifications, False otherwise\r\n    \"\"\"\r\n    allGrowers = open_pickle()\r\n    cimis_stations_pickle = open_pickle(filename=\"cimisStation.pickle\")\r\n    for g in allGrowers:\r\n        if g.name == grower_name:\r\n            for f in g.fields:\r\n                if f.name == field_name:\r\n                    f.updated = False\r\n                    for logger in f.loggers:\r\n                        logger.updated = False\r\n                    f.update(\r\n                        cimis_stations_pickle = cimis_stations_pickle,\r\n                        get_weather=get_weather,\r\n                        get_data=get_data,\r\n                        write_to_portal=write_to_portal,\r\n                        write_to_db=write_to_db,\r\n                        check_for_notifications=check_for_notifications,\r\n                        subtract_from_mrid=subtract_from_mrid\r\n                    )\r\n    write_pickle(allGrowers)\r\n\r\n\r\ndef only_certain_growers_fields_update(\r\n        fields: list[str],\r\n        get_weather: bool = False,\r\n        get_data: bool = False,\r\n        write_to_portal: bool = False,\r\n        write_to_db: bool = False,\r\n        check_for_notifications: bool = False,\r\n        subtract_from_mrid: int = 0,\r\n        specific_mrid = None\r\n) -> None:\r\n    \"\"\"\r\n    Function to only update certain fields\r\n\r\n    :param fields: List of strings\r\n    :param get_et: Boolean, True if you want to get ET, False otherwise\r\n    :param get_weather: Boolean, True if you want to get Weather, False otherwise\r\n    :param get_data: Boolean, True if you want to get Data, False otherwise\r\n    :param write_to_portal: Boolean, True if you want to write data to grower portal sheet, False otherwise\r\n    :param write_to_db: Boolean, True if you want to write to DB, False otherwise\r\n    :param check_for_notifications: Boolean, True if you want to check for notifications, False otherwise\r\n    \"\"\"\r\n    allGrowers = open_pickle()\r\n    cimis_stations_pickle = open_pickle(filename=\"cimisStation.pickle\")\r\n    for g in allGrowers:\r\n        for f in g.fields:\r\n            if f.name in fields:\r\n                f.updated = False\r\n                for l in f.loggers:\r\n                    l.updated = False\r\n                f.update(\r\n                    cimis_stations_pickle=cimis_stations_pickle,\r\n                    get_weather=get_weather,\r\n                    get_data=get_data,\r\n                    write_to_portal=write_to_portal,\r\n                    write_to_db=write_to_db,\r\n                    check_for_notifications=check_for_notifications,\r\n                    subtract_from_mrid=subtract_from_mrid,\r\n                    specific_mrid=specific_mrid\r\n                )\r\n    write_pickle(allGrowers)\r\n\r\n\r\ndef only_certain_growers_field_logger_update(\r\n        grower_name: str,\r\n        field_name: str,\r\n        logger_name: str = '',\r\n        logger_id: str = '',\r\n        write_to_db: bool = False,\r\n        check_for_notifications: bool = False,\r\n        specific_mrid: float = None,\r\n        subtract_from_mrid: float = 0\r\n) -> None:\r\n    \"\"\"\r\n    Function to update a specific Logger\r\n\r\n    :param grower_name: String of grower name\r\n    :param field_name: String of field name\r\n    :param logger_name: String of logger ID\r\n    :param write_to_db: Boolean, True if you want to write to DB, False otherwise\r\n    :param check_for_notifications: Boolean, True if you want to check notifications\r\n    :param specific_mrid: Float value if you want to call METER API with a specific MRID\r\n    :param subtract_from_mrid: Float value if you want to subtract a certain amount from the MRID\r\n                                before calling the METER API\r\n    \"\"\"\r\n    cimis_stations_pickle = open_pickle(filename=\"cimisStation.pickle\")\r\n    all_growers = open_pickle()\r\n    logger_to_update = None\r\n    for grower in all_growers:\r\n        if grower.name == grower_name:\r\n            for field in grower.fields:\r\n                if field.name == field_name:\r\n                    field.updated = False\r\n                    for logger in field.loggers:\r\n                        if len(logger_name) > 0 and logger.name == logger_name:\r\n                                logger_to_update = logger\r\n                        if len(logger_id) > 0 and logger.id == logger_id:\r\n                                logger_to_update = logger\r\n                        if logger_to_update is not None:\r\n                            logger_to_update.updated = False\r\n                            logger_to_update.update(\r\n                                cimis_stations_pickle,\r\n                                write_to_db=write_to_db,\r\n                                check_for_notifications=check_for_notifications, specific_mrid=specific_mrid,\r\n                                subtract_from_mrid=subtract_from_mrid\r\n                            )\r\n                            break\r\n    write_pickle(all_growers)\r\n\r\n\r\ndef setup_grower(grower_name: str, technician_name: str, email: str = '', region: str = '',\r\n                 active: bool = True) -> Grower:\r\n    \"\"\"\r\n    Function to create a Grower object\r\n\r\n    :param technician_name: \r\n    :param grower_name: String of the grower name\r\n    :param email: String of the grower email\r\n    :param region: String of the grower region\r\n    :param active: Boolean indicating if the grower is active\r\n    :return:\r\n    \"\"\"\r\n    fields = []\r\n\r\n    tech = None\r\n    growers = open_pickle()\r\n    for grower in growers:\r\n        if grower.technician.name == technician_name:\r\n            tech = grower.technician\r\n            break\r\n    if tech is None:\r\n        tech = Technician(technician_name, '')\r\n\r\n    new_grower = Grower(grower_name, fields, tech, email, region=region, active=active)\r\n    growers.append(new_grower)\r\n    write_pickle(growers)\r\n    return new_grower\r\n\r\n\r\ndef setup_field(\r\n        field_name,\r\n        lat,\r\n        long,\r\n        cimis_station,\r\n        acres,\r\n        crop_type,\r\n        grower=None,\r\n        active=True,\r\n        field_type='Commercial'\r\n):\r\n    \"\"\"\r\n    Function to create a Field object\r\n\r\n    :param field_type:\r\n    :param crop_type:\r\n    :param acres:\r\n    :param field_name:\r\n    :param lat:\r\n    :param long:\r\n    :param cimis_station:\r\n    :param grower:\r\n    :param active:\r\n    :return:\r\n    \"\"\"\r\n    loggers = []\r\n    field = Field(field_name, loggers, lat, long, cimis_station, acres, crop_type, grower=grower, active=active, field_type=field_type)\r\n    return field\r\n\r\n\r\ndef setup_logger(logger_id, password, name, crop_type, soil_type, gpm, acres, loggerDirection, lat, long, install_date,\r\n                 planting_date=None, grower=None, field=None, rnd=False, active=True):\r\n    \"\"\"\r\n    Function to set up a new Logger\r\n\r\n    :param install_date:\r\n    :param soil_type:\r\n    :param name:\r\n    :param lat:\r\n    :param long:\r\n    :param loggerDirection:\r\n    :param logger_id:\r\n    :param password:\r\n    :param crop_type:\r\n    :param gpm:\r\n    :param acres:\r\n    :param planting_date:\r\n    :param grower:\r\n    :param field:\r\n    :param rnd:\r\n    :param active:\r\n    :return:\r\n    \"\"\"\r\n    logger = Logger(logger_id, password, name, crop_type, soil_type, gpm, acres, loggerDirection, install_date, lat, long, grower=grower, field=field,\r\n                    planting_date=planting_date, rnd=rnd, active=active)\r\n    return logger\r\n\r\n\r\ndef set_loggers_rnd(logger_ids):\r\n    \"\"\"\r\n\r\n    :param logger_ids: \r\n    \"\"\"\r\n    logger_ids_list = logger_ids\r\n    growers = open_pickle()\r\n    for grower in growers:\r\n        for field in grower.fields:\r\n            for logger in field.loggers:\r\n                for logger_id in logger_ids_list:\r\n                    if logger.id == logger_id:\r\n                        print(\"Found id: \" + str(logger_id) + \" and set R&D to TRUE\")\r\n                        logger.rnd = True\r\n                        logger_ids_list.remove(logger_id)\r\n                        print(\" Remaining IDs:\" + str(logger_ids_list))\r\n                        logger.to_string()\r\n    write_pickle(growers)\r\n\r\n\r\ndef get_grower(grower_name: str) -> Grower:\r\n    \"\"\"\r\n    Function to get a grower object from the pickle\r\n\r\n    :param grower_name: String of grower name\r\n    :return: Grower object\r\n    \"\"\"\r\n    growers = open_pickle()\r\n    for grower in growers:\r\n        if grower.name == grower_name:\r\n            return grower\r\n\r\n\r\ndef get_gpm(field_name: str, logger_name: str) -> float:\r\n    \"\"\"\r\n    Function to get the GPM for a logger in a field\r\n\r\n    :param field_name: String of the field name\r\n    :param logger_name: String of the logger name\r\n    :return: float of the Gallons per Minute\r\n    \"\"\"\r\n    growers = open_pickle()\r\n    for g in growers:\r\n        for f in g.fields:\r\n            if f.name == field_name:\r\n                for log in f.loggers:\r\n                    if log.name == logger_name:\r\n                        # print(log.gpm)\r\n                        # print(type(log.gpm))\r\n                        return log.gpm\r\n\r\n\r\ndef get_acres(field_name: str, logger_name: str) -> float:\r\n    \"\"\"\r\n    Function to get the acres for a logger in a field\r\n\r\n    :param logger_name: String of the logger name\r\n    :param field_name: String of the field name\r\n    :return: Float of the acres\r\n    \"\"\"\r\n    growers = open_pickle()\r\n    for g in growers:\r\n        for f in g.fields:\r\n            if f.name == field_name:\r\n                for log in f.loggers:\r\n                    if log.name == logger_name:\r\n                        # print(log.acres)\r\n                        return log.irrigation_set_acres\r\n\r\ndef show_grower(grower_name: str) -> None:\r\n    \"\"\"\r\n    Function to call a grower's to_string()\r\n\r\n    :param grower_name: String of the grower name\r\n    \"\"\"\r\n    g = get_grower(grower_name)\r\n    if g is not None:\r\n        g.to_string()\r\n    else:\r\n        print('Grower -' + grower_name + '- not found')\r\n\r\n\r\ndef get_field(field_name: str, grower_name: str = '') -> Field:\r\n    \"\"\"\r\n    Function to get a field\r\n\r\n    :param field_name: String for the field name\r\n    :param grower_name: Optional parameter of the string for the grower name\r\n    :return: Field object of the field\r\n    \"\"\"\r\n    if grower_name:\r\n        grower = get_grower(grower_name)\r\n        for field in grower.fields:\r\n            if field.name == field_name:\r\n                return field\r\n    else:\r\n        growers = open_pickle()\r\n        for grower in growers:\r\n            for field in grower.fields:\r\n                if field.name == field_name:\r\n                    return field\r\n\r\n\r\ndef show_field(field_name: str, grower_name: str = '') -> None:\r\n    \"\"\"\r\n    Function to pring out to console a field's information (to_string*())\r\n\r\n    :param field_name: String of the field name\r\n    :param grower_name: String of the grower name\r\n    \"\"\"\r\n    f = get_field(field_name, grower_name)\r\n    if f is not None:\r\n        f.to_string()\r\n    else:\r\n        print('Field -' + field_name + '- not found')\r\n\r\n\r\ndef write_new_historical_et_to_db(table, filename, historicalET=\"Historical_ET\", overwrite=False):\r\n    \"\"\"\r\n    Function to write mew historical ET table to DB\r\n\r\n    :param table:\r\n    :param filename:\r\n    :param historicalET:\r\n    :param overwrite:\r\n    \"\"\"\r\n    schema = [\r\n        bigquery.SchemaField(\"Year4\", \"DATE\"),\r\n        bigquery.SchemaField(\"Year4ET\", \"FLOAT\"),\r\n        bigquery.SchemaField(\"Year3\", \"DATE\"),\r\n        bigquery.SchemaField(\"Year3ET\", \"FLOAT\"),\r\n        bigquery.SchemaField(\"Year2\", \"DATE\"),\r\n        bigquery.SchemaField(\"Year2ET\", \"FLOAT\"),\r\n        bigquery.SchemaField(\"Year1\", \"DATE\"),\r\n        bigquery.SchemaField(\"Year1ET\", \"FLOAT\"),\r\n        bigquery.SchemaField(\"Average\", \"FLOAT\"),\r\n    ]\r\n\r\n    dbwriter = DBWriter()\r\n    project = 'stomato-info'\r\n    dbwriter.write_to_table_from_csv(historicalET, table, filename, schema, project, overwrite=overwrite)\r\n\r\n\r\ndef write_new_historical_et_to_db_2(dataset_id, table, data, filename=\"HistoricalET.csv\", overwrite=False):\r\n    \"\"\"\r\n    Function writes irr scheduling data into csv then creates a db table from csv given a data table of dates and etos\r\n\r\n    :param dataset_id:\r\n    :param table: The station number\r\n    :param data: Dictionary of dates and etos\r\n    :param filename:\r\n    :param overwrite:\r\n    \"\"\"\r\n    print('\\t-Writing data to csv')\r\n    with open(filename, \"w\", newline='') as outfile:\r\n        writer = csv.writer(outfile)\r\n        writer.writerow(data.keys())\r\n        writer.writerows(zip_longest(*data.values()))\r\n    print('\\t...Done - file: ' + filename)\r\n    keys_list = list(data.keys())\r\n    schema = [\r\n        bigquery.SchemaField(keys_list[0], \"DATE\", mode=\"NULLABLE\"),\r\n        bigquery.SchemaField(keys_list[1], \"Float\", mode=\"NULLABLE\"),\r\n        bigquery.SchemaField(keys_list[2], \"DATE\", mode=\"NULLABLE\"),\r\n        bigquery.SchemaField(keys_list[3], \"Float\", mode=\"NULLABLE\"),\r\n        bigquery.SchemaField(keys_list[4], \"DATE\", mode=\"NULLABLE\"),\r\n        bigquery.SchemaField(keys_list[5], \"Float\", mode=\"NULLABLE\"),\r\n        bigquery.SchemaField(keys_list[6], \"DATE\", mode=\"NULLABLE\"),\r\n        bigquery.SchemaField(keys_list[7], \"Float\", mode=\"NULLABLE\"),\r\n        bigquery.SchemaField(keys_list[8], \"DATE\", mode=\"NULLABLE\"),\r\n        bigquery.SchemaField(keys_list[9], \"Float\", mode=\"NULLABLE\"),\r\n        bigquery.SchemaField(keys_list[10], \"Float\", mode=\"NULLABLE\"),\r\n    ]\r\n    dbwriter = DBWriter()\r\n    print(\"\\tWriting Data to DB\")\r\n    project = 'stomato-info'\r\n    dbwriter.write_to_table_from_csv(dataset_id, table, filename, schema, project, overwrite=overwrite)\r\n\r\ndef update_irr_scheduling(table, fieldName, data, filename=\"irrScheduling.csv\", overwrite=False, logger=None):\r\n    \"\"\"\r\n    Function writes irr scheduling data into csv then creates a db table from csv\r\n\r\n    :param table:\r\n    :param fieldName:\r\n    :param data:\r\n    :param filename:\r\n    :param overwrite:\r\n    \"\"\"\r\n    print('\\tWriting data to csv...')\r\n    with open(filename, \"w\", newline='') as outfile:\r\n        writer = csv.writer(outfile)\r\n        writer.writerow(data.keys())\r\n        writer.writerows(zip_longest(*data.values()))\r\n    print('\\t<-Writing done - file: ' + filename)\r\n    schema = [\r\n        bigquery.SchemaField(\"current_date\", \"DATE\", mode=\"REQUIRED\"),\r\n        bigquery.SchemaField(\"historical_eto\", \"FLOAT\", mode=\"NULLABLE\"),\r\n        bigquery.SchemaField(\"kc\", \"FLOAT\", mode=\"REQUIRED\"),\r\n        bigquery.SchemaField(\"historical_etc\", \"FLOAT\", mode=\"NULLABLE\"),\r\n        bigquery.SchemaField(\"historical_hours\", \"FLOAT\", mode=\"NULLABLE\")\r\n    ]\r\n    dbwriter = DBWriter()\r\n    print(\"\\tWriting Irrigation Scheduling Table to DB...\")\r\n    # project = 'stomato-' + DATABASE_YEAR\r\n    project = dbwriter.get_db_project(logger.crop_type)\r\n    dbwriter.write_to_table_from_csv(fieldName, table, filename, schema, project, overwrite=overwrite)\r\n\r\n\r\ndef get_all_current_cimis_stations():\r\n    \"\"\"\r\n    Get all the cimis stations for where we have equipment currently in the pickle\r\n\r\n    :return:\r\n    \"\"\"\r\n    cimis_stations = []\r\n    growers = open_pickle()\r\n    for g in growers:\r\n        for f in g.fields:\r\n            if f.cimis_station not in cimis_stations:\r\n                cimis_stations.append(f.cimis_station)\r\n    print('Cimis Stations currently in pickle: ' + str(cimis_stations))\r\n    return cimis_stations\r\n\r\n\r\ndef pull_all_et_values(start_date, end_date):\r\n    \"\"\"\r\n    Function to grab all ET values from a given starDate through to a given endDate\r\n    for all CIMIS stations in our current pickle\r\n\r\n    :param start_date:\r\n    :param end_date:\r\n    :return:\r\n    \"\"\"\r\n    cimis = CIMIS()\r\n\r\n    cimis_stations_pickle = open_pickle(filename=\"cimisStation.pickle\")\r\n\r\n    all_cimis_station_et = cimis.get_all_stations_et_data(cimis_stations_pickle, start_date, end_date)\r\n\r\n    write_pickle(cimis_stations_pickle, filename=\"cimisStation.pickle\")\r\n    return all_cimis_station_et\r\n\r\n\r\ndef update_all_eto_values(start_date: str, end_date: str):\r\n    \"\"\"\r\n    Function to grab all ETo values from a given start_date through to a given end_date\r\n    for all CIMIS stations in our current pickle and update them to the DB to ensure we have current/correct values\r\n\r\n    :param start_date: str indicating first day of data to pull from CIMIS\r\n    :param end_date: str indicating the last day of data to pull from CIMIS\r\n    :return:\r\n    \"\"\"\r\n    record_limit = 1750\r\n    cimis = CIMIS()\r\n    cimis_stations_pickle = open_pickle(filename=\"cimisStation.pickle\")\r\n    # Convert the string dates to datetime objects\r\n    date1 = datetime.strptime(start_date, '%Y-%m-%d')\r\n    date2 = datetime.strptime(end_date, '%Y-%m-%d')\r\n    # Calculate the difference between the two dates\r\n    difference = date2 - date1\r\n\r\n    station_limit = math.floor(record_limit / difference.days)\r\n    all_cimis_station_eto = {}\r\n    for i in range(0, len(cimis_stations_pickle), station_limit):\r\n        current_pairs = cimis_stations_pickle[i:i+station_limit]\r\n        print(f'Pulling data from CIMIS for {len(current_pairs)} stations')\r\n        some_stations = cimis.get_all_stations_et_data(current_pairs, start_date, end_date)\r\n        all_cimis_station_eto.update(some_stations)\r\n\r\n    print(f'Preparing to write CIMIS stations data to DB')\r\n    write_all_et_values_to_db(all_cimis_station_eto, overwrite=True)\r\n\r\n\r\ndef remove_duplicates_already_in_et_db(db_dates, table_id):\r\n    \"\"\"\r\n    Removes duplicate rows in the ET DB table so new rows on those dates can be inserted\r\n    :param db_dates: list of dates to delete from the DB table at the dataset id\r\n    :param table_id:\r\n    :return:\r\n    \"\"\"\r\n    # Remove the rows first then append the new lines\r\n    project = 'stomato-info'\r\n    dbwriter = DBWriter()\r\n    print()\r\n    print('\\tChecking for and removing duplicate ET data')\r\n    first_date = db_dates[0]\r\n    last_date = db_dates[-1]\r\n    dml_statement = (f\"Delete From `{project}.ET.{table_id}`\"\r\n                     f\"Where Date between '{first_date}' and '{last_date}'\")\r\n    dbwriter.run_dml(dml_statement)\r\n\r\n    print('\\t\\t<-Duplicate ET data removal complete.')\r\n\r\n\r\ndef write_all_et_values_to_db(all_cimis_station_et, overwrite: bool = False):\r\n    \"\"\"\r\n    Write all ET values for all CIMIS stations in our current pickle\r\n    to their corresponding ET tables in the DB\r\n\r\n    :return:\r\n    \"\"\"\r\n    print('\\n\\tWriting ET values to DB...')\r\n    dbwriter = DBWriter()\r\n    project = 'stomato-info'\r\n    dataset_id = 'ET'\r\n    schema = [\r\n        bigquery.SchemaField(\"date\", \"DATE\"),\r\n        bigquery.SchemaField(\"eto\", \"FLOAT\"),\r\n    ]\r\n    filename = 'all et.csv'\r\n\r\n    for et_data_dict_key in all_cimis_station_et:\r\n        if all_cimis_station_et is not None:\r\n            db_dates_list = [date for date in all_cimis_station_et[et_data_dict_key]['dates']]\r\n            # Remove any duplicates in the DB based on the dates\r\n            remove_duplicates_already_in_et_db(db_dates_list, et_data_dict_key)\r\n\r\n\r\n        # Write to the ET table in DB\r\n        print(f'\\tWriting station {et_data_dict_key} data to csv')\r\n        keys = [\"dates\", \"eto\"]\r\n        just_et_data = {key: all_cimis_station_et[et_data_dict_key][key] for key in keys}\r\n\r\n        with open(filename, \"w\", newline='') as outfile:\r\n            writer = csv.writer(outfile)\r\n            writer.writerow(just_et_data.keys())\r\n            writer.writerows(zip_longest(*just_et_data.values()))\r\n        print('\\t\\t<-csv Done')\r\n\r\n        table_id = et_data_dict_key\r\n        print('\\tWriting to DB')\r\n        dbwriter.write_to_table_from_csv(dataset_id, table_id, filename, schema, project, overwrite)\r\n\r\n\r\ndef write_et_values_specific_station(start_date: str, end_date: str, cimis_station: str) -> None:\r\n    \"\"\"\r\n    Get et for a specific station for a date range and write those values to the DB\r\n\r\n    :param start_date:\r\n    :param end_date:\r\n    :param cimis_station:\r\n    \"\"\"\r\n    stations = cimis_station\r\n    c = CIMIS()\r\n    dicts = c.getDictForStation(stations, start_date, end_date)\r\n    dbwriter = DBWriter()\r\n    for etDataDict in dicts:\r\n        # write_alletsdicts_to_db(etDataDict)\r\n        if etDataDict is not None:\r\n            dataset_id = 'ET'\r\n            table_id = etDataDict['station']\r\n            schema = [\r\n                bigquery.SchemaField(\"date\", \"DATE\"),\r\n                bigquery.SchemaField(\"eto\", \"FLOAT\"),\r\n            ]\r\n            filename = 'all et.csv'\r\n            print('- writing data to csv')\r\n            keys = [\"dates\", \"eto\"]\r\n            justEtData = {key: etDataDict[key] for key in keys}\r\n\r\n            with open(filename, \"w\", newline='') as outfile:\r\n                writer = csv.writer(outfile)\r\n                writer.writerow(justEtData.keys())\r\n                writer.writerows(zip_longest(*justEtData.values()))\r\n            print('...Done - file: ' + filename)\r\n            #\r\n            project = 'stomato-info'\r\n            dbwriter.write_to_table_from_csv(dataset_id, table_id, filename, schema, project)\r\n\r\n\r\ndef calculate_mrid_subtraction_for_date(startDate):\r\n    \"\"\"\r\n    Calculate the subtraction necessary make to MRID to get values from a specific start date\r\n\r\n    :param startDate:\r\n    :return:\r\n    \"\"\"\r\n    # Turn date string into datetimes\r\n    startDateDT = datetime.strptime(startDate, \"%Y-%m-%d\")\r\n    endDateDT = datetime.today()\r\n    delta = endDateDT - startDateDT + timedelta(days=1)\r\n    days = delta.days - 1\r\n    # print(days)\r\n    subMridResult = (days * 24)\r\n    # print(subMridResult)\r\n\r\n    return subMridResult\r\n\r\n\r\ndef get_previous_data_field(grower_name, field_name, startDate,\r\n                            write_to_db=False, specific_mrid=None):\r\n    \"\"\"\r\n    Grab data for a field from a specific start date\r\n\r\n    :param grower_name:\r\n    :param field_name:\r\n    :param startDate:\r\n    :param write_to_db:\r\n    :param specific_mrid:\r\n    \"\"\"\r\n    growers = open_pickle()\r\n    for grower in growers:\r\n        if grower.name == grower_name:\r\n            for field in grower.fields:\r\n                if field.name == field_name:\r\n                    for logger in field.loggers:\r\n                        print(\"Updating Logger: \" + logger.id)\r\n                        print(\"Converting date to MRID\")\r\n                        print(\"Start Date: \" + startDate)\r\n                        mridResult = calculate_mrid_subtraction_for_date(startDate)\r\n                        print(\"Subtract from MRID: \" + str(mridResult))\r\n                        print(\"This is previous day switch: \" + str(logger.prev_day_switch))\r\n                        print(\"Assigning new prev day switch\")\r\n                        logger.prev_day_switch = 0\r\n                        print(\"New prev day switch: \" + str(logger.prev_day_switch))\r\n                        write_pickle(growers)\r\n                        only_certain_growers_field_logger_update(\r\n                            grower_name, field_name, logger_id=logger.id,\r\n                            write_to_db=write_to_db,\r\n                            specific_mrid=specific_mrid,\r\n                            subtract_from_mrid=mridResult\r\n                        )\r\n\r\n\r\ndef failed_cimis_update_et_from_prev_day_eto():\r\n    \"\"\"\r\n    Update et tables for all fields\r\n\r\n    \"\"\"\r\n    growers = open_pickle()\r\n    for grower in growers:\r\n        for field in grower.fields:\r\n            for logger in field.loggers:\r\n                print(f'\\tUpdating et values in Logger {logger.name} table...')\r\n                try:\r\n                    logger.merge_et_db_with_logger_db_values()\r\n                except Exception as err:\r\n                    print(f\"ET Did not update for this logger {logger.name}\")\r\n                    print(err)\r\n\r\n\r\ndef reset_updated_all():\r\n    \"\"\"\r\n    Reset the updated boolean for all growers, fields and loggers\r\n\r\n    \"\"\"\r\n    print('Resetting updated on all growers')\r\n    growers = open_pickle()\r\n    for g in growers:\r\n        g.updated = False\r\n        for f in g.fields:\r\n            f.updated = False\r\n            for logger in f.loggers:\r\n                logger.updated = False\r\n    write_pickle(growers)\r\n    cimisStationsPickle = CimisStation.open_cimis_station_pickle(CimisStation)\r\n    for stations in cimisStationsPickle:\r\n        stations.updated = False\r\n    write_pickle(cimisStationsPickle, filename=\"cimisStation.pickle\")\r\n\r\n\r\ndef set_planting_date_for_field(grower_name, field_name, year, month, day):\r\n    \"\"\"\r\n    Set a planting date for a field\r\n\r\n    :param grower_name:\r\n    :param field_name:\r\n    :param year:\r\n    :param month:\r\n    :param day:\r\n    \"\"\"\r\n    growers = open_pickle()\r\n    for grower in growers:\r\n        if grower.name == grower_name:\r\n            for field in grower.fields:\r\n                if field.name == field_name:\r\n                    for logger in field.loggers:\r\n                        print(\"Old Planting Date: \" + str(logger.planting_date))\r\n                        # print(type(logger.planting_date))\r\n                        newPlantingDate = datetime(year, month, day).date()\r\n                        logger.planting_date = newPlantingDate\r\n                        print(\"New Planting Date: \" + str(logger.planting_date))\r\n    write_pickle(growers)\r\n\r\n\r\ndef update_prev_switch(logger_id, switch):\r\n    \"\"\"\r\n\r\n\r\n    :param logger_id:\r\n    :param switch:\r\n    \"\"\"\r\n    growers = open_pickle()\r\n    for grower in growers:\r\n        for field in grower.fields:\r\n            for logger in field.loggers:\r\n                if logger.id in logger_id:\r\n                    print(\"Logger ID: \" + str(logger.id))\r\n                    print(\"Prev Switch: \" + str(logger.prev_day_switch))\r\n                    logger.prev_day_switch = switch\r\n                    print(\"New Switch: \" + str(logger.prev_day_switch))\r\n    write_pickle(growers)\r\n\r\n\r\ndef check_successful_updated_growers():\r\n    \"\"\"\r\n    Check and print out the number of growers that updated successfully based on their updated boolean\r\n\r\n    :return:\r\n    \"\"\"\r\n    growers = open_pickle()\r\n    active_growers = get_number_of_active_growers()\r\n    successfulGrowers = 0\r\n    for g in growers:\r\n        if g.updated and g.active:\r\n            successfulGrowers = successfulGrowers + 1\r\n    if successfulGrowers == len(active_growers):\r\n        print(\"\\tAll growers successful! \")\r\n        print(\"\\t{0}/{1}\".format(successfulGrowers, active_growers))\r\n        return True\r\n    else:\r\n        print(\"\\t{0}/{1} active growers updated successfully\".format(successfulGrowers, active_growers))\r\n        return False\r\n\r\n\r\ndef get_number_of_active_growers() -> tuple[float, float]:\r\n    \"\"\"\r\n    Function to calculate the number of active growers\r\n\r\n    :return active_growers: Number of active growers\r\n            inactive_growers: Number of inactive growers\r\n    \"\"\"\r\n    active_growers = 0\r\n    inactive_growers = 0\r\n    growers = open_pickle()\r\n    for grower in growers:\r\n        if grower.active:\r\n            active_growers += 1\r\n        else:\r\n            inactive_growers += 1\r\n    return active_growers, inactive_growers\r\n\r\n\r\ndef get_number_of_active_fields_for_grower(grower: Grower) -> tuple[float, float]:\r\n    \"\"\"\r\n    Function to calculate the number of active fields for a grower\r\n\r\n    :param grower: Grower object\r\n    :return active_fields: Number of active fields\r\n            inactive_fields: Number of inactive fields\r\n    \"\"\"\r\n    active_fields, inactive_fields = grower.get_number_of_active_fields()\r\n    return active_fields, inactive_fields\r\n\r\n\r\ndef get_number_of_active_loggers_for_field(field: Field) -> tuple[float, float]:\r\n    \"\"\"\r\n    Function to calculate the number of active and inactive logger for a field\r\n\r\n    :param field: Field object\r\n    :return active_loggers: Number of active logger\r\n            inactive_loggers: Number of inactive loggers\r\n    \"\"\"\r\n    active_loggers, inactive_loggers = field.get_number_of_active_loggers()\r\n    return active_loggers, inactive_loggers\r\n\r\n\r\ndef updated_run_report() -> bool:\r\n    \"\"\"\r\n    Function to show a report on how many growers/fields/loggers got updated successfully\r\n\r\n    :return:\r\n    \"\"\"\r\n    print('\\t--Updated Run Report--')\r\n    growers = open_pickle()\r\n    successfulGrowers = 0\r\n    successfulFields = 0\r\n    successfulLoggers = 0\r\n    number_of_active_growers, number_of_inactive_growers = get_number_of_active_growers()\r\n    for g in growers:\r\n        if g.active:\r\n            if g.updated:\r\n                successfulGrowers = successfulGrowers + 1\r\n            else:\r\n                print(\"\\tGrower: -{0}- was unsuccessful in updating\".format(g.name))\r\n                for f in g.fields:\r\n                    if f.active:\r\n                        if f.updated:\r\n                            successfulFields = successfulFields + 1\r\n                        else:\r\n                            print(\"\\t due to Field: {0}\".format(f.name))\r\n                            for logger in f.loggers:\r\n                                if logger.active:\r\n                                    if logger.updated:\r\n                                        successfulLoggers = successfulLoggers + 1\r\n                                    else:\r\n                                        print(\"\\t  > Logger: {0}\".format(logger.name))\r\n    if successfulGrowers == number_of_active_growers:\r\n        print('\\t Clean run! All active growers updated successfully!')\r\n        return True\r\n    else:\r\n        return False\r\n\r\n\r\ndef set_cimis_station(field_name, cimisStation):\r\n    \"\"\"\r\n    Set the cimis station for a field\r\n\r\n    :param field_name:\r\n    :param cimisStation:\r\n    \"\"\"\r\n    growers = open_pickle()\r\n    for grower in growers:\r\n        for field in grower.fields:\r\n            if field.name == field_name:\r\n                field.cimis_station = str(cimisStation)\r\n                print(\"New Cimis Station: \" + field.cimis_station)\r\n    write_pickle(growers)\r\n\r\n\r\ndef apply_ai_recommendation_to_logger(project, dataset, logger_name):\r\n    \"\"\"\r\n    Apply the AI Irrigation recommendation to a specific logger's data\r\n\r\n    :param project:\r\n    :param dataset:\r\n    :param logger_name:\r\n    \"\"\"\r\n    print(f'Grabbing data from {project} - {dataset} for logger {logger_name} - ')\r\n    dml = 'SELECT *' \\\r\n          'FROM `' + project + '.' + dataset + '.' + logger_name + '` ORDER BY date DESC'\r\n    # 'WHERE et_hours is not NULL ORDER BY date DESC'\r\n\r\n    dbwriter = DBWriter()\r\n    expertSys = IrrigationRecommendationExpert()\r\n    result = dbwriter.run_dml(dml, project=project)\r\n    applied_finals = {}\r\n    ai_results = {\"logger_id\": [], \"date\": [], \"time\": [], \"canopy_temperature\": [], \"ambient_temperature\": [],\r\n                  \"vpd\": [], \"vwc_1\": [], \"vwc_2\": [], \"vwc_3\": [], \"field_capacity\": [], \"wilting_point\": [],\r\n                  \"daily_gallons\": [], \"daily_switch\": [], \"daily_hours\": [], \"daily_pressure\": [],\r\n                  \"daily_inches\": [], \"psi\": [], \"psi_threshold\": [], \"psi_critical\": [],\r\n                  \"sdd\": [], \"rh\": [], 'eto': [], 'kc': [], 'etc': [], 'et_hours': [],\r\n                  \"phase1_adjustment\": [], \"phase1_adjusted\": [], \"phase2_adjustment\": [], \"phase2_adjusted\": [],\r\n                  \"phase3_adjustment\": [], \"phase3_adjusted\": [], \"vwc_1_ec\": [], \"vwc_2_ec\": [], \"vwc_3_ec\": [],\r\n                  \"lowest_ambient_temperature\": [], \"gdd\": [], \"crop_stage\": [], \"id\": [], \"planting_date\": [],\r\n                  \"variety\": []}\r\n\r\n    applied_finals['date'] = []\r\n    applied_finals['base'] = []\r\n    applied_finals['final_rec'] = []\r\n    applied_finals['adjustment_values'] = []\r\n    applied_finals['adjustment_steps'] = []\r\n\r\n    planting_date = None\r\n    crop_type = ''\r\n    growers = open_pickle()\r\n    for grower in growers:\r\n        for field in grower.fields:\r\n            field_name = dbwriter.remove_unwanted_chars_for_db_dataset(field.name)\r\n            if dataset in field_name:\r\n                for logger in field.loggers:\r\n                    planting_date = logger.planting_date\r\n                    crop_type = logger.crop_type\r\n\r\n    # harvest_date = result[-1][1]\r\n    harvest_date = planting_date + timedelta(days=126)\r\n    prev_et_hours = 0\r\n\r\n    for r in result:\r\n        logger_id = r[0]\r\n        date = r[1]\r\n        time = r[2]\r\n        canopy_temperature = r[3]\r\n        ambient_temperature = r[4]\r\n        vpd = r[5]\r\n        vwc_1 = r[6]\r\n        vwc_2 = r[7]\r\n        vwc_3 = r[8]\r\n        field_capacity = r[9]\r\n        wilting_point = r[10]\r\n        daily_gallons = r[11]\r\n        daily_switch = r[12]\r\n        daily_hours = r[13]\r\n        daily_pressure = r[14]\r\n        daily_inches = r[15]\r\n        psi = r[16]\r\n        psi_threshold = r[17]\r\n        psi_critical = r[18]\r\n        sdd = r[19]\r\n        rh = r[20]\r\n        eto = r[21]\r\n        kc = r[22]\r\n        etc = r[23]\r\n        et_hours = r[24]\r\n        lta = r[34]\r\n        gdd = r[35]\r\n        crop_stage = r[36]\r\n        id = r[37]\r\n        t_planting_date = r[38]\r\n        variety = r[39]\r\n\r\n        rec = expertSys.make_recommendation(\r\n            psi, field_capacity, wilting_point, vwc_1, vwc_2, vwc_3,\r\n            crop='Tomatoes', date=date, planting_date=planting_date,\r\n            harvest_date=harvest_date\r\n        )\r\n        if et_hours is None:\r\n            et_hours = prev_et_hours\r\n        applied_final, applied_steps = expertSys.apply_recommendations(et_hours, rec)\r\n\r\n        ai_results['logger_id'].append(logger_id)\r\n        ai_results['date'].append(date)\r\n        ai_results['time'].append(time)\r\n        ai_results['canopy_temperature'].append(canopy_temperature)\r\n        ai_results['ambient_temperature'].append(ambient_temperature)\r\n        ai_results['vpd'].append(vpd)\r\n        ai_results['vwc_1'].append(vwc_1)\r\n        ai_results['vwc_2'].append(vwc_2)\r\n        ai_results['vwc_3'].append(vwc_3)\r\n        ai_results['field_capacity'].append(field_capacity)\r\n        ai_results['wilting_point'].append(wilting_point)\r\n        ai_results['daily_gallons'].append(daily_gallons)\r\n        ai_results['daily_switch'].append(daily_switch)\r\n        ai_results['daily_hours'].append(daily_hours)\r\n        ai_results['daily_pressure'].append(daily_pressure)\r\n        ai_results['daily_inches'].append(daily_inches)\r\n        ai_results['psi'].append(psi)\r\n        ai_results['psi_threshold'].append(psi_threshold)\r\n        ai_results['psi_critical'].append(psi_critical)\r\n        ai_results['sdd'].append(sdd)\r\n        ai_results['rh'].append(rh)\r\n        ai_results['eto'].append(eto)\r\n        ai_results['kc'].append(kc)\r\n        ai_results['etc'].append(etc)\r\n        ai_results['et_hours'].append(et_hours)\r\n        ai_results['phase1_adjustment'].append(rec.recommendation_info[0])\r\n        ai_results['phase1_adjusted'].append(applied_steps[0])\r\n        ai_results['phase2_adjustment'].append(rec.recommendation_info[1])\r\n        ai_results['phase2_adjusted'].append(applied_steps[1])\r\n        ai_results['lowest_ambient_temperature'].append(lta)\r\n        ai_results['gdd'].append(gdd)\r\n        ai_results['crop_stage'].append(crop_stage)\r\n        ai_results['id'].append(id)\r\n        ai_results['planting_date'].append(t_planting_date)\r\n        ai_results['variety'].append(variety)\r\n\r\n        if et_hours is not None:\r\n            prev_et_hours = et_hours\r\n\r\n    print(f'Adding AI info to DB {dataset} - {logger_name} ')\r\n\r\n    filename = 'ai_data.csv'\r\n    print('\\t- writing data to csv')\r\n    with open(filename, \"w\", newline='') as outfile:\r\n        writer = csv.writer(outfile)\r\n        writer.writerow(ai_results.keys())\r\n        # Using zip_longest because dict rows are uneven length due to daily_switch algo issue\r\n        # This will add full null rows for any additional daily_switch list values\r\n        writer.writerows(zip_longest(*ai_results.values()))\r\n    # print('...Done - file: ' + filename)\r\n\r\n    schema = [\r\n        bigquery.SchemaField(\"logger_id\", \"STRING\"),\r\n        bigquery.SchemaField(\"date\", \"DATE\"),\r\n        bigquery.SchemaField(\"time\", \"STRING\"),\r\n        bigquery.SchemaField(\"canopy_temperature\", \"FLOAT\"),\r\n        bigquery.SchemaField(\"ambient_temperature\", \"FLOAT\"),\r\n        bigquery.SchemaField(\"vpd\", \"FLOAT\"),\r\n        bigquery.SchemaField(\"vwc_1\", \"FLOAT\"),\r\n        bigquery.SchemaField(\"vwc_2\", \"FLOAT\"),\r\n        bigquery.SchemaField(\"vwc_3\", \"FLOAT\"),\r\n        bigquery.SchemaField(\"field_capacity\", \"FLOAT\"),\r\n        bigquery.SchemaField(\"wilting_point\", \"FLOAT\"),\r\n        bigquery.SchemaField(\"daily_gallons\", \"FLOAT\"),\r\n        bigquery.SchemaField(\"daily_switch\", \"FLOAT\"),\r\n        bigquery.SchemaField(\"daily_hours\", \"FLOAT\"),\r\n        bigquery.SchemaField(\"daily_pressure\", \"FLOAT\"),\r\n        bigquery.SchemaField(\"daily_inches\", \"FLOAT\"),\r\n        bigquery.SchemaField(\"psi\", \"FLOAT\"),\r\n        bigquery.SchemaField(\"psi_threshold\", \"FLOAT\"),\r\n        bigquery.SchemaField(\"psi_critical\", \"FLOAT\"),\r\n        bigquery.SchemaField(\"sdd\", \"FLOAT\"),\r\n        bigquery.SchemaField(\"rh\", \"FLOAT\"),\r\n        bigquery.SchemaField(\"eto\", \"FLOAT\"),\r\n        bigquery.SchemaField(\"kc\", \"FLOAT\"),\r\n        bigquery.SchemaField(\"etc\", \"FLOAT\"),\r\n        bigquery.SchemaField(\"et_hours\", \"FLOAT\"),\r\n        bigquery.SchemaField(\"phase1_adjustment\", \"FLOAT\"),\r\n        bigquery.SchemaField(\"phase1_adjusted\", \"FLOAT\"),\r\n        bigquery.SchemaField(\"phase2_adjustment\", \"FLOAT\"),\r\n        bigquery.SchemaField(\"phase2_adjusted\", \"FLOAT\"),\r\n        bigquery.SchemaField(\"phase3_adjustment\", \"FLOAT\"),\r\n        bigquery.SchemaField(\"phase3_adjusted\", \"FLOAT\"),\r\n        bigquery.SchemaField(\"vwc_1_ec\", \"FLOAT\"),\r\n        bigquery.SchemaField(\"vwc_2_ec\", \"FLOAT\"),\r\n        bigquery.SchemaField(\"vwc_3_ec\", \"FLOAT\"),\r\n        bigquery.SchemaField(\"lowest_ambient_temperature\", \"FLOAT\"),\r\n        bigquery.SchemaField(\"gdd\", \"FLOAT\"),\r\n        bigquery.SchemaField(\"crop_stage\", \"STRING\"),\r\n        bigquery.SchemaField(\"id\", \"STRING\"),\r\n        bigquery.SchemaField(\"planting_date\", \"DATE\"),\r\n        bigquery.SchemaField(\"variety\", \"STRING\"),\r\n    ]\r\n    # project = 'stomato-' + DIRECTORY_YEAR\r\n    project = dbwriter.get_db_project(crop_type)\r\n    dbwriter.write_to_table_from_csv(dataset, logger_name, filename, schema, project, overwrite=True)\r\n    print()\r\n\r\n    print('Fully Done')\r\n\r\n\r\ndef apply_cwsi_to_whole_table(dataset, logger_name, crop_type):\r\n    \"\"\"\r\n    Apply CWSI to a specific logger's data table in the DB\r\n\r\n    :param dataset:\r\n    :param logger_name:\r\n    :param crop_type:\r\n    \"\"\"\r\n    dbwriter = DBWriter()\r\n    project = dbwriter.get_db_project(crop_type)\r\n    print(f'Grabbing data from {project} - {dataset} for logger {logger_name} - ')\r\n    dml = 'SELECT *' \\\r\n          'FROM `' + project + '.' + dataset + '.' + logger_name + '` ' \\\r\n                                                                   'WHERE et_hours is not NULL ORDER BY date DESC'\r\n\r\n    cwsi_processor = CwsiProcessor()\r\n    result = dbwriter.run_dml(dml, project=project)\r\n    modified_data = {\"logger_id\": [], \"date\": [], \"time\": [], \"canopy_temperature\": [], \"ambient_temperature\": [],\r\n                     \"vpd\": [], \"vwc_1\": [], \"vwc_2\": [], \"vwc_3\": [], \"field_capacity\": [], \"wilting_point\": [],\r\n                     \"daily_gallons\": [], \"daily_switch\": [], \"daily_hours\": [], \"daily_pressure\": [],\r\n                     \"daily_inches\": [], \"psi\": [], \"psi_threshold\": [], \"psi_critical\": [],\r\n                     \"sdd\": [], \"rh\": [], 'eto': [], 'kc': [], 'etc': [], 'et_hours': [],\r\n                     \"phase1_adjustment\": [], \"phase1_adjusted\": [], \"phase2_adjustment\": [], \"phase2_adjusted\": [],\r\n                     \"phase3_adjustment\": [], \"phase3_adjusted\": [], \"vwc_1_ec\": [], \"vwc_2_ec\": [], \"vwc_3_ec\": []}\r\n\r\n    for r in result:\r\n        logger_name = r[0]\r\n        date = r[1]\r\n        time = r[2]\r\n        canopy_temperature = r[3]\r\n        ambient_temperature = r[4]\r\n        vpd = r[5]\r\n        vwc_1 = r[6]\r\n        vwc_2 = r[7]\r\n        vwc_3 = r[8]\r\n        field_capacity = r[9]\r\n        wilting_point = r[10]\r\n        daily_gallons = r[11]\r\n        daily_switch = r[12]\r\n        daily_hours = r[13]\r\n        daily_pressure = r[14]\r\n        daily_inches = r[15]\r\n        sdd = r[19]\r\n        rh = r[20]\r\n        eto = r[21]\r\n        kc = r[22]\r\n        etc = r[23]\r\n        et_hours = r[24]\r\n\r\n        # def get_cwsi(self, tc, vpd, ta, cropType, rh=0, return_negative=False):\r\n        psi = cwsi_processor.get_cwsi(canopy_temperature, vpd, ambient_temperature, crop_type)\r\n        psi_threshold = 0.5\r\n        psi_critical = 1\r\n\r\n        modified_data['logger_id'].append(logger_name)\r\n        modified_data['date'].append(date)\r\n        modified_data['time'].append(time)\r\n        modified_data['canopy_temperature'].append(canopy_temperature)\r\n        modified_data['ambient_temperature'].append(ambient_temperature)\r\n        modified_data['vpd'].append(vpd)\r\n        modified_data['vwc_1'].append(vwc_1)\r\n        modified_data['vwc_2'].append(vwc_2)\r\n        modified_data['vwc_3'].append(vwc_3)\r\n        modified_data['field_capacity'].append(field_capacity)\r\n        modified_data['wilting_point'].append(wilting_point)\r\n        modified_data['daily_gallons'].append(daily_gallons)\r\n        modified_data['daily_switch'].append(daily_switch)\r\n        modified_data['daily_hours'].append(daily_hours)\r\n        modified_data['daily_pressure'].append(daily_pressure)\r\n        modified_data['daily_inches'].append(daily_inches)\r\n        modified_data['psi'].append(psi)\r\n        modified_data['psi_threshold'].append(psi_threshold)\r\n        modified_data['psi_critical'].append(psi_critical)\r\n        modified_data['sdd'].append(sdd)\r\n        modified_data['rh'].append(rh)\r\n        modified_data['eto'].append(eto)\r\n        modified_data['kc'].append(kc)\r\n        modified_data['etc'].append(etc)\r\n        modified_data['et_hours'].append(et_hours)\r\n        modified_data['phase1_adjustment'].append(None)\r\n        modified_data['phase1_adjusted'].append(None)\r\n        modified_data['phase2_adjustment'].append(None)\r\n        modified_data['phase2_adjusted'].append(None)\r\n        modified_data['phase3_adjustment'].append(None)\r\n        modified_data['phase3_adjusted'].append(None)\r\n        modified_data['vwc_1_ec'].append(None)\r\n        modified_data['vwc_2_ec'].append(None)\r\n        modified_data['vwc_3_ec'].append(None)\r\n\r\n    print(f'Adding PSI modified info to DB {dataset} - {logger_name}')\r\n\r\n    filename = 'psi_modified_data.csv'\r\n    print('\\t- writing data to csv')\r\n    with open(filename, \"w\", newline='') as outfile:\r\n        writer = csv.writer(outfile)\r\n        writer.writerow(modified_data.keys())\r\n        # Using zip_longest because dict rows are uneven length due to daily_switch algo issue\r\n        # This will add full null rows for any additional daily_switch list values\r\n        writer.writerows(zip_longest(*modified_data.values()))\r\n    # print('...Done - file: ' + filename)\r\n\r\n    schema = [\r\n        bigquery.SchemaField(\"logger_id\", \"STRING\"),\r\n        bigquery.SchemaField(\"date\", \"DATE\"),\r\n        bigquery.SchemaField(\"time\", \"STRING\"),\r\n        bigquery.SchemaField(\"canopy_temperature\", \"FLOAT\"),\r\n        bigquery.SchemaField(\"ambient_temperature\", \"FLOAT\"),\r\n        bigquery.SchemaField(\"vpd\", \"FLOAT\"),\r\n        bigquery.SchemaField(\"vwc_1\", \"FLOAT\"),\r\n        bigquery.SchemaField(\"vwc_2\", \"FLOAT\"),\r\n        bigquery.SchemaField(\"vwc_3\", \"FLOAT\"),\r\n        bigquery.SchemaField(\"field_capacity\", \"FLOAT\"),\r\n        bigquery.SchemaField(\"wilting_point\", \"FLOAT\"),\r\n        bigquery.SchemaField(\"daily_gallons\", \"FLOAT\"),\r\n        bigquery.SchemaField(\"daily_switch\", \"FLOAT\"),\r\n        bigquery.SchemaField(\"daily_hours\", \"FLOAT\"),\r\n        bigquery.SchemaField(\"daily_pressure\", \"FLOAT\"),\r\n        bigquery.SchemaField(\"daily_inches\", \"FLOAT\"),\r\n        bigquery.SchemaField(\"psi\", \"FLOAT\"),\r\n        bigquery.SchemaField(\"psi_threshold\", \"FLOAT\"),\r\n        bigquery.SchemaField(\"psi_critical\", \"FLOAT\"),\r\n        bigquery.SchemaField(\"sdd\", \"FLOAT\"),\r\n        bigquery.SchemaField(\"rh\", \"FLOAT\"),\r\n        bigquery.SchemaField(\"eto\", \"FLOAT\"),\r\n        bigquery.SchemaField(\"kc\", \"FLOAT\"),\r\n        bigquery.SchemaField(\"etc\", \"FLOAT\"),\r\n        bigquery.SchemaField(\"et_hours\", \"FLOAT\"),\r\n        bigquery.SchemaField('phase1_adjustment', 'FLOAT'),\r\n        bigquery.SchemaField('phase1_adjusted', 'FLOAT'),\r\n        bigquery.SchemaField('phase2_adjustment', 'FLOAT'),\r\n        bigquery.SchemaField('phase2_adjusted', 'FLOAT'),\r\n        bigquery.SchemaField('phase3_adjustment', 'FLOAT'),\r\n        bigquery.SchemaField('phase3_adjusted', 'FLOAT'),\r\n        bigquery.SchemaField('vwc_1_ec', 'FLOAT'),\r\n        bigquery.SchemaField('vwc_2_ec', 'FLOAT'),\r\n        bigquery.SchemaField('vwc_3_ec', 'FLOAT'),\r\n    ]\r\n    # project = 'stomato-' + DATABASE_YEAR\r\n    project = dbwriter.get_db_project(crop_type)\r\n    dbwriter.write_to_table_from_csv(dataset, logger_name, filename, schema, project, overwrite=True)\r\n    print()\r\n\r\n    print('Fully Done')\r\n\r\n\r\ndef get_crop_stage(data_point_date, harvest_date, planting_date):\r\n    \"\"\"\r\n    Method to get the crop stage for the artificial intelligence irrigation system\r\n     for a tomato crop based on current date, harvest date,\r\n     and planting date. Grabs total crop days and divides that by 4 to get a chunk.\r\n     Chunk 1 is stage 1, chunk 2 is stage 2, and chunk 3 + 4 is stage 3.\r\n\r\n    :param data_point_date:\r\n    :param harvest_date:\r\n    :param planting_date:\r\n    :return:\r\n    \"\"\"\r\n    crop_stage = None\r\n    if harvest_date is not None and planting_date is not None:\r\n        delta = harvest_date - planting_date\r\n        total_crop_days = delta.days\r\n\r\n        one_fourth_chunk = total_crop_days // 4\r\n\r\n        stage_one_start = planting_date\r\n        stage_one_end = planting_date + timedelta(days=one_fourth_chunk)\r\n        stage_two_start = stage_one_end\r\n        stage_two_end = stage_one_end + timedelta(days=one_fourth_chunk * 2)\r\n        stage_three_start = stage_two_end\r\n        stage_three_end = harvest_date\r\n\r\n        print('Stage 1 Start: {}'.format(stage_one_start))\r\n        print('Stage 1 End: {}'.format(stage_one_end))\r\n        print('Stage 2 Start: {}'.format(stage_two_start))\r\n        print('Stage 2 End: {}'.format(stage_two_end))\r\n        print('Stage 3 Start: {}'.format(stage_three_start))\r\n        print('Stage 3 End: {}'.format(stage_three_end))\r\n\r\n        # if stage_one_start <= date <= stage_one_end:\r\n        if data_point_date <= stage_one_end:\r\n            crop_stage = 'Stage 1'\r\n        elif stage_two_start <= data_point_date < stage_two_end:\r\n            crop_stage = 'Stage 2'\r\n        elif stage_three_start <= data_point_date:\r\n            crop_stage = 'Stage 3'\r\n\r\n        print('Crop Stage: {}'.format(crop_stage))\r\n    return crop_stage\r\n\r\n\r\ndef reassign_technician(old_technician_name: str, new_technician_name: str):\r\n    \"\"\"\r\n    Change all growers with a technician to a different technician. Takes in the old technician name and the new \r\n    technician name and pulls up all growers that have the old technician. Then it looks for a technician with the \r\n    new technician name and if it finds one it uses that technician as the new technician for the growers \r\n\r\n    :param old_technician_name:\r\n    :param new_technician_name:\r\n    \"\"\"\r\n    growers = open_pickle()\r\n    all_technicians = get_all_technicians(growers)\r\n    new_technician = None\r\n    for tech in all_technicians:\r\n        if tech.name == new_technician_name:\r\n            new_technician = tech\r\n    for g in growers:\r\n        if g.technician.name == old_technician_name:\r\n            print(\r\n                'Changing Technician for Grower {} \\n from {} to {}'.format(\r\n                    g.name, old_technician_name,\r\n                    new_technician_name\r\n                )\r\n            )\r\n            g.technician = new_technician\r\n    write_pickle(growers)\r\n\r\n\r\ndef deactivate_growers_with_all_inactive_fields():\r\n    \"\"\"\r\n    Method to check if all of a growers fields are inactive and if they are set the grower itself as inactive\r\n\r\n    :return: None\r\n    \"\"\"\r\n    growers = open_pickle()\r\n    all_fields_inactive = True\r\n    for g in growers:\r\n        for f in g.fields:\r\n            if f.active:\r\n                all_fields_inactive = False\r\n        if all_fields_inactive:\r\n            g.deactivate()\r\n        all_fields_inactive = True\r\n    # for g in growers:\r\n    #     if not g.active:\r\n    #         g.to_string()\r\n    write_pickle(growers)\r\n\r\n\r\ndef remove_inactive_growers_from_pickle():\r\n    \"\"\"\r\n    Method that goes through the current pickle, picks out active growers, and overwrites the current pickle\r\n    only with these active growers. There is a confirmation dialog to make sure we want to run this and a\r\n    pickle backup is also created before overwriting it.\r\n    \"\"\"\r\n    new_growers = []\r\n    growers = open_pickle()\r\n    print('Removing inactive growers from pickle will overwrite current pickle only leaving active growers')\r\n    confirm = input('Are you sure you want to do this? (Y/N) ').lower().strip()\r\n\r\n    if confirm[:1] == 'y':\r\n        backup_pickle('before_removing_inactive')\r\n        for grower in growers:\r\n            if grower.active:\r\n                print('Transferring > ', grower.name)\r\n                new_growers.append(grower)\r\n            else:\r\n                print('Not Transferring >', grower.name)\r\n        write_pickle(new_growers)\r\n\r\n\r\ndef remove_inactive_fields_from_growers_from_pickle():\r\n    \"\"\"\r\n    Method that goes through the current pickle, picks out active growers, and overwrites the current pickle\r\n    only with these active growers. There is a confirmation dialog to make sure we want to run this and a\r\n    pickle backup is also created before overwriting it.\r\n    \"\"\"\r\n    growers = open_pickle()\r\n    print('Removing inactive fields from pickle will overwrite current pickle only leaving active fields')\r\n    confirm = input('Are you sure you want to do this? (Y/N) ').lower().strip()\r\n\r\n    if confirm[:1] == 'y':\r\n        backup_pickle('before_removing_inactive_fields')\r\n        for grower in growers:\r\n            if grower.active:\r\n                active_fields = []\r\n                for field in grower.fields:\r\n                    if field.active:\r\n                        active_fields.append(field)\r\n                    else:\r\n                        print(f'Removing {field.nickname} from {grower.name}')\r\n                grower.fields = active_fields\r\n        write_pickle(growers)\r\n\r\n\r\ndef new_year_pickle_cleanup():\r\n    \"\"\"\r\n    Convenience method to chain a couple of methods together. This should be run when we are ready to setup\r\n    a pickle for the new year. This will first deactivate growers that have all their fields inactive and\r\n    will then remove these growers from the current pickle\r\n    \"\"\"\r\n    deactivate_growers_with_all_inactive_fields()\r\n    remove_inactive_growers_from_pickle()\r\n\r\n\r\ndef get_technician(technician_name: str, growers = None):\r\n    \"\"\"\r\n    Grab the technician object for a technician with technician_name\r\n\r\n    :param technician_name:\r\n    :return:\r\n    \"\"\"\r\n    if growers is None:\r\n        growers = open_pickle()\r\n    technicians = get_all_technicians(growers)\r\n    for technician in technicians:\r\n        if technician.name == technician_name:\r\n            print('Got Technician = ', technician_name, technician)\r\n            return technician\r\n    print(\"Technician - \", technician_name, ' - not found')\r\n    return None\r\n\r\n\r\ndef check_technician_clones():\r\n    growers = open_pickle()\r\n    tech_dict = {'Vanessa': [], 'Exsaelth': [], 'Adriana': [], 'Development Test Tech': []}\r\n    for grower in growers:\r\n        # print(grower.name, ' - ', grower.technician.name, ' - ', id(grower.technician))\r\n        if id(grower.technician) not in tech_dict[grower.technician.name]:\r\n            tech_dict[grower.technician.name].append(id(grower.technician))\r\n    for key, values in tech_dict.items():\r\n        print(key, \" : \", values)\r\n\r\n\r\ndef temp_ai_application():\r\n    # AI Logger\r\n    apply_ai_recommendation_to_logger('stomato-2023', 'Lucero_Dillard_RoadD3', 'DI-D3-SE')\r\n\r\n    # Old Control\r\n    # apply_ai_recommendation_to_logger('stomato-2023', 'Lucero_Dillard_RoadD3', 'DI-D3-NW')\r\n\r\n    # New Control\r\n    apply_ai_recommendation_to_logger('stomato-2023', 'Lucero_Dillard_RoadD4', 'DI-D4-W')\r\n\r\n\r\ndef check_for_new_cimis_stations():\r\n    cimisStationsPickle = CimisStation.open_cimis_station_pickle(CimisStation)\r\n    cimisStationList = []\r\n    for stations in cimisStationsPickle:\r\n        cimisStationList.append(stations.station_number)\r\n    growers = open_pickle()\r\n    for g in growers:\r\n        for f in g.fields:\r\n            if f.cimis_station not in cimisStationList:\r\n                cimisStationList.append(f.cimis_station)\r\n                print(\"Adding Cimis Station: \", f.cimis_station, \" to pickle\")\r\n                x = input(\"ET for \" + f.cimis_station + \"\\n\")\r\n                cimisStation = CimisStation(f.cimis_station, float(x))\r\n                cimisStationsPickle.append(cimisStation)\r\n    CimisStation.write_cimis_station_pickle(CimisStation, cimisStationsPickle)\r\n\r\n\r\ndef write_uninstallation_progress_to_db():\r\n    dbwriter = DBWriter()\r\n    schema = [\r\n        bigquery.SchemaField(\"Grower\", \"STRING\"),\r\n        bigquery.SchemaField(\"Field\", \"STRING\"),\r\n        bigquery.SchemaField(\"Uninstalled_Date\", \"DATE\"),\r\n        bigquery.SchemaField(\"Acres\", \"FLOAT\"),\r\n        bigquery.SchemaField(\"Region\", \"STRING\"),\r\n        bigquery.SchemaField(\"Latt_Long\", \"STRING\")\r\n    ]\r\n    uninstallDictNorth = {\"Grower\": [], \"Field\": [], \"Uninstalled_Date\": [], \"Acres\": [], \"Region\": [], \"Latt_Long\": []}\r\n    uninstallDictSouth = {\"Grower\": [], \"Field\": [], \"Uninstalled_Date\": [], \"Acres\": [], \"Region\": [], \"Latt_Long\": []}\r\n\r\n    growers = open_pickle()\r\n    for g in growers:\r\n        for f in g.fields:\r\n            if g.region == 'North' and f.loggers[-1].crop_type == \"Tomatoes\":\r\n                uninstallDictNorth[\"Grower\"].append(g.name)\r\n                uninstallDictNorth[\"Field\"].append(f.nickname)\r\n                lat_long = str(f.loggers[-1].lat) + \",\" + str(f.loggers[-1].long)\r\n                uninstallDictNorth[\"Latt_Long\"].append(str(lat_long))\r\n            elif g.region == \"South\" and f.loggers[-1].crop_type == \"Tomatoes\":\r\n                uninstallDictSouth[\"Grower\"].append(g.name)\r\n                uninstallDictSouth[\"Field\"].append(f.nickname)\r\n                lat_long = str(f.loggers[-1].lat) + \",\" + str(f.loggers[-1].long)\r\n                uninstallDictSouth[\"Latt_Long\"].append(str(lat_long))\r\n\r\n    print(uninstallDictNorth)\r\n    print(uninstallDictSouth)\r\n\r\n    with open('uninstallation.csv', \"w\", newline='') as outfile:\r\n        writer = csv.writer(outfile)\r\n        writer.writerow(uninstallDictNorth.keys())\r\n        # Using zip_longest because dict rows are uneven length due to daily_switch algo issue\r\n        # This will add full null rows for any additional daily_switch list values\r\n        writer.writerows(zip_longest(*uninstallDictNorth.values()))\r\n    project = 'stomato-info'\r\n    dbwriter.write_to_table_from_csv(\r\n        '1_uninstallation_progress', 'North', 'uninstallation.csv', schema, project\r\n    )\r\n\r\n    with open('uninstallation.csv', \"w\", newline='') as outfile:\r\n        writer = csv.writer(outfile)\r\n        writer.writerow(uninstallDictSouth.keys())\r\n        # Using zip_longest because dict rows are uneven length due to daily_switch algo issue\r\n        # This will add full null rows for any additional daily_switch list values\r\n        writer.writerows(zip_longest(*uninstallDictSouth.values()))\r\n\r\n    dbwriter.write_to_table_from_csv(\r\n        '1_uninstallation_progress', 'South', 'uninstallation.csv', schema, project\r\n    )\r\n\r\n\r\ndef setup_ai_game_data(\r\n        grower_pickle_file_name: str,\r\n        grower_pickle_file_path: str,\r\n        ai_game_pickle_file_name: str,\r\n        ai_game_pickle_file_path: str\r\n):\r\n    ai_game_data = AIGameData()\r\n    dbw = DBWriter()\r\n    tomato_crop = False\r\n    growers = open_pickle(filename=grower_pickle_file_name, specific_file_path=grower_pickle_file_path)\r\n\r\n    for grower in growers:\r\n        if grower.region == 'South':\r\n            for field in grower.fields:\r\n                if field.crop_type in ['Tomatoes', 'tomatoes', 'Tomato', 'tomato'] and field.field_type != 'R&D':\r\n                    for logger in field.loggers:\r\n                        print(f'Adding {logger.field.name} - {logger.name}')\r\n                        ai_game_data.south_pool.append(logger)\r\n                        ai_game_data.all_pool.append(logger)\r\n        elif grower.region == 'North':\r\n            for field in grower.fields:\r\n                if field.crop_type in ['Tomatoes', 'tomatoes', 'Tomato', 'tomato'] and field.field_type != 'R&D':\r\n                    for logger in field.loggers:\r\n                        print(f'Adding {logger.field.name} - {logger.name}')\r\n                        ai_game_data.north_pool.append(logger)\r\n                        ai_game_data.all_pool.append(logger)\r\n    ai_game_data.show_content()\r\n    # print('Total Data Points: {}'.format(ai_game_data.number_of_data_points()))\r\n    # #\r\n    if path.exists(ai_game_pickle_file_path):\r\n        with open(ai_game_pickle_file_path + ai_game_pickle_file_name, 'wb') as f:\r\n            pickle.dump(ai_game_data, f)\r\n\r\n\r\ndef test_switch_cases():\r\n    \"\"\"\r\n    Method to set up and loop through all switch test cases and record results.\r\n\r\n    \"\"\"\r\n\r\n    # Setup test cases\r\n    test_cases = []\r\n\r\n    standard_data = {\r\n        'dates': [datetime(2022, 6, 12, 0, 0), datetime(2022, 6, 12, 1, 0),\r\n                               datetime(2022, 6, 12, 2, 0), datetime(2022, 6, 12, 3, 0),\r\n                               datetime(2022, 6, 12, 4, 0), datetime(2022, 6, 12, 5, 0),\r\n                               datetime(2022, 6, 12, 6, 0), datetime(2022, 6, 12, 7, 0),\r\n                               datetime(2022, 6, 12, 8, 0), datetime(2022, 6, 12, 9, 0),\r\n                               datetime(2022, 6, 12, 10, 0), datetime(2022, 6, 12, 11, 0),\r\n                               datetime(2022, 6, 12, 12, 0), datetime(2022, 6, 12, 13, 0),\r\n                               datetime(2022, 6, 12, 14, 0), datetime(2022, 6, 12, 15, 0),\r\n                               datetime(2022, 6, 12, 16, 0), datetime(2022, 6, 12, 17, 0),\r\n                               datetime(2022, 6, 12, 18, 0), datetime(2022, 6, 12, 19, 0),\r\n                               datetime(2022, 6, 12, 20, 0), datetime(2022, 6, 12, 21, 0),\r\n                               datetime(2022, 6, 12, 22, 0), datetime(2022, 6, 12, 23, 0),\r\n                               datetime(2022, 6, 13, 0, 0), datetime(2022, 6, 13, 1, 0)],\r\n                     'canopy temperature': [76, 77, 88, 93, 78, 76, 88, 83, 76, 89, 79, 93, 84, 80, 81, 94, 85, 78, 80,\r\n                                            94, 89, 87, 76, 82, 93, 89],\r\n                     'ambient temperature': [93, 79, 103, 89, 94, 84, 102, 77, 96, 84, 95, 79, 77, 97, 74, 73, 88, 100,\r\n                                             82, 81, 96, 78, 93, 89, 94, 85],\r\n                     'rh': [31, 56, 46, 15, 15, 41, 39, 21, 16, 43, 34, 16, 55, 29, 33, 51, 28, 54, 23, 32, 58, 42, 24,\r\n                            59, 50, 58],\r\n                     'vpd': [2, 2, 2, 4, 5, 2, 5, 3, 1, 3, 0, 5, 3, 0, 2, 2, 0, 4, 2, 5, 1, 3, 1, 5, 2, 2],\r\n                     'vwc_1': [34, 22, 33, 23, 24, 21, 26, 31, 33, 22, 38, 35, 36, 30, 40, 23, 34, 36, 32, 35, 41, 33,\r\n                               39, 27, 36, 39],\r\n                     'vwc_2': [43, 35, 20, 41, 35, 35, 25, 42, 40, 35, 30, 41, 32, 20, 29, 32, 40, 26, 25, 42, 38, 31,\r\n                               29, 38, 40, 35],\r\n                     'vwc_3': [20, 25, 39, 20, 34, 32, 25, 39, 23, 31, 22, 25, 42, 26, 25, 21, 31, 42, 42, 37, 23, 42,\r\n                               33, 20, 34, 35], 'vwc_1_ec': [], 'vwc_2_ec': [], 'vwc_3_ec': [], 'daily gallons': [],\r\n                     'daily switch': [0, 0, 0, 0, 0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\r\n                                      0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}\r\n\r\n    standard_data_2023 = {\r\n        'dates': [datetime(2023, 3, 15, 0, 0, 15, 512237), datetime(2023, 3, 15, 1, 0, 15, 512237),\r\n                  datetime(2023, 3, 15, 2, 0, 15, 512237), datetime(2023, 3, 15, 3, 0, 15, 512237),\r\n                  datetime(2023, 3, 15, 4, 0, 15, 512237), datetime(2023, 3, 15, 5, 0, 15, 512237),\r\n                  datetime(2023, 3, 15, 6, 0, 15, 512237), datetime(2023, 3, 15, 7, 0, 15, 512237),\r\n                  datetime(2023, 3, 15, 8, 0, 15, 512237), datetime(2023, 3, 15, 9, 0, 15, 512237),\r\n                  datetime(2023, 3, 15, 10, 0, 15, 512237), datetime(2023, 3, 15, 11, 0, 15, 512237),\r\n                  datetime(2023, 3, 15, 12, 0, 15, 512237), datetime(2023, 3, 15, 13, 0, 15, 512237),\r\n                  datetime(2023, 3, 15, 14, 0, 15, 512237), datetime(2023, 3, 15, 15, 0, 15, 512237),\r\n                  datetime(2023, 3, 15, 16, 0, 15, 512237), datetime(2023, 3, 15, 17, 0, 15, 512237),\r\n                  datetime(2023, 3, 15, 18, 0, 15, 512237), datetime(2023, 3, 15, 19, 0, 15, 512237),\r\n                  datetime(2023, 3, 15, 20, 0, 15, 512237), datetime(2023, 3, 15, 21, 0, 15, 512237),\r\n                  datetime(2023, 3, 15, 22, 0, 15, 512237), datetime(2023, 3, 15, 23, 0, 15, 512237),\r\n                  datetime(2023, 3, 16, 0, 0, 15, 512237), datetime(2023, 3, 16, 1, 0, 15, 512237)],\r\n        'canopy temperature': [90, 83, 93, 91, 94, 93, 81, 78, 90, 76, 92, 94, 88, 77, 91, 86, 84, 90, 94, 86, 85, 85,\r\n                               77, 85, 84, 76],\r\n        'ambient temperature': [104, 74, 76, 100, 99, 83, 92, 101, 81, 77, 76, 78, 82, 90, 96, 101, 100, 92, 89, 84, 97,\r\n                                74, 98, 99, 93, 77],\r\n        'rh': [52, 51, 40, 36, 42, 20, 16, 27, 29, 25, 20, 29, 35, 50, 47, 15, 39, 52, 52, 50, 52, 38, 56, 38, 31, 50],\r\n        'vpd': [4, 1, 0, 1, 3, 2, 5, 2, 4, 1, 5, 5, 1, 1, 4, 5, 0, 1, 4, 3, 2, 4, 0, 5, 0, 4],\r\n        'vwc_1': [38, 24, 41, 34, 30, 29, 41, 26, 23, 34, 27, 38, 31, 40, 39, 41, 30, 44, 39, 31, 31, 29, 23, 23, 29,\r\n                  38],\r\n        'vwc_2': [20, 25, 23, 24, 21, 22, 43, 38, 20, 30, 24, 40, 32, 42, 31, 42, 24, 33, 20, 36, 41, 36, 28, 22, 24,\r\n                  35],\r\n        'vwc_3': [27, 27, 20, 38, 34, 43, 20, 44, 27, 37, 27, 24, 34, 43, 26, 23, 36, 21, 26, 36, 32, 42, 38, 23, 41,\r\n                  29], 'vwc_1_ec': [], 'vwc_2_ec': [], 'vwc_3_ec': [], 'daily gallons': [],\r\n        'daily switch': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 60, 60, 60, 60, 60, 60, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\r\n                         0.0, 0.0, 0.0, 0.0, 0.0, 0.0]}\r\n\r\n    half_and_half = {\r\n        'dates': [datetime(2023, 3, 15, 12, 0, 8, 544470), datetime(2023, 3, 15, 13, 0, 8, 544470),\r\n                  datetime(2023, 3, 15, 14, 0, 8, 544470), datetime(2023, 3, 15, 15, 0, 8, 544470),\r\n                  datetime(2023, 3, 15, 16, 0, 8, 544470), datetime(2023, 3, 15, 17, 0, 8, 544470),\r\n                  datetime(2023, 3, 15, 18, 0, 8, 544470), datetime(2023, 3, 15, 19, 0, 8, 544470),\r\n                  datetime(2023, 3, 15, 20, 0, 8, 544470), datetime(2023, 3, 15, 21, 0, 8, 544470),\r\n                  datetime(2023, 3, 15, 22, 0, 8, 544470), datetime(2023, 3, 15, 23, 0, 8, 544470),\r\n                  datetime(2023, 3, 16, 0, 0, 8, 544470), datetime(2023, 3, 16, 1, 0, 8, 544470),\r\n                  datetime(2023, 3, 16, 2, 0, 8, 544470), datetime(2023, 3, 16, 3, 0, 8, 544470),\r\n                  datetime(2023, 3, 16, 4, 0, 8, 544470), datetime(2023, 3, 16, 5, 0, 8, 544470),\r\n                  datetime(2023, 3, 16, 6, 0, 8, 544470), datetime(2023, 3, 16, 7, 0, 8, 544470),\r\n                  datetime(2023, 3, 16, 8, 0, 8, 544470), datetime(2023, 3, 16, 9, 0, 8, 544470),\r\n                  datetime(2023, 3, 16, 10, 0, 8, 544470), datetime(2023, 3, 16, 11, 0, 8, 544470),\r\n                  datetime(2023, 3, 16, 12, 0, 8, 544470)],\r\n        'canopy temperature': [76, 76, 78, 88, 78, 90, 78, 91, 82, 83, 82, 91, 93, 80, 93, 90, 83, 78, 82, 79, 89, 87,\r\n                               88, 81, 80],\r\n        'ambient temperature': [90, 103, 85, 103, 88, 76, 86, 103, 88, 75, 98, 89, 74, 93, 74, 103, 91, 95, 100, 100,\r\n                                78, 95, 79, 81, 95],\r\n        'rh': [23, 37, 27, 31, 59, 37, 21, 43, 42, 31, 32, 19, 38, 54, 20, 48, 57, 42, 45, 59, 17, 44, 48, 51, 32],\r\n        'vpd': [4, 3, 1, 3, 1, 4, 1, 5, 3, 5, 4, 3, 0, 3, 4, 4, 5, 4, 1, 2, 2, 0, 3, 3, 3],\r\n        'vwc_1': [39, 23, 20, 20, 42, 43, 28, 40, 39, 25, 41, 28, 41, 29, 37, 25, 38, 30, 28, 21, 43, 37, 20, 36, 44],\r\n        'vwc_2': [21, 42, 25, 32, 38, 31, 22, 39, 24, 28, 44, 25, 31, 29, 33, 23, 38, 38, 38, 24, 30, 24, 42, 44, 20],\r\n        'vwc_3': [40, 33, 25, 30, 40, 26, 23, 23, 41, 44, 36, 29, 40, 23, 43, 43, 25, 24, 28, 37, 20, 31, 20, 39, 25],\r\n        'vwc_1_ec': [], 'vwc_2_ec': [], 'vwc_3_ec': [], 'daily gallons': [],\r\n        'daily switch': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 0.0, 0.0, 0.0, 0.0, 0.0,\r\n                         0.0, 0.0, 0.0, 0.0]}\r\n\r\n    multiple_days = {\r\n        'dates': [datetime(2023, 3, 14, 0, 0, 36, 783492), datetime(2023, 3, 14, 1, 0, 36, 783492),\r\n                  datetime(2023, 3, 14, 2, 0, 36, 783492), datetime(2023, 3, 14, 3, 0, 36, 783492),\r\n                  datetime(2023, 3, 14, 4, 0, 36, 783492), datetime(2023, 3, 14, 5, 0, 36, 783492),\r\n                  datetime(2023, 3, 14, 6, 0, 36, 783492), datetime(2023, 3, 14, 7, 0, 36, 783492),\r\n                  datetime(2023, 3, 14, 8, 0, 36, 783492), datetime(2023, 3, 14, 9, 0, 36, 783492),\r\n                  datetime(2023, 3, 14, 10, 0, 36, 783492), datetime(2023, 3, 14, 11, 0, 36, 783492),\r\n                  datetime(2023, 3, 14, 12, 0, 36, 783492), datetime(2023, 3, 14, 13, 0, 36, 783492),\r\n                  datetime(2023, 3, 14, 14, 0, 36, 783492), datetime(2023, 3, 14, 15, 0, 36, 783492),\r\n                  datetime(2023, 3, 14, 16, 0, 36, 783492), datetime(2023, 3, 14, 17, 0, 36, 783492),\r\n                  datetime(2023, 3, 14, 18, 0, 36, 783492), datetime(2023, 3, 14, 19, 0, 36, 783492),\r\n                  datetime(2023, 3, 14, 20, 0, 36, 783492), datetime(2023, 3, 14, 21, 0, 36, 783492),\r\n                  datetime(2023, 3, 14, 22, 0, 36, 783492), datetime(2023, 3, 14, 23, 0, 36, 783492),\r\n                  datetime(2023, 3, 15, 0, 0, 36, 783492), datetime(2023, 3, 15, 1, 0, 36, 783492),\r\n                  datetime(2023, 3, 15, 2, 0, 36, 783492), datetime(2023, 3, 15, 3, 0, 36, 783492),\r\n                  datetime(2023, 3, 15, 4, 0, 36, 783492), datetime(2023, 3, 15, 5, 0, 36, 783492),\r\n                  datetime(2023, 3, 15, 6, 0, 36, 783492), datetime(2023, 3, 15, 7, 0, 36, 783492),\r\n                  datetime(2023, 3, 15, 8, 0, 36, 783492), datetime(2023, 3, 15, 9, 0, 36, 783492),\r\n                  datetime(2023, 3, 15, 10, 0, 36, 783492), datetime(2023, 3, 15, 11, 0, 36, 783492),\r\n                  datetime(2023, 3, 15, 12, 0, 36, 783492), datetime(2023, 3, 15, 13, 0, 36, 783492),\r\n                  datetime(2023, 3, 15, 14, 0, 36, 783492), datetime(2023, 3, 15, 15, 0, 36, 783492),\r\n                  datetime(2023, 3, 15, 16, 0, 36, 783492), datetime(2023, 3, 15, 17, 0, 36, 783492),\r\n                  datetime(2023, 3, 15, 18, 0, 36, 783492), datetime(2023, 3, 15, 19, 0, 36, 783492),\r\n                  datetime(2023, 3, 15, 20, 0, 36, 783492), datetime(2023, 3, 15, 21, 0, 36, 783492),\r\n                  datetime(2023, 3, 15, 22, 0, 36, 783492), datetime(2023, 3, 15, 23, 0, 36, 783492),\r\n                  datetime(2023, 3, 16, 0, 0, 36, 783492), datetime(2023, 3, 16, 1, 0, 36, 783492)],\r\n        'canopy temperature': [90, 92, 88, 90, 76, 94, 80, 76, 94, 88, 91, 86, 92, 84, 79, 90, 89, 83, 88, 93, 85, 82,\r\n                               94, 76, 93, 76, 89, 81, 83, 80, 82, 86, 78, 84, 89, 76, 93, 79, 76, 89, 92, 79, 82, 83,\r\n                               84, 93, 88, 78, 94, 84],\r\n        'ambient temperature': [101, 76, 75, 87, 81, 85, 99, 101, 99, 101, 97, 97, 90, 89, 91, 98, 89, 76, 87, 77, 87,\r\n                                96, 97, 96, 90, 82, 99, 84, 90, 77, 76, 84, 95, 93, 72, 73, 84, 77, 77, 99, 77, 93, 80,\r\n                                78, 87, 93, 80, 103, 90, 88],\r\n        'rh': [57, 24, 39, 34, 46, 59, 19, 21, 57, 41, 17, 25, 45, 38, 15, 38, 42, 34, 25, 31, 51, 35, 16, 32, 31, 36,\r\n               50, 48, 27, 38, 51, 41, 21, 24, 31, 45, 17, 34, 28, 16, 22, 35, 24, 19, 24, 23, 46, 49, 18, 20],\r\n        'vpd': [5, 0, 5, 1, 1, 2, 4, 1, 4, 5, 2, 4, 5, 5, 4, 0, 5, 2, 3, 2, 1, 4, 4, 1, 0, 3, 2, 1, 4, 3, 4, 5, 0, 3, 4,\r\n                3, 1, 2, 3, 2, 5, 5, 3, 1, 4, 5, 3, 1, 0, 0],\r\n        'vwc_1': [29, 26, 32, 39, 37, 34, 26, 25, 25, 29, 24, 39, 26, 38, 37, 26, 26, 22, 29, 34, 43, 33, 33, 26, 28,\r\n                  31, 36, 22, 25, 44, 43, 20, 31, 22, 23, 22, 43, 39, 21, 34, 40, 43, 24, 36, 22, 24, 36, 21, 30, 34],\r\n        'vwc_2': [26, 33, 21, 38, 20, 20, 24, 33, 32, 30, 25, 26, 24, 21, 31, 31, 30, 28, 31, 32, 23, 38, 31, 35, 30,\r\n                  21, 36, 37, 29, 29, 33, 43, 36, 29, 23, 35, 29, 23, 39, 31, 36, 25, 20, 37, 41, 36, 37, 28, 36, 38],\r\n        'vwc_3': [22, 27, 44, 32, 42, 38, 42, 43, 31, 28, 40, 29, 41, 40, 33, 22, 27, 32, 35, 39, 24, 38, 21, 30, 44,\r\n                  28, 26, 29, 34, 38, 26, 21, 39, 37, 21, 43, 41, 33, 28, 22, 25, 26, 28, 34, 25, 30, 41, 25, 44, 38],\r\n        'vwc_1_ec': [], 'vwc_2_ec': [], 'vwc_3_ec': [], 'daily gallons': [],\r\n        'daily switch': [0.0, 0.0, 60, 60, 60, 60, 60, 60, 60, 60, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\r\n                         0.0, 0.0, 60, 60, 60, 60, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\r\n                         0.0, 0.0, 0.0, 0.0, 0.0, 60, 60, 60, 60, 0.0]}\r\n\r\n    small_edges = {\r\n        'dates': [datetime(2023, 3, 15, 23, 0, 39, 744241), datetime(2023, 3, 16, 0, 0, 39, 744241),\r\n                  datetime(2023, 3, 16, 1, 0, 39, 744241), datetime(2023, 3, 16, 2, 0, 39, 744241),\r\n                  datetime(2023, 3, 16, 3, 0, 39, 744241), datetime(2023, 3, 16, 4, 0, 39, 744241),\r\n                  datetime(2023, 3, 16, 5, 0, 39, 744241), datetime(2023, 3, 16, 6, 0, 39, 744241),\r\n                  datetime(2023, 3, 16, 7, 0, 39, 744241), datetime(2023, 3, 16, 8, 0, 39, 744241),\r\n                  datetime(2023, 3, 16, 9, 0, 39, 744241), datetime(2023, 3, 16, 10, 0, 39, 744241),\r\n                  datetime(2023, 3, 16, 11, 0, 39, 744241), datetime(2023, 3, 16, 12, 0, 39, 744241),\r\n                  datetime(2023, 3, 16, 13, 0, 39, 744241), datetime(2023, 3, 16, 14, 0, 39, 744241),\r\n                  datetime(2023, 3, 16, 15, 0, 39, 744241), datetime(2023, 3, 16, 16, 0, 39, 744241),\r\n                  datetime(2023, 3, 16, 17, 0, 39, 744241), datetime(2023, 3, 16, 18, 0, 39, 744241),\r\n                  datetime(2023, 3, 16, 19, 0, 39, 744241), datetime(2023, 3, 16, 20, 0, 39, 744241),\r\n                  datetime(2023, 3, 16, 21, 0, 39, 744241), datetime(2023, 3, 16, 22, 0, 39, 744241),\r\n                  datetime(2023, 3, 16, 23, 0, 39, 744241), datetime(2023, 3, 17, 0, 0, 39, 744241)],\r\n        'canopy temperature': [77, 81, 90, 77, 92, 93, 79, 87, 87, 81, 86, 78, 91, 88, 88, 76, 92, 94, 88, 79, 93, 80,\r\n                               93, 90, 85, 94],\r\n        'ambient temperature': [85, 92, 90, 104, 82, 80, 86, 102, 89, 74, 79, 84, 78, 102, 103, 98, 97, 103, 84, 88,\r\n                                101, 83, 79, 73, 96, 83],\r\n        'rh': [25, 37, 51, 53, 27, 49, 54, 49, 26, 38, 58, 35, 30, 38, 20, 28, 25, 44, 32, 49, 53, 41, 36, 26, 21, 35],\r\n        'vpd': [1, 2, 5, 3, 1, 5, 3, 5, 3, 1, 2, 5, 1, 1, 4, 3, 2, 4, 5, 4, 5, 5, 3, 3, 5, 4],\r\n        'vwc_1': [34, 37, 41, 41, 42, 33, 36, 21, 42, 31, 34, 28, 33, 39, 30, 38, 25, 22, 35, 24, 39, 23, 20, 33, 39,\r\n                  44],\r\n        'vwc_2': [28, 42, 24, 37, 21, 21, 39, 42, 29, 24, 44, 22, 37, 39, 40, 28, 21, 22, 39, 38, 21, 25, 26, 33, 31,\r\n                  30],\r\n        'vwc_3': [26, 30, 40, 34, 20, 27, 44, 28, 43, 36, 37, 36, 26, 36, 41, 22, 37, 42, 26, 25, 43, 35, 43, 36, 24,\r\n                  34], 'vwc_1_ec': [], 'vwc_2_ec': [], 'vwc_3_ec': [], 'daily gallons': [],\r\n        'daily switch': [60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\r\n                         0.0, 0.0, 0.0, 0.0, 0.0]}\r\n\r\n    lacking_full_days = {\r\n        'dates': [datetime(2023, 3, 16, 16, 0, 32, 92243), datetime(2023, 3, 16, 17, 0, 32, 92243),\r\n                  datetime(2023, 3, 16, 18, 0, 32, 92243), datetime(2023, 3, 16, 19, 0, 32, 92243),\r\n                  datetime(2023, 3, 16, 20, 0, 32, 92243), datetime(2023, 3, 16, 21, 0, 32, 92243),\r\n                  datetime(2023, 3, 16, 22, 0, 32, 92243), datetime(2023, 3, 16, 23, 0, 32, 92243),\r\n                  datetime(2023, 3, 17, 0, 0, 32, 92243), datetime(2023, 3, 17, 1, 0, 32, 92243)],\r\n        'canopy temperature': [86, 88, 88, 83, 76, 86, 91, 82, 80, 84],\r\n        'ambient temperature': [95, 100, 86, 72, 94, 89, 90, 89, 86, 103],\r\n        'rh': [28, 18, 38, 16, 21, 46, 53, 51, 47, 31], 'vpd': [5, 2, 3, 3, 2, 3, 5, 5, 4, 5],\r\n        'vwc_1': [25, 40, 40, 35, 29, 21, 24, 41, 39, 34], 'vwc_2': [37, 23, 35, 26, 38, 36, 44, 32, 30, 34],\r\n        'vwc_3': [23, 40, 23, 31, 29, 44, 38, 27, 37, 33], 'vwc_1_ec': [], 'vwc_2_ec': [], 'vwc_3_ec': [],\r\n        'daily gallons': [], 'daily switch': [0.0, 0.0, 0.0, 60, 60, 60, 60, 60, 60, 0.0]}\r\n\r\n    small_med_edges = {\r\n        'dates': [datetime(2023, 3, 15, 21, 0, 23, 131219), datetime(2023, 3, 15, 22, 0, 23, 131219),\r\n                  datetime(2023, 3, 15, 23, 0, 23, 131219), datetime(2023, 3, 16, 0, 0, 23, 131219),\r\n                  datetime(2023, 3, 16, 1, 0, 23, 131219), datetime(2023, 3, 16, 2, 0, 23, 131219),\r\n                  datetime(2023, 3, 16, 3, 0, 23, 131219), datetime(2023, 3, 16, 4, 0, 23, 131219),\r\n                  datetime(2023, 3, 16, 5, 0, 23, 131219), datetime(2023, 3, 16, 6, 0, 23, 131219),\r\n                  datetime(2023, 3, 16, 7, 0, 23, 131219), datetime(2023, 3, 16, 8, 0, 23, 131219),\r\n                  datetime(2023, 3, 16, 9, 0, 23, 131219), datetime(2023, 3, 16, 10, 0, 23, 131219),\r\n                  datetime(2023, 3, 16, 11, 0, 23, 131219), datetime(2023, 3, 16, 12, 0, 23, 131219),\r\n                  datetime(2023, 3, 16, 13, 0, 23, 131219), datetime(2023, 3, 16, 14, 0, 23, 131219),\r\n                  datetime(2023, 3, 16, 15, 0, 23, 131219), datetime(2023, 3, 16, 16, 0, 23, 131219),\r\n                  datetime(2023, 3, 16, 17, 0, 23, 131219), datetime(2023, 3, 16, 18, 0, 23, 131219),\r\n                  datetime(2023, 3, 16, 19, 0, 23, 131219), datetime(2023, 3, 16, 20, 0, 23, 131219),\r\n                  datetime(2023, 3, 16, 21, 0, 23, 131219), datetime(2023, 3, 16, 22, 0, 23, 131219),\r\n                  datetime(2023, 3, 16, 23, 0, 23, 131219), datetime(2023, 3, 17, 0, 0, 23, 131219),\r\n                  datetime(2023, 3, 17, 1, 0, 23, 131219), datetime(2023, 3, 17, 2, 0, 23, 131219)],\r\n        'canopy temperature': [87, 83, 79, 76, 90, 84, 79, 90, 94, 93, 81, 81, 82, 82, 89, 81, 76, 76, 88, 91, 85, 88,\r\n                               84, 86, 85, 85, 88, 92, 89, 84],\r\n        'ambient temperature': [80, 102, 102, 93, 96, 98, 102, 95, 94, 102, 104, 94, 95, 75, 81, 102, 101, 85, 74, 79,\r\n                                101, 103, 77, 95, 89, 92, 94, 84, 103, 94],\r\n        'rh': [33, 58, 55, 43, 27, 25, 41, 25, 58, 59, 15, 30, 54, 23, 21, 56, 29, 43, 39, 37, 31, 37, 37, 56, 43, 53,\r\n               58, 25, 53, 25],\r\n        'vpd': [4, 5, 3, 5, 2, 4, 5, 2, 1, 4, 3, 1, 3, 3, 1, 4, 5, 5, 1, 4, 4, 5, 3, 3, 4, 1, 2, 2, 1, 2],\r\n        'vwc_1': [26, 26, 37, 29, 42, 37, 40, 25, 35, 35, 33, 42, 20, 35, 33, 23, 21, 23, 42, 39, 21, 28, 35, 43, 44,\r\n                  22, 26, 44, 41, 42],\r\n        'vwc_2': [28, 20, 31, 20, 23, 23, 38, 28, 42, 27, 28, 32, 30, 25, 27, 25, 42, 22, 38, 24, 21, 38, 44, 23, 25,\r\n                  26, 40, 44, 28, 43],\r\n        'vwc_3': [20, 27, 42, 40, 21, 39, 40, 22, 27, 24, 23, 22, 36, 20, 24, 35, 39, 37, 21, 26, 32, 37, 27, 21, 42,\r\n                  32, 22, 41, 29, 42], 'vwc_1_ec': [], 'vwc_2_ec': [], 'vwc_3_ec': [], 'daily gallons': [],\r\n        'daily switch': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 60, 60, 60, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,\r\n                         0.0, 0.0, 60, 60, 60, 60, 60, 0.0, 0.0, 0.0, 0.0]}\r\n\r\n    med_edges = {\r\n        'dates': [datetime(2023, 3, 15, 16, 0, 21, 121390), datetime(2023, 3, 15, 17, 0, 21, 121390),\r\n                  datetime(2023, 3, 15, 18, 0, 21, 121390), datetime(2023, 3, 15, 19, 0, 21, 121390),\r\n                  datetime(2023, 3, 15, 20, 0, 21, 121390), datetime(2023, 3, 15, 21, 0, 21, 121390),\r\n                  datetime(2023, 3, 15, 22, 0, 21, 121390), datetime(2023, 3, 15, 23, 0, 21, 121390),\r\n                  datetime(2023, 3, 16, 0, 0, 21, 121390), datetime(2023, 3, 16, 1, 0, 21, 121390),\r\n                  datetime(2023, 3, 16, 2, 0, 21, 121390), datetime(2023, 3, 16, 3, 0, 21, 121390),\r\n                  datetime(2023, 3, 16, 4, 0, 21, 121390), datetime(2023, 3, 16, 5, 0, 21, 121390),\r\n                  datetime(2023, 3, 16, 6, 0, 21, 121390), datetime(2023, 3, 16, 7, 0, 21, 121390),\r\n                  datetime(2023, 3, 16, 8, 0, 21, 121390), datetime(2023, 3, 16, 9, 0, 21, 121390),\r\n                  datetime(2023, 3, 16, 10, 0, 21, 121390), datetime(2023, 3, 16, 11, 0, 21, 121390),\r\n                  datetime(2023, 3, 16, 12, 0, 21, 121390), datetime(2023, 3, 16, 13, 0, 21, 121390),\r\n                  datetime(2023, 3, 16, 14, 0, 21, 121390), datetime(2023, 3, 16, 15, 0, 21, 121390),\r\n                  datetime(2023, 3, 16, 16, 0, 21, 121390), datetime(2023, 3, 16, 17, 0, 21, 121390),\r\n                  datetime(2023, 3, 16, 18, 0, 21, 121390), datetime(2023, 3, 16, 19, 0, 21, 121390),\r\n                  datetime(2023, 3, 16, 20, 0, 21, 121390), datetime(2023, 3, 16, 21, 0, 21, 121390),\r\n                  datetime(2023, 3, 16, 22, 0, 21, 121390), datetime(2023, 3, 16, 23, 0, 21, 121390),\r\n                  datetime(2023, 3, 17, 0, 0, 21, 121390), datetime(2023, 3, 17, 1, 0, 21, 121390),\r\n                  datetime(2023, 3, 17, 2, 0, 21, 121390), datetime(2023, 3, 17, 3, 0, 21, 121390),\r\n                  datetime(2023, 3, 17, 4, 0, 21, 121390), datetime(2023, 3, 17, 5, 0, 21, 121390),\r\n                  datetime(2023, 3, 17, 6, 0, 21, 121390), datetime(2023, 3, 17, 7, 0, 21, 121390)],\r\n        'canopy temperature': [91, 87, 87, 85, 76, 93, 86, 88, 78, 84, 91, 79, 91, 81, 83, 82, 82, 78, 77, 94, 79, 92,\r\n                               90, 94, 93, 88, 91, 84, 83, 89, 77, 94, 94, 79, 85, 77, 84, 77, 76, 86],\r\n        'ambient temperature': [100, 74, 90, 80, 98, 90, 96, 85, 77, 92, 87, 76, 88, 102, 95, 77, 83, 96, 80, 104, 95,\r\n                                100, 79, 77, 104, 78, 91, 91, 74, 78, 78, 97, 77, 79, 99, 102, 100, 75, 86, 102],\r\n        'rh': [56, 34, 24, 47, 54, 32, 21, 24, 46, 18, 30, 41, 36, 27, 32, 49, 55, 32, 52, 21, 21, 58, 41, 43, 44, 58,\r\n               27, 51, 50, 22, 46, 18, 48, 43, 32, 46, 49, 33, 51, 56],\r\n        'vpd': [1, 4, 2, 5, 1, 2, 5, 2, 5, 5, 5, 2, 3, 5, 3, 1, 4, 5, 3, 3, 1, 2, 5, 1, 4, 4, 2, 5, 1, 3, 1, 2, 1, 2, 2,\r\n                1, 2, 5, 4, 2],\r\n        'vwc_1': [27, 37, 26, 27, 34, 24, 31, 32, 21, 44, 39, 37, 25, 21, 22, 42, 42, 27, 42, 34, 44, 26, 22, 28, 26,\r\n                  42, 26, 23, 23, 39, 29, 29, 37, 24, 26, 39, 24, 26, 25, 41],\r\n        'vwc_2': [35, 31, 31, 27, 32, 22, 42, 33, 28, 44, 35, 30, 26, 30, 40, 33, 26, 39, 25, 41, 30, 28, 33, 27, 25,\r\n                  31, 20, 43, 31, 38, 44, 39, 24, 37, 43, 31, 20, 22, 42, 44],\r\n        'vwc_3': [27, 20, 40, 24, 37, 34, 34, 35, 38, 44, 24, 37, 32, 28, 29, 26, 22, 25, 23, 34, 32, 43, 23, 40, 28,\r\n                  42, 30, 35, 29, 28, 33, 29, 25, 44, 23, 34, 28, 24, 26, 34], 'vwc_1_ec': [], 'vwc_2_ec': [],\r\n        'vwc_3_ec': [], 'daily gallons': [],\r\n        'daily switch': [0.0, 0.0, 60, 60, 60, 60, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 60, 60, 60, 60, 60, 0.0, 0.0, 0.0,\r\n                         0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 60, 60, 60, 0.0, 0.0]}\r\n\r\n    overnight_irrigation_day_one = {\r\n        'dates': [datetime(2023, 7, 20, 2, 0), datetime(2023, 7, 20, 3, 0), datetime(2023, 7, 20, 4, 0),\r\n                  datetime(2023, 7, 20, 5, 0), datetime(2023, 7, 20, 6, 0), datetime(2023, 7, 20, 7, 0),\r\n                  datetime(2023, 7, 20, 8, 0), datetime(2023, 7, 20, 9, 0), datetime(2023, 7, 20, 10, 0),\r\n                  datetime(2023, 7, 20, 11, 0), datetime(2023, 7, 20, 12, 0), datetime(2023, 7, 20, 13, 0),\r\n                  datetime(2023, 7, 20, 14, 0), datetime(2023, 7, 20, 15, 0), datetime(2023, 7, 20, 16, 0),\r\n                  datetime(2023, 7, 20, 17, 0), datetime(2023, 7, 20, 18, 0), datetime(2023, 7, 20, 19, 0),\r\n                  datetime(2023, 7, 20, 20, 0), datetime(2023, 7, 20, 21, 0), datetime(2023, 7, 20, 22, 0),\r\n                  datetime(2023, 7, 20, 23, 0), datetime(2023, 7, 21, 0, 0), datetime(2023, 7, 21, 1, 0),\r\n                  datetime(2023, 7, 21, 2, 0), datetime(2023, 7, 21, 3, 0)],\r\n        'canopy temperature': [55.256, 52.646, 50.432, 54.014, 52.646, 54.032, 60.836, 65.53399999999999, 69.476, 73.112, 76.622, 79.43,\r\n                               81.10400000000001, 81.69800000000001, 81.518, 82.4, 81.95, 81.05000000000001, 78.35, 75.992, 69.782,\r\n                               60.980000000000004, 56.678, 54.59, 53.474000000000004, 51.602000000000004],\r\n        'ambient temperature': [63.032, 61.736000000000004, 59.900000000000006, 61.358, 60.403999999999996, 59.252, 64.598, 68.684, 73.346, 78.332, 82.706, 86.36, 89.924, 92.588, 94.244, 95.23400000000001, 95.036, 94.028, 90.878, 86.25200000000001, 81.536, 72.84200000000001, 68.53999999999999, 65.012, 62.852000000000004, 60.8],\r\n        'rh': [80.52663133187149, 84.9819295323405, 85.41674613884034, 80.08178528513635, 80.04695849791608, 83.16846927153982, 75.26573241955552, 70.81848333402235, 62.873531439573505, 56.13080204457853, 53.03899459857565, 50.8968949812234, 46.67802474050185, 40.80560744310242, 34.37989538811474, 33.32984365718972, 32.0236663681046, 31.38527916263018, 36.93748457949296, 39.63722226367993, 40.44633845441565, 59.775765008068696, 70.74914888721767, 78.17381475287601, 82.4200261823118, 82.39672792694286],\r\n        'vpd': [0.3828092904396303, 0.282046319709216, 0.25660811894772273, 0.3691055404373449, 0.35744845339418174, 0.28940161040013845, 0.5136422510676344, 0.6980308939829762, 1.0404511422480271, 1.451343961529694, 1.7920602690880454, 2.1070279709709396, 2.561116696698284, 3.0898708301793576, 3.605490249825002, 3.7765930278545765, 3.8272110422802132, 3.7449728003172726, 3.120902235250944, 2.5812835113023276, 2.1880037708846514, 1.108297234218054, 0.6962406481023409, 0.4598435833744532, 0.3434087461203972, 0.3198197180442852],\r\n        'vwc_1': [37.81065358526901, 37.79828504828351, 37.79004264992585, 37.761214986098366, 37.72419817839436, 37.69954984881112, 37.6380322318548, 37.389371860434025, 36.958990627096796, 36.38341214214202, 35.828577821592766, 35.3420855502482, 34.83238091480594, 35.17096666579316, 38.83677691331808, 39.07367340431285, 39.208122199644, 39.31703769827042, 39.40886566369435, 39.46586716084299, 39.492215771878136, 39.61551279683273, 39.68621804209782, 39.810391166228996, 39.868233487841124, 39.92173395220333],\r\n        'vwc_2': [36.75641704982944, 36.75246138128032, 36.75246138128032, 36.748506339913774, 36.74455192560839, 36.73664497769727, 36.708990394633425, 36.67347955751499, 36.61834103141008, 36.55547508905435, 36.49276815504403, 36.422412301226316, 36.352256358073596, 36.282299619206526, 36.23964604119867, 36.23964604119867, 36.247395730636626, 36.26290244917483, 36.2900627749064, 36.352256358073596, 36.426315708681486, 36.524101777013044, 36.62621047918755, 36.75641704982944, 36.91117861667661, 37.0468830523206],\r\n        'vwc_3': [35.24521542653574, 35.24893394212669, 35.252653037420515, 35.24893394212669, 35.23778013397809, 35.23778013397809, 35.21548815657716, 35.17467361276228, 35.115431652549134, 35.041586267445936, 34.96797005037418, 34.89091860430733, 34.81046666495629, 34.74484591327776, 34.66126258585405, 34.57436120037958, 34.520210985084, 34.513000410496275, 34.49858592464108, 34.51660542005071, 34.54185604711748, 34.58520628060442, 34.63588308364716, 34.690301353063546, 34.7995171730743, 34.98635267312977],\r\n        'vwc_1_ec': [None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None],\r\n        'vwc_2_ec': [None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None],\r\n        'vwc_3_ec': [None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None],\r\n        'daily gallons': [],\r\n        'daily switch': [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 26, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60]}\r\n\r\n    overnight_irrigation_day_two = {\r\n        'dates': [datetime(2023, 7, 21, 1, 0), datetime(2023, 7, 21, 2, 0), datetime(2023, 7, 21, 3, 0), datetime(2023, 7, 21, 4, 0), datetime(2023, 7, 21, 5, 0), datetime(2023, 7, 21, 6, 0), datetime(2023, 7, 21, 7, 0), datetime(2023, 7, 21, 8, 0), datetime(2023, 7, 21, 9, 0), datetime(2023, 7, 21, 10, 0), datetime(2023, 7, 21, 11, 0), datetime(2023, 7, 21, 12, 0), datetime(2023, 7, 21, 13, 0), datetime(2023, 7, 21, 14, 0), datetime(2023, 7, 21, 15, 0), datetime(2023, 7, 21, 16, 0), datetime(2023, 7, 21, 17, 0), datetime(2023, 7, 21, 18, 0), datetime(2023, 7, 21, 19, 0), datetime(2023, 7, 21, 20, 0), datetime(2023, 7, 21, 21, 0), datetime(2023, 7, 21, 22, 0), datetime(2023, 7, 21, 23, 0), datetime(2023, 7, 22, 0, 0), datetime(2023, 7, 22, 1, 0), datetime(2023, 7, 22, 2, 0)],\r\n        'canopy temperature': [54.59, 53.474000000000004, 51.602000000000004, 50.522, 50.666, 49.226, 54.734, 66.524, 69.512, 74.156, 77.55799999999999, 81.212, 83.732, 85.55000000000001, 85.69399999999999, 86.21600000000001, 85.82, 84.758, 83.94800000000001, 81.68, 73.22, 68.576, 65.39, 62.150000000000006, 60.674, 58.532],\r\n        'ambient temperature': [65.012, 62.852000000000004, 60.8, 59.792, 60.476, 58.298, 59.432, 69.404, 75.686, 80.348, 85.082, 89.042, 92.318, 95.108, 97.052, 98.65400000000001, 99.464, 99.536, 97.538, 94.55000000000001, 83.98400000000001, 77.52199999999999, 73.634, 70.898, 69.44, 66.344],\r\n        'rh': [78.17381475287601, 82.4200261823118, 82.39672792694286, 83.34996194814208, 84.68639802744981, 86.41868764880799, 86.44958779401792, 73.17213815093343, 58.20340342969467, 52.87868262989771, 47.53165878959346, 45.52971631712544, 43.02344763973963, 40.99106914614765, 36.61769065878929, 34.558226440246166, 34.31095218764193, 34.081545019652154, 37.921839234075506, 42.007816273763446, 62.11206565314386, 72.42868645910339, 77.25167903094132, 80.93471469328003, 80.78140174444168, 83.65649351429238],\r\n        'vpd': [0.4598435833744532, 0.3434087461203972, 0.3198197180442852, 0.29185023034442614, 0.2750381305944678, 0.22567762907448796, 0.2344882974855773, 0.6577528738868952, 1.2667506022921484, 1.6655056023450625, 2.161359706479848, 2.5446741768934658, 2.949247191178093, 3.329692538107112, 3.7959085317662358, 4.114938426925017, 4.233006531524808, 4.257040557524185, 3.773291682458248, 3.216586818070887, 1.5066830711980659, 0.8880994207624084, 0.643711958913475, 0.4918571205358999, 0.4717729516669893, 0.36064281091893724],\r\n        'vwc_1': [39.810391166228996, 39.868233487841124, 39.92173395220333, 39.970866930672024, 39.993228886621466, 40.02456588706544, 40.042488610257806, 40.015608853715804, 39.993228886621466, 39.988755055024505, 39.97980955268336, 39.96192718802165, 39.99770343866413, 40.02456588706544, 40.042488610257806, 40.06939435081267, 40.11878911490907, 40.1367725539453, 40.1772776764044, 40.2133315549563, 40.258464284680564, 40.30819432891111, 40.34894814283696, 40.40337855847787, 40.42608897370039, 40.45336563172306],\r\n        'vwc_2': [36.75641704982944, 36.91117861667661, 37.0468830523206, 37.11501074163381, 37.151152973989184, 37.16321188771556, 37.17125436162099, 37.13106756872091, 37.09896412581797, 37.05889220971872, 37.02288191017966, 36.97095787484219, 36.91515947184176, 36.87537934688412, 36.85948496992455, 36.835662315643525, 36.823759491438594, 36.83963118300228, 36.8674308970145, 36.903218802307336, 36.96696815857785, 37.042881272306836, 37.07491333127058, 37.11902399079601, 37.151152973989184, 37.17929939512668],\r\n        'vwc_3': [34.690301353063546, 34.7995171730743, 34.98635267312977, 35.286151001679, 35.552069624724545, 35.74111697485347, 35.881969738714005, 35.96610907095909, 35.95844800581616, 35.90870978026892, 35.820959986301574, 35.710769736515324, 35.61240370425121, 35.53324619765732, 35.48437415173851, 35.45809956500814, 35.45809956500814, 35.503159331271746, 35.60107954599157, 35.74111697485347, 35.88960674851385, 36.04285247977002, 36.10442144014985, 36.14298092881212, 36.18160124622358, 36.1970664323692],\r\n        'vwc_1_ec': [None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None],\r\n        'vwc_2_ec': [None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None],\r\n        'vwc_3_ec': [None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None], 'daily gallons': [],\r\n        'daily switch': [60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60]}\r\n\r\n    overnight_irrigation_day_three = {\r\n        'dates': [datetime(2023, 7, 22, 2, 0), datetime(2023, 7, 22, 3, 0), datetime(2023, 7, 22, 4, 0), datetime(2023, 7, 22, 5, 0), datetime(2023, 7, 22, 6, 0), datetime(2023, 7, 22, 7, 0), datetime(2023, 7, 22, 8, 0), datetime(2023, 7, 22, 9, 0), datetime(2023, 7, 22, 10, 0), datetime(2023, 7, 22, 11, 0), datetime(2023, 7, 22, 12, 0), datetime(2023, 7, 22, 13, 0), datetime(2023, 7, 22, 14, 0), datetime(2023, 7, 22, 15, 0), datetime(2023, 7, 22, 16, 0), datetime(2023, 7, 22, 17, 0), datetime(2023, 7, 22, 18, 0), datetime(2023, 7, 22, 19, 0), datetime(2023, 7, 22, 20, 0), datetime(2023, 7, 22, 21, 0), datetime(2023, 7, 22, 22, 0), datetime(2023, 7, 22, 23, 0), datetime(2023, 7, 23, 0, 0), datetime(2023, 7, 23, 1, 0), datetime(2023, 7, 23, 2, 0), datetime(2023, 7, 23, 3, 0)],\r\n        'canopy temperature': [58.532, 56.948, 55.147999999999996, 54.41, 53.672, 56.804, 66.884, 69.98, 73.922, 77.846, 80.582, 82.886, 85.04599999999999, 85.80199999999999, 85.56800000000001, 85.298, 84.30799999999999, 83.048, 79.016, 73.22, 71.258, 68.594, 66.362, 65.53399999999999, 65.03, 64.958],\r\n        'ambient temperature': [66.344, 65.156, 63.30200000000001, 62.635999999999996, 61.664, 61.772, 70.05199999999999, 76.766, 80.474, 85.51400000000001, 88.952, 91.094, 94.676, 97.412, 99.35600000000001, 100.274, 100.058, 98.76200000000001, 89.78, 81.19399999999999, 77.48599999999999, 75.326, 73.58000000000001, 71.654, 70.646, 69.80000000000001],\r\n        'rh': [83.65649351429238, 84.3944299919226, 84.60400435099564, 83.20356948394618, 83.16993384184633, 84.2357419641575, 75.12122437098719, 55.451363620329666, 52.35162079540826, 46.232604551676936, 41.666313570327986, 43.27197278745828, 40.192790165691264, 36.54826043011582, 32.05664575550409, 31.07477928066611, 30.332670272672672, 35.61829353341872, 60.44205115297182, 70.01781272516467, 75.1569111796218, 75.2687377411146, 78.06462167640082, 80.92529333584851, 82.02210854992953, 82.69107240727365],\r\n        'vpd': [0.36064281091893724, 0.3304383192955198, 0.3055396360133882, 0.32561875158039033, 0.3152731024683151, 0.29643692981774383, 0.6236151620489818, 1.3994917258434119, 1.6910782745029445, 2.2457060686395165, 2.71743947707016, 2.826439812924091, 3.3301628241633403, 3.8420077458036417, 4.3640051256890215, 4.5514258247385175, 4.57058296915861, 4.061556017406509, 1.891439320591132, 1.0893611419507776, 0.7992689936938517, 0.7406031614779964, 0.6195829578734493, 0.5048856790057883, 0.4598469477200515, 0.43015340311300454],\r\n        'vwc_1': [40.45336563172306, 40.485221743660915, 40.49433009094425, 40.50799811634924, 40.5171138043519, 40.51255559316331, 40.49433009094425, 40.4761163309697, 40.462463712173324, 40.44427048247332, 40.42608897370039, 40.42608897370039, 40.41700261223875, 40.430633252480504, 40.44427048247332, 40.47156472479321, 40.49433009094425, 40.5216727500366, 40.55360594588748, 40.5992873219101, 40.63588545758981, 40.66794755055376, 40.67711479316234, 40.68628499071156, 40.700045829668326, 40.709223418807916],\r\n        'vwc_2': [37.17929939512668, 37.19137174675629, 37.211505144535685, 37.22359287557082, 37.2316545700992, 37.227623402036954, 37.19942318350298, 37.17125436162099, 37.14311689484595, 37.11501074163381, 37.09495406646659, 37.066901496992614, 37.0468830523206, 37.02288191017966, 36.99091596624822, 36.97095787484219, 36.97095787484219, 36.98692307955209, 37.030879747926626, 37.09495406646659, 37.15919161031036, 37.19942318350298, 37.227623402036954, 37.25585505876743, 37.2639270265811, 37.267963974456194],\r\n        'vwc_3': [36.1970664323692, 36.22028251350562, 36.23577211351336, 36.251271492631744, 36.27065948021286, 36.27065948021286, 36.243520580199196, 36.200934253560746, 36.13912224465059, 36.07361756452598, 36.00445057900821, 35.90870978026892, 35.8400090624988, 35.79051255997969, 35.77910463132148, 35.78290667726512, 35.824768604829906, 35.91253218883277, 36.07361756452598, 36.23189879702258, 36.29394527203087, 36.33280397887848, 36.348364653062994, 36.36004161270008, 36.37561950376518, 36.37561950376518],\r\n        'vwc_1_ec': [None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None],\r\n        'vwc_2_ec': [None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None],\r\n        'vwc_3_ec': [None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None],\r\n        'daily gallons': [],\r\n        'daily switch': [60, 60, 60, 60, 60, 60, 59, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60]\r\n\r\n    }\r\n\r\n    overnight_irrigation_day_four = {\r\n        'dates': [datetime(2023, 7, 23, 2, 0), datetime(2023, 7, 23, 3, 0), datetime(2023, 7, 23, 4, 0),\r\n                  datetime(2023, 7, 23, 5, 0), datetime(2023, 7, 23, 6, 0), datetime(2023, 7, 23, 7, 0),\r\n                  datetime(2023, 7, 23, 8, 0), datetime(2023, 7, 23, 9, 0), datetime(2023, 7, 23, 10, 0),\r\n                  datetime(2023, 7, 23, 11, 0), datetime(2023, 7, 23, 12, 0), datetime(2023, 7, 23, 13, 0),\r\n                  datetime(2023, 7, 23, 14, 0), datetime(2023, 7, 23, 15, 0), datetime(2023, 7, 23, 16, 0),\r\n                  datetime(2023, 7, 23, 17, 0), datetime(2023, 7, 23, 18, 0), datetime(2023, 7, 23, 19, 0),\r\n                  datetime(2023, 7, 23, 20, 0)],\r\n        'canopy temperature': [58.532, 56.948, 55.147999999999996, 54.41, 53.672, 56.804, 66.884, 69.98, 73.922, 77.846,\r\n                               80.582, 82.886, 85.04599999999999, 85.80199999999999, 85.56800000000001, 85.298,\r\n                               84.30799999999999, 83.048, 79.016],\r\n        'ambient temperature': [66.344, 65.156, 63.30200000000001, 62.635999999999996, 61.664, 61.772,\r\n                                70.05199999999999, 76.766, 80.474, 85.51400000000001, 88.952, 91.094, 94.676, 97.412,\r\n                                99.35600000000001, 100.274, 100.058, 98.76200000000001, 89.78],\r\n        'rh': [83.65649351429238, 84.3944299919226, 84.60400435099564, 83.20356948394618, 83.16993384184633,\r\n               84.2357419641575, 75.12122437098719, 55.451363620329666, 52.35162079540826, 46.232604551676936,\r\n               41.666313570327986, 43.27197278745828, 40.192790165691264, 36.54826043011582, 32.05664575550409,\r\n               31.07477928066611, 30.332670272672672, 35.61829353341872, 60.44205115297182],\r\n        'vpd': [0.36064281091893724, 0.3304383192955198, 0.3055396360133882, 0.32561875158039033, 0.3152731024683151,\r\n                0.29643692981774383, 0.6236151620489818, 1.3994917258434119, 1.6910782745029445, 2.2457060686395165,\r\n                2.71743947707016, 2.826439812924091, 3.3301628241633403, 3.8420077458036417, 4.3640051256890215,\r\n                4.5514258247385175, 4.57058296915861, 4.061556017406509, 1.891439320591132],\r\n        'vwc_1': [40.45336563172306, 40.485221743660915, 40.49433009094425, 40.50799811634924, 40.5171138043519,\r\n                  40.51255559316331, 40.49433009094425, 40.4761163309697, 40.462463712173324, 40.44427048247332,\r\n                  40.42608897370039, 40.42608897370039, 40.41700261223875, 40.430633252480504, 40.44427048247332,\r\n                  40.47156472479321, 40.49433009094425, 40.5216727500366, 40.55360594588748],\r\n        'vwc_2': [37.17929939512668, 37.19137174675629, 37.211505144535685, 37.22359287557082, 37.2316545700992,\r\n                  37.227623402036954, 37.19942318350298, 37.17125436162099, 37.14311689484595, 37.11501074163381,\r\n                  37.09495406646659, 37.066901496992614, 37.0468830523206, 37.02288191017966, 36.99091596624822,\r\n                  36.97095787484219, 36.97095787484219, 36.98692307955209, 37.030879747926626],\r\n        'vwc_3': [36.1970664323692, 36.22028251350562, 36.23577211351336, 36.251271492631744, 36.27065948021286,\r\n                  36.27065948021286, 36.243520580199196, 36.200934253560746, 36.13912224465059, 36.07361756452598,\r\n                  36.00445057900821, 35.90870978026892, 35.8400090624988, 35.79051255997969, 35.77910463132148,\r\n                  35.78290667726512, 35.824768604829906, 35.91253218883277, 36.07361756452598],\r\n        'vwc_1_ec': [None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None,\r\n                     None, None, None],\r\n        'vwc_2_ec': [None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None,\r\n                     None, None, None],\r\n        'vwc_3_ec': [None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None,\r\n                     None, None, None],\r\n        'daily gallons': [],\r\n        'daily switch': [60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60]\r\n\r\n    }\r\n\r\n    overnight_irrigation_day_five = {\r\n        'dates': [datetime(2023, 7, 23, 20, 0), datetime(2023, 7, 23, 21, 0), datetime(2023, 7, 23, 22, 0),\r\n                  datetime(2023, 7, 23, 23, 0), datetime(2023, 7, 24, 0, 0), datetime(2023, 7, 24, 1, 0),\r\n                  datetime(2023, 7, 24, 2, 0), datetime(2023, 7, 24, 3, 0), datetime(2023, 7, 24, 4, 0),\r\n                  datetime(2023, 7, 24, 5, 0), datetime(2023, 7, 24, 6, 0), datetime(2023, 7, 24, 7, 0),\r\n                  datetime(2023, 7, 24, 8, 0), datetime(2023, 7, 24, 9, 0), datetime(2023, 7, 24, 10, 0),\r\n                  datetime(2023, 7, 24, 11, 0), datetime(2023, 7, 24, 12, 0), datetime(2023, 7, 24, 13, 0),\r\n                  datetime(2023, 7, 24, 14, 0), datetime(2023, 7, 24, 15, 0), datetime(2023, 7, 24, 16, 0),\r\n                  datetime(2023, 7, 24, 17, 0), datetime(2023, 7, 24, 18, 0), datetime(2023, 7, 24, 19, 0),\r\n                  datetime(2023, 7, 24, 20, 0), datetime(2023, 7, 24, 21, 0),\r\n                  datetime(2023, 7, 24, 22, 0), datetime(2023, 7, 24, 23, 0), datetime(2023, 7, 25, 0, 0)],\r\n        'canopy temperature': [58.532, 56.948, 55.147999999999996, 54.41, 53.672, 56.804, 66.884, 69.98, 73.922, 77.846,\r\n                               80.582, 82.886, 85.04599999999999, 85.80199999999999, 85.56800000000001, 85.298,\r\n                               84.30799999999999, 83.048, 79.016, 73.22, 71.258, 68.594, 66.362, 65.53399999999999,\r\n                               65.03, 64.958, 0, 0, 0],\r\n        'ambient temperature': [66.344, 65.156, 63.30200000000001, 62.635999999999996, 61.664, 61.772,\r\n                                70.05199999999999, 76.766, 80.474, 85.51400000000001, 88.952, 91.094, 94.676, 97.412,\r\n                                99.35600000000001, 100.274, 100.058, 98.76200000000001, 89.78, 81.19399999999999,\r\n                                77.48599999999999, 75.326, 73.58000000000001, 71.654, 70.646, 69.80000000000001,\r\n                                0, 0, 0],\r\n        'rh': [83.65649351429238, 84.3944299919226, 84.60400435099564, 83.20356948394618, 83.16993384184633,\r\n               84.2357419641575, 75.12122437098719, 55.451363620329666, 52.35162079540826, 46.232604551676936,\r\n               41.666313570327986, 43.27197278745828, 40.192790165691264, 36.54826043011582, 32.05664575550409,\r\n               31.07477928066611, 30.332670272672672, 35.61829353341872, 60.44205115297182, 70.01781272516467,\r\n               75.1569111796218, 75.2687377411146, 78.06462167640082, 80.92529333584851, 82.02210854992953,\r\n               82.69107240727365, 0, 0, 0],\r\n        'vpd': [0.36064281091893724, 0.3304383192955198, 0.3055396360133882, 0.32561875158039033, 0.3152731024683151,\r\n                0.29643692981774383, 0.6236151620489818, 1.3994917258434119, 1.6910782745029445, 2.2457060686395165,\r\n                2.71743947707016, 2.826439812924091, 3.3301628241633403, 3.8420077458036417, 4.3640051256890215,\r\n                4.5514258247385175, 4.57058296915861, 4.061556017406509, 1.891439320591132, 1.0893611419507776,\r\n                0.7992689936938517, 0.7406031614779964, 0.6195829578734493, 0.5048856790057883, 0.4598469477200515,\r\n                0.43015340311300454, 0, 0, 0],\r\n        'vwc_1': [40.45336563172306, 40.485221743660915, 40.49433009094425, 40.50799811634924, 40.5171138043519,\r\n                  40.51255559316331, 40.49433009094425, 40.4761163309697, 40.462463712173324, 40.44427048247332,\r\n                  40.42608897370039, 40.42608897370039, 40.41700261223875, 40.430633252480504, 40.44427048247332,\r\n                  40.47156472479321, 40.49433009094425, 40.5216727500366, 40.55360594588748, 40.5992873219101,\r\n                  40.63588545758981, 40.66794755055376, 40.67711479316234, 40.68628499071156, 40.700045829668326,\r\n                  40.709223418807916, 0, 0, 0],\r\n        'vwc_2': [37.17929939512668, 37.19137174675629, 37.211505144535685, 37.22359287557082, 37.2316545700992,\r\n                  37.227623402036954, 37.19942318350298, 37.17125436162099, 37.14311689484595, 37.11501074163381,\r\n                  37.09495406646659, 37.066901496992614, 37.0468830523206, 37.02288191017966, 36.99091596624822,\r\n                  36.97095787484219, 36.97095787484219, 36.98692307955209, 37.030879747926626, 37.09495406646659,\r\n                  37.15919161031036, 37.19942318350298, 37.227623402036954, 37.25585505876743, 37.2639270265811,\r\n                  37.267963974456194, 0, 0, 0],\r\n        'vwc_3': [36.1970664323692, 36.22028251350562, 36.23577211351336, 36.251271492631744, 36.27065948021286,\r\n                  36.27065948021286, 36.243520580199196, 36.200934253560746, 36.13912224465059, 36.07361756452598,\r\n                  36.00445057900821, 35.90870978026892, 35.8400090624988, 35.79051255997969, 35.77910463132148,\r\n                  35.78290667726512, 35.824768604829906, 35.91253218883277, 36.07361756452598, 36.23189879702258,\r\n                  36.29394527203087, 36.33280397887848, 36.348364653062994, 36.36004161270008, 36.37561950376518,\r\n                  36.37561950376518, 0, 0, 0],\r\n        'vwc_1_ec': [None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None,\r\n                     None, None, None, None, None, None, None, None, None, None, None, None, None],\r\n        'vwc_2_ec': [None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None,\r\n                     None, None, None, None, None, None, None, None, None, None, None, None, None],\r\n        'vwc_3_ec': [None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None,\r\n                     None, None, None, None, None, None, None, None, None, None, None, None, None],\r\n        'daily gallons': [],\r\n        'daily switch': [60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60, 60,\r\n                         60, 60, 0, 0, 0]\r\n\r\n    }\r\n\r\n    # test_case_1 = SwitchTestCase()\r\n    # test_case_standard = SwitchTestCase(120, standard_data_2023, 0, [480])\r\n    # test_case_half_and_half = SwitchTestCase(55, half_and_half, 0, [415, 240])\r\n    # test_case_multiple_days = SwitchTestCase(60, multiple_days, 60, [660, 300])\r\n    # test_case_small_edges = SwitchTestCase(60, small_edges, 0, [660])\r\n    # test_case_lacking_full_days = SwitchTestCase(0, lacking_full_days, 60, [300])\r\n    # test_case_small_med_edges = SwitchTestCase(0, small_med_edges, 0, [480])\r\n    # test_case_med_edges = SwitchTestCase(240, med_edges, 180, [300])\r\n\r\n    test_case_overnight_irrigation_day_one = SwitchTestCase(0, overnight_irrigation_day_one, 240, [506])\r\n    test_case_overnight_irrigation_day_two = SwitchTestCase(0, overnight_irrigation_day_two, 240, [1440])\r\n    test_case_overnight_irrigation_day_three = SwitchTestCase(0, overnight_irrigation_day_three, 240, [1439])\r\n    test_case_overnight_irrigation_day_four = SwitchTestCase(0, overnight_irrigation_day_four, 240, [1260])\r\n    test_case_overnight_irrigation_day_five = SwitchTestCase(0, overnight_irrigation_day_five, 240, [1320])\r\n\r\n    # test_cases.append(test_case_standard)\r\n    # test_cases.append(test_case_half_and_half)\r\n    # test_cases.append(test_case_multiple_days)\r\n    # test_cases.append(test_case_small_edges)\r\n    # test_cases.append(test_case_lacking_full_days)\r\n    # test_cases.append(test_case_small_med_edges)\r\n    # test_cases.append(test_case_med_edges)\r\n    test_cases.append(test_case_overnight_irrigation_day_one)\r\n    test_cases.append(test_case_overnight_irrigation_day_two)\r\n    test_cases.append(test_case_overnight_irrigation_day_three)\r\n    test_cases.append(test_case_overnight_irrigation_day_four)\r\n    test_cases.append(test_case_overnight_irrigation_day_five)\r\n\r\n    # test_cases.append(test_case_small_edges)\r\n\r\n    test_results = []\r\n\r\n    logger = Logger('z6', '', '', 'tomato', 'Clay', 0, 0, 'N', None, planting_date='3/1/2023')\r\n\r\n    # Loop through each test case and record results\r\n    for ind, test_case in enumerate(test_cases):\r\n        print(f'================TEST CASE {ind + 1}================')\r\n        test_result = test_data_pipeline(test_case, logger)\r\n        test_result_tup = (ind, test_result)\r\n        test_results.append(test_result_tup)\r\n    print('All Test Results: ', test_results)\r\n    # print(test_results)\r\n\r\n\r\ndef test_data_pipeline(test_case, logger):\r\n    \"\"\"\r\n    Method to simulate a standard STomato run including switch processing, high/low temp processing,\r\n     final results cleanup, kc calculation, last day removal\r\n    :param test_case: SwitchTestCase instance with all the switch test criteria\r\n    :return:\r\n    \"\"\"\r\n\r\n    # Grab test criteria\r\n    in_prev_switch = test_case.in_prev_switch\r\n    in_test_case_data = test_case.in_test_case_data\r\n    out_prev_switch = test_case.out_prev_switch\r\n    out_switch_values = test_case.out_switch_values\r\n\r\n    test_pass = False\r\n    cwsi_processor = CwsiProcessor()\r\n    # logger = Logger('z6', '', '', 'tomato', 'Clay', 0, 0, 'N', None, planting_date='3/1/2023')\r\n\r\n    for key, values in in_test_case_data.items():\r\n        print(key, \" : \", values)\r\n    print('=========================================')\r\n    print()\r\n    print('>================Testing switch================')\r\n    cwsi_processor.update_irrigation_ledger(in_test_case_data, logger.irrigation_ledger)\r\n    print('After updating ledger: ')\r\n    for date, list in logger.irrigation_ledger.items():\r\n        print(f'{date} : {list}')\r\n    print('<================Testing switch================')\r\n    print()\r\n\r\n    print('>================Testing Getting High and Low Temp Indexes================')\r\n    highest_temp_values_ind, lowest_temp_values_ind, _ = cwsi_processor.get_highest_and_lowest_temperature_indexes(\r\n        in_test_case_data\r\n    )\r\n    print('<================Testing Getting High and Low Temp Indexes================')\r\n    print()\r\n\r\n    print('>================Testing Final Results================')\r\n    final_results_converted = cwsi_processor.final_results(\r\n        in_test_case_data, highest_temp_values_ind, lowest_temp_values_ind, logger\r\n    )\r\n    print('After final results ledger: ')\r\n    for date, list in logger.irrigation_ledger.items():\r\n        print(f'{date} : {list}')\r\n    print('<================Testing Final Results================')\r\n    print()\r\n\r\n    print('>================Testing Switch Overflow================')\r\n    logger.check_and_update_delayed_ledger_filled_lists()\r\n\r\n    print('After switch overflow ledger: ')\r\n    for date, list in logger.irrigation_ledger.items():\r\n        print(f'{date} : {list}')\r\n    print('<================Testing Switch Overflow================')\r\n\r\n    print('>================Testing get kc================')\r\n    final_results_converted = logger.get_kc(final_results_converted)\r\n    print('<================Testing get kc================')\r\n    print()\r\n\r\n    print()\r\n    print('=======================================')\r\n    print('================RESULTS================')\r\n    for key, values in final_results_converted.items():\r\n        print(key, \" : \", values)\r\n    print()\r\n\r\n    switch_list_length_match = False\r\n    switch_list_values_match = False\r\n    output_prev_switch_match = False\r\n\r\n    data_points = len(out_switch_values)\r\n    switch_data_points = len(final_results_converted['daily switch'])\r\n    if data_points == switch_data_points:\r\n        switch_list_length_match = True\r\n\r\n    if switch_list_length_match:\r\n        if final_results_converted['daily switch'] == out_switch_values:\r\n            switch_list_values_match = True\r\n\r\n    if switch_list_length_match and switch_list_values_match:\r\n        test_pass = True\r\n        print('SUCCESS')\r\n    else:\r\n        print('FAIL')\r\n        print('Switch list length match: ', switch_list_length_match)\r\n        print('Switch list values match: ', switch_list_values_match)\r\n    print('=======================================')\r\n    print()\r\n    print()\r\n    return test_pass\r\n\r\n\r\ndef cleanup_cimis_stations_pickle():\r\n    \"\"\"\r\n    Go through the cimis stations pickle and remove any cimis station objects for stations that are no longer part\r\n    of our current operating growers pickle\r\n    \"\"\"\r\n    growers = open_pickle()\r\n    cimis_stations_pickle_list = open_pickle(filename=\"cimisStation.pickle\")\r\n    grower_pickle_cimis_stations = []\r\n\r\n    for grower in growers:\r\n        for field in grower.fields:\r\n            if field.cimis_station not in grower_pickle_cimis_stations:\r\n                grower_pickle_cimis_stations.append(field.cimis_station)\r\n\r\n    # print('Grower Pickle Active Stations #:')\r\n    # grower_pickle_cimis_stations.sort()\r\n    # print(grower_pickle_cimis_stations)\r\n\r\n    pickle_indexes_to_be_removed = []\r\n    for ind, station in enumerate(cimis_stations_pickle_list):\r\n        if station.station_number not in grower_pickle_cimis_stations:\r\n            pickle_indexes_to_be_removed.append(ind)\r\n\r\n    # Reverse the list of indexes we will be removing using pop() so we don't run into an index out of bounds issue\r\n    # from removing from our list from the beginning\r\n    pickle_indexes_to_be_removed.reverse()\r\n    for ind in pickle_indexes_to_be_removed:\r\n        cimis_stations_pickle_list.pop(ind)\r\n\r\n    # print('Pickle Cimis Stations:')\r\n    # pickle_stations = []\r\n    # for station in cimis_stations_pickle_list:\r\n    #     pickle_stations.append(station.station_number)\r\n    # pickle_stations.sort()\r\n    # print(pickle_stations)\r\n    write_pickle(cimis_stations_pickle_list, filename=\"cimisStation.pickle\")\r\n\r\n\r\ndef turn_ai_game_data_into_csv(pickle_name, pickle_path, csv_name):\r\n    global ai_game_data, vwc, field_capacity, wilting_point, psi, psi_threshold, psi_critical, et_hours\r\n    grower_pickle_file_name = pickle_name\r\n    grower_pickle_file_path = pickle_path\r\n    ai_game_data = open_pickle(filename=grower_pickle_file_name, specific_file_path=grower_pickle_file_path)\r\n    # ai_game_data.show_content()\r\n    stage_dict = {'Stage 1': 1, 'Stage 2': 2, 'Stage 3': 3, 'Stage 4': 4}\r\n    all_data = []\r\n    all_labels = []\r\n    for data_point in ai_game_data.ai_game_data:\r\n        print(data_point)\r\n        data_point_values = []\r\n\r\n        crop_stage = stage_dict[data_point.crop_stage]\r\n        vwc = data_point.vwc_avg\r\n        field_capacity = data_point.field_capacity\r\n        wilting_point = data_point.wilting_point\r\n        psi = data_point.psi\r\n        psi_threshold = data_point.psi_threshold\r\n        psi_critical = data_point.psi_critical\r\n        et_hours = data_point.et_hours\r\n        label_hours = data_point.human_p2\r\n\r\n        data_point_values.append(crop_stage)\r\n        data_point_values.append(vwc)\r\n        data_point_values.append(field_capacity)\r\n        data_point_values.append(wilting_point)\r\n        data_point_values.append(psi)\r\n        data_point_values.append(psi_threshold)\r\n        data_point_values.append(psi_critical)\r\n        data_point_values.append(et_hours)\r\n        all_data.append(data_point_values)\r\n        all_labels.append(label_hours)\r\n    df = pd.DataFrame(\r\n        all_data,\r\n        columns=['crop_stage', 'vwc', 'field_capacity', 'wilting_point', 'psi', 'psi_threshold',\r\n                 'psi_critical',\r\n                 'et_hours']\r\n    )\r\n    df['irrigation_hours'] = all_labels\r\n    df.to_csv(csv_name + '.csv', index=False)\r\n    print(all_data)\r\n    print(all_labels)\r\n\r\n\r\ndef get_tomato_yield_data(pickle_name: str, pickle_path: str, excel_filename: str, excel_data_start_row: int,\r\n                          excel_data_end_row: int):\r\n    growers = open_pickle(filename=pickle_name, specific_file_path=pickle_path)\r\n    for grower in growers:\r\n        for field in grower.fields:\r\n            field.crop_type = field.loggers[0].cropType\r\n            field.net_yield = None\r\n            field.paid_yield = None\r\n\r\n    whole_file_df = pd.read_excel(excel_filename)\r\n    df = whole_file_df.loc[excel_data_start_row:excel_data_end_row,\r\n         ['Grower', 'Field - variety area', 'Grower field #', 'Gradient Field #', 'Loads', 'Acres', 'Net T/A',\r\n          'Paid T/A']]\r\n\r\n    mask_grower = None\r\n    for grower in growers:\r\n        tomato_fields = 0\r\n        for field in grower.fields:\r\n            if field.crop_type in ['Tomato', 'Tomatoes', 'tomato', 'tomatoes']:\r\n                if hasattr(field.loggers[0], 'rnd') and not field.loggers[0].rnd:\r\n                    # print(f'Grower: {grower.name} - Field: {field.nickname}')\r\n                    mask_grower_field = (df['Grower'] == grower.name) & (df['Gradient Field #'] == field.nickname)\r\n                    tomato_fields += 1\r\n                    results_count = mask_grower_field.sum()\r\n                    search_grower_field = df[mask_grower_field]\r\n\r\n                    net_yield_search = search_grower_field.loc[mask_grower_field, 'Net T/A']\r\n                    if not net_yield_search.empty:\r\n                        net_yield = round(net_yield_search.mean(), 1)\r\n                    else:\r\n                        net_yield = None\r\n\r\n                    paid_yield_search = search_grower_field.loc[mask_grower_field, 'Paid T/A']\r\n                    if not paid_yield_search.empty:\r\n                        paid_yield = round(paid_yield_search.mean(), 1)\r\n                    else:\r\n                        paid_yield = None\r\n\r\n                    print(\r\n                        f'{grower.name:25} - {field.nickname:20}\\t Net: {str(net_yield):10}  Paid: {str(paid_yield):10} Results: {results_count}'\r\n                    )\r\n                    field.net_yield = net_yield\r\n                    field.paid_yield = paid_yield\r\n                    results_count = 0\r\n\r\n    write_pickle(growers, filename=pickle_name, specific_file_path=pickle_path)\r\n\r\n\r\ndef graph_yields(pickle_name: str, pickle_path: str, yield_type: str):\r\n    yield_data = []\r\n    growers = open_pickle(filename=pickle_name, specific_file_path=pickle_path)\r\n    for grower in growers:\r\n        for field in grower.fields:\r\n            if field.net_yield is not None:\r\n                # field_names.append(grower.name + '-' + field.nickname)\r\n                # net_yields.append(field.net_yield)\r\n                # paid_yields.append(field.paid_yield)\r\n                yield_data.append((grower.name + '-' + field.nickname, field.net_yield, field.paid_yield))\r\n\r\n    # Extract the field names and yield values\r\n    field_names, net_yields, paid_yields = zip(*yield_data)\r\n\r\n    # Create a figure with two subplots\r\n    fig, ax = plt.subplots(ncols=1, figsize=(50, 10))\r\n\r\n    if yield_type in ['Net', 'net', 'n', 'N']:\r\n        # Sort the data by net_yield\r\n        sorted_data = sorted(zip(net_yields, paid_yields, field_names), reverse=True)\r\n\r\n        # Unpack the sorted data into separate lists\r\n        net_yields, paid_yields, field_names = zip(*sorted_data)\r\n\r\n        # Plot the net yields\r\n        ax.bar(field_names, net_yields)\r\n        ax.set_title('Net Yields')\r\n        ax.tick_params(axis='x', rotation=90)  # Rotate x-axis labels by 90 degrees\r\n\r\n        # Add data labels to the net yields bars\r\n        for i, v in enumerate(net_yields):\r\n            ax.text(i, v + 1, str(v), ha='center', fontsize=8)\r\n\r\n    if yield_type in ['Paid', 'paid', 'p', 'P']:\r\n        # Sort the data by paid_yield\r\n        sorted_data = sorted(zip(paid_yields, net_yields, field_names), reverse=True)\r\n\r\n        # Unpack the sorted data into separate lists\r\n        paid_yields, net_yields, field_names = zip(*sorted_data)\r\n\r\n        # Plot the net yields\r\n        ax.bar(field_names, paid_yields)\r\n        ax.set_title('Paid Yields')\r\n        ax.tick_params(axis='x', rotation=90)  # Rotate x-axis labels by 90 degrees\r\n\r\n        # Add data labels to the paid yields bars\r\n        for i, v in enumerate(paid_yields):\r\n            ax.text(i, v + 1, str(v), ha='center', fontsize=8)\r\n\r\n    fig.tight_layout()\r\n    plt.show()\r\n\r\n\r\ndef compare_new_psi_algo_vs_old():\r\n    # all_data = {\r\n    #     'logger': [],\r\n    #     'field': [],\r\n    #     'grower': [],\r\n    #     'soil type': [],\r\n    #     'field capacity': [],\r\n    #     'wilting point': [],\r\n    #     'net yield': [],\r\n    #     'paid yield': [],\r\n    #     'ambient temp hours in opt with sun': [],\r\n    #     'ambient temp hours in opt without sun': [],\r\n    #     'vwc 1 in optimum hours': [],\r\n    #     'vwc 2 in optimum hours': [],\r\n    #     'vwc 3 in optimum hours': [],\r\n    #     'vwc 1 in optimum %': [],\r\n    #     'vwc 2 in optimum %': [],\r\n    #     'vwc 3 in optimum %': [],\r\n    #     'vwc 1_2 in optimum hours': [],\r\n    #     'vwc 2_3 in optimum hours': [],\r\n    #     'vwc 1_2 in optimum %': [],\r\n    #     'vwc 2_3 in optimum %': [],\r\n    #     'vwc total datapoints': [],\r\n    #     'psi average': [],\r\n    #     'psi first 3 values': [],\r\n    #     'psi first 3 dates': []\r\n    # }\r\n\r\n    all_data_pickle_file_name = 'all_2022_analysis.pickle'\r\n    all_data_pickle_file_path = 'H:\\\\Shared drives\\\\Stomato\\\\Data Analysis\\\\All Data\\\\Pickles\\\\'\r\n    new_algo_data = open_pickle(filename=all_data_pickle_file_name, specific_file_path=all_data_pickle_file_path)\r\n\r\n    db_2022_psi_file_name = 'psi_pickle_2022.pickle'\r\n    # db_2022_psi_file_path = 'H:\\\\Shared drives\\\\Stomato\\\\2023\\\\Pickle\\\\'\r\n    db_2022_psi_file_path = 'H:\\\\Shared drives\\\\Stomato\\\\Data Analysis\\\\All Data\\\\Pickles\\\\'\r\n    old_algo_data = open_pickle(filename=db_2022_psi_file_name, specific_file_path=db_2022_psi_file_path)\r\n\r\n    for ind, dp in enumerate(new_algo_data['logger']):\r\n        logger_index = -1\r\n        new_algo_first_date = None\r\n        old_algo_last_date = None\r\n        days_diff = None\r\n        try:\r\n            logger_index = old_algo_data['logger'].index(dp)\r\n        except ValueError:\r\n            print(f'{dp} not in database tables')\r\n\r\n        if logger_index >= 0:\r\n            if new_algo_data['psi first 3 dates'][ind]:\r\n                new_algo_first_date_dt = new_algo_data['psi first 3 dates'][ind][0]\r\n                new_algo_first_date = new_algo_first_date_dt.date()\r\n\r\n            if old_algo_data['dates'][logger_index]:\r\n                old_algo_last_date = old_algo_data['dates'][logger_index][-1]\r\n\r\n        if new_algo_first_date is not None and old_algo_last_date is not None:\r\n            days_diff_dt = new_algo_first_date - old_algo_last_date\r\n            days_diff = days_diff_dt.days\r\n\r\n        print(f'Field: {new_algo_data[\"field\"][ind]}\\tLogger: {dp} found')\r\n        if days_diff is not None:\r\n            print(f'Days between: {days_diff}')\r\n            print(f'New dates: {new_algo_data[\"psi first 3 dates\"][ind]}')\r\n            print(f'Old dates: {old_algo_data[\"dates\"][logger_index]}')\r\n        else:\r\n            print('One of the two dates is None')\r\n            print(f'New: {new_algo_first_date}')\r\n            print(f'Old {old_algo_last_date}')\r\n        print()\r\n\r\n\r\ndef setup_weather_stations():\r\n    updated_stations = []\r\n\r\n    # Atmos 41 Weather Stations\r\n    eight_mile = WeatherStation(\r\n        'z6-07111',\r\n        '99294-35668',\r\n        '8 Mile',\r\n        'Tomatoes',\r\n        datetime(2023, 1, 1).date(),\r\n        planting_date=None,\r\n        start_date=None,\r\n        end_date=None,\r\n        station_type='Weather Station',\r\n        cimis_station='248'\r\n    )\r\n    eight_mile.uninstall_date = datetime(2023, month=12, day=31).date()\r\n    updated_stations.append(eight_mile)\r\n\r\n    ave_7 = WeatherStation(\r\n        'z6-02178',\r\n        '74962-10103',\r\n        'Ave 7',\r\n        'Tomatoes',\r\n        datetime(2023, 1, 1).date(),\r\n        planting_date=None,\r\n        start_date=None,\r\n        end_date=None,\r\n        station_type='Weather Station',\r\n        cimis_station='80'\r\n    )\r\n    ave_7.uninstall_date = datetime(2023, month=12, day=31).date()\r\n    updated_stations.append(ave_7)\r\n\r\n    ben_fast = WeatherStation(\r\n        'z6-15905',\r\n        '28402-51648',\r\n        'Ben Fast',\r\n        'Tomatoes',\r\n        datetime(2023, month=4, day=12).date(),\r\n        planting_date=None,\r\n        start_date=None,\r\n        end_date=None,\r\n        station_type='Weather Station',\r\n        cimis_station='5'\r\n    )\r\n    ben_fast.uninstall_date = datetime(2023, month=12, day=31).date()\r\n    updated_stations.append(ben_fast)\r\n\r\n    bone_farms = WeatherStation(\r\n        'z6-16164',\r\n        '64334-12769',\r\n        'Bone Farms',\r\n        'Tomatoes',\r\n        datetime(2023, 1, 1).date(),\r\n        planting_date=None,\r\n        start_date=None,\r\n        end_date=None,\r\n        station_type='Weather Station',\r\n        cimis_station='146'\r\n    )\r\n    bone_farms.uninstall_date = datetime(2023, month=12, day=31).date()\r\n    updated_stations.append(bone_farms)\r\n\r\n    bullseye = WeatherStation(\r\n        'z6-02054',\r\n        '16128-76869',\r\n        'Bullseye',\r\n        'Tomatoes',\r\n        datetime(2023, 1, 1).date(),\r\n        planting_date=None,\r\n        start_date=None,\r\n        end_date=None,\r\n        station_type='Weather Station',\r\n        cimis_station='250'\r\n    )\r\n    bullseye.uninstall_date = datetime(2023, month=12, day=31).date()\r\n    updated_stations.append(bullseye)\r\n\r\n    bullero = WeatherStation(\r\n        'z6-11967',\r\n        '36891-99736',\r\n        'Bullero',\r\n        'Tomatoes',\r\n        datetime(2023, 1, 1).date(),\r\n        planting_date=None,\r\n        start_date=None,\r\n        end_date=None,\r\n        station_type='Weather Station',\r\n        cimis_station='226'\r\n    )\r\n    bullero.uninstall_date = datetime(2023, month=12, day=31).date()\r\n    updated_stations.append(bullero)\r\n\r\n    david_santos = WeatherStation(\r\n        'z6-12376',\r\n        '78094-52955',\r\n        'David Santos',\r\n        'Tomatoes',\r\n        datetime(2023, 5, 17).date(),\r\n        planting_date=None,\r\n        start_date=datetime(2023, 4, 14).date(),\r\n        end_date=datetime(2023, 9, 2).date(),\r\n        station_type='Weather Station',\r\n        cimis_station='124'\r\n    )\r\n    david_santos.uninstall_date = datetime(2023, month=12, day=31).date()\r\n    updated_stations.append(david_santos)\r\n\r\n    dresick_n = WeatherStation(\r\n        'z6-16147',\r\n        '13969-76188',\r\n        'Dresick N',\r\n        'Tomatoes',\r\n        datetime(2023, month=4, day=15).date(),\r\n        planting_date=None,\r\n        start_date=None,\r\n        end_date=None,\r\n        station_type='Weather Station',\r\n        cimis_station='2'\r\n    )\r\n    dresick_n.uninstall_date = datetime(2023, month=12, day=31).date()\r\n    updated_stations.append(dresick_n)\r\n\r\n    dresick_s = WeatherStation(\r\n        'z6-16138',\r\n        '65607-66874',\r\n        'Dresick S',\r\n        'Tomatoes',\r\n        datetime(2023, month=4, day=15).date(),\r\n        planting_date=None,\r\n        start_date=None,\r\n        end_date=None,\r\n        station_type='Weather Station',\r\n        cimis_station='2'\r\n    )\r\n    dresick_s.uninstall_date = datetime(2023, month=12, day=31).date()\r\n    updated_stations.append(dresick_s)\r\n\r\n    fantozzi = WeatherStation(\r\n        'z6-23435',\r\n        '92415-62348',\r\n        'Fantozzi',\r\n        'Tomatoes',\r\n        datetime(2023, month=7, day=5).date(),\r\n        planting_date=None,\r\n        start_date=None,\r\n        end_date=None,\r\n        station_type='Weather Station',\r\n        cimis_station='71'\r\n    )\r\n    fantozzi.uninstall_date = datetime(2023, month=12, day=31).date()\r\n    updated_stations.append(fantozzi)\r\n\r\n    lafayet = WeatherStation(\r\n        'z6-07111',\r\n        '99294-35668',\r\n        'Lafayet',\r\n        'Tomatoes',\r\n        datetime(2023, month=6, day=16).date(),\r\n        planting_date=None,\r\n        start_date=None,\r\n        end_date=None,\r\n        station_type='Weather Station',\r\n        cimis_station='248'\r\n    )\r\n    lafayet.uninstall_date = datetime(2023, month=12, day=31).date()\r\n    updated_stations.append(lafayet)\r\n\r\n    matteoli_mb = WeatherStation(\r\n        'z6-11406',\r\n        '71062-40854',\r\n        'Matteoli MB',\r\n        'Tomatoes',\r\n        datetime(2023, month=1, day=1).date(),\r\n        planting_date=None,\r\n        start_date=None,\r\n        end_date=None,\r\n        station_type='Weather Station',\r\n        cimis_station='235'\r\n    )\r\n    matteoli_mb.uninstall_date = datetime(2023, month=12, day=31).date()\r\n    updated_stations.append(matteoli_mb)\r\n\r\n    matteoli_kbasin = WeatherStation(\r\n        'z6-11920',\r\n        '84949-16401',\r\n        'Matteoli KBasin',\r\n        'Tomatoes',\r\n        datetime(2023, month=1, day=1).date(),\r\n        planting_date=None,\r\n        start_date=None,\r\n        end_date=None,\r\n        station_type='Weather Station',\r\n        cimis_station='235'\r\n    )\r\n    matteoli_kbasin.uninstall_date = datetime(2023, month=12, day=31).date()\r\n    updated_stations.append(matteoli_kbasin)\r\n\r\n    nees = WeatherStation(\r\n        'z6-16154',\r\n        '51205-71538',\r\n        'Nees',\r\n        'Tomatoes',\r\n        datetime(2023, month=1, day=1).date(),\r\n        planting_date=None,\r\n        start_date=None,\r\n        end_date=None,\r\n        station_type='Weather Station',\r\n        cimis_station='124'\r\n    )\r\n    nees.uninstall_date = datetime(2023, month=12, day=31).date()\r\n    updated_stations.append(nees)\r\n\r\n    rg_farms = WeatherStation(\r\n        'z6-23255',\r\n        '56884-23705',\r\n        'RG Farms',\r\n        'Tomatoes',\r\n        datetime(2023, month=7, day=27).date(),\r\n        planting_date=None,\r\n        start_date=None,\r\n        end_date=None,\r\n        station_type='Weather Station',\r\n        cimis_station='126'\r\n    )\r\n    rg_farms.uninstall_date = datetime(2023, month=12, day=31).date()\r\n    updated_stations.append(rg_farms)\r\n\r\n    tp = WeatherStation(\r\n        'z6-16139',\r\n        '29296-99927',\r\n        'TP',\r\n        'Tomatoes',\r\n        datetime(2023, month=1, day=1).date(),\r\n        planting_date=None,\r\n        start_date=None,\r\n        end_date=None,\r\n        station_type='Weather Station',\r\n        cimis_station='250'\r\n    )\r\n    tp.uninstall_date = datetime(2023, month=12, day=31).date()\r\n    updated_stations.append(tp)\r\n\r\n    wild_oak = WeatherStation(\r\n        'z6-01874',\r\n        '67392-80462',\r\n        'Wild Oak',\r\n        'Tomatoes',\r\n        datetime(2023, month=1, day=1).date(),\r\n        planting_date=None,\r\n        start_date=None,\r\n        end_date=None,\r\n        station_type='Weather Station',\r\n        cimis_station='39'\r\n    )\r\n    wild_oak.uninstall_date = datetime(2023, month=12, day=31).date()\r\n    updated_stations.append(wild_oak)\r\n\r\n    sanguinetti = WeatherStation(\r\n        'z6-07162',\r\n        '68090-60446',\r\n        'Sanguinetti',\r\n        'Tomatoes',\r\n        datetime(2023, month=5, day=26).date(),\r\n        planting_date=None,\r\n        start_date=None,\r\n        end_date=None,\r\n        station_type='Weather Station',\r\n        cimis_station='70'\r\n    )\r\n    sanguinetti.uninstall_date = datetime(2023, month=12, day=31).date()\r\n    updated_stations.append(sanguinetti)\r\n\r\n    # Gradient Atmos 14 Stations\r\n    bullseye_oe10_E = WeatherStation(\r\n        'z6-11974',\r\n        '60644-59745',\r\n        'Bullseye OE10 East Gradient',\r\n        'Tomatoes',\r\n        datetime(2023, month=4, day=26).date(),\r\n        planting_date=datetime(2023, month=4, day=16).date(),\r\n        start_date=None,\r\n        end_date=None,\r\n        station_type='Gradient Station',\r\n        cimis_station='226'\r\n    )\r\n    bullseye_oe10_E.uninstall_date = datetime(2023, month=8, day=11).date()\r\n    updated_stations.append(bullseye_oe10_E)\r\n\r\n    bullseye_oe10_C = WeatherStation(\r\n        'z6-11407',\r\n        '60892-87745',\r\n        'Bullseye OE10 Center Gradient',\r\n        'Tomatoes',\r\n        datetime(2023, month=4, day=26).date(),\r\n        planting_date=datetime(2023, month=4, day=16).date(),\r\n        start_date=None,\r\n        end_date=None,\r\n        station_type='Gradient Station',\r\n        cimis_station='226'\r\n    )\r\n    bullseye_oe10_C.uninstall_date = datetime(2023, month=8, day=11).date()\r\n    updated_stations.append(bullseye_oe10_C)\r\n\r\n    bullseye_oe10_W = WeatherStation(\r\n        'z6-11580',\r\n        '93196-73070',\r\n        'Bullseye OE10 West Gradient',\r\n        'Tomatoes',\r\n        datetime(2023, month=4, day=26).date(),\r\n        planting_date=datetime(2023, month=4, day=16).date(),\r\n        start_date=None,\r\n        end_date=None,\r\n        station_type='Gradient Station',\r\n        cimis_station='226'\r\n    )\r\n    bullseye_oe10_W.uninstall_date = datetime(2023, month=8, day=11).date()\r\n    updated_stations.append(bullseye_oe10_W)\r\n\r\n    # lucero_towerline_blue_s = WeatherStation(\r\n    #     'z6-12295',\r\n    #     '52213-43606',\r\n    #     'Lucero Towerline Blue S Gradient',\r\n    #     'Tomatoes',\r\n    #     datetime(2023, month=4, day=6).date(),\r\n    #     planting_date=datetime(2023, month=3, day=28).date(),\r\n    #     start_date=None,\r\n    #     end_date=None,\r\n    #     station_type='Gradient Station'\r\n    # )\r\n    # lucero_towerline_blue_s.uninstall_date = datetime(2023, month=7, day=14).date()\r\n    # updated_stations.append(lucero_towerline_blue_s)\r\n\r\n    print()\r\n    pickle_file_name = 'weather_stations_2023.pickle'\r\n    pickle_file_path = 'H:\\\\Shared drives\\\\Stomato\\\\HeatUnits\\\\Pickles\\\\'\r\n    write_pickle(updated_stations, filename=pickle_file_name, specific_file_path=pickle_file_path)\r\n\r\n\r\ndef generate_gradient_grower_fields_report():\r\n    growers = open_pickle()\r\n    data = []\r\n    data2 = {\r\n        'Grower': [],\r\n        'Field': [],\r\n        'Crop Type': [],\r\n        'Field Type': [],\r\n        'Acres': [],\r\n        'Region': []\r\n    }\r\n\r\n    for grower in growers:\r\n        print(grower.name)\r\n        for field in grower.fields:\r\n            print('\\t', field.nickname, field.crop_type, field.acres)\r\n            rnd = False\r\n            for logger in field.loggers:\r\n                if logger.rnd:\r\n                    print('\\t\\t', logger.name, 'R&D')\r\n                    rnd = True\r\n                    break\r\n            if rnd:\r\n                field_type = 'R&D'\r\n            else:\r\n                field_type = field.field_type\r\n            field_dict = {\r\n                'Grower': grower.name,\r\n                'Field': field.nickname,\r\n                'Crop Type': field.crop_type,\r\n                'Field Type': field_type,\r\n                'Acres': float(field.acres),\r\n                'Region': grower.region\r\n            }\r\n            data.append(field_dict)\r\n            data2['Grower'].append(grower.name)\r\n            data2['Field'].append(field.nickname)\r\n            data2['Crop Type'].append(field.crop_type)\r\n            data2['Field Type'].append(field_type)\r\n            data2['Acres'].append(float(field.acres))\r\n            data2['Region'].append(grower.region)\r\n\r\n\r\n    print()\r\n    # df = pd.DataFrame(data)\r\n    # df.to_excel(\"gradient_grower_fields_2024.xlsx\", index=False)\r\n\r\n    df2 = pd.DataFrame(data2)\r\n    # df2.to_excel(\"gradient_grower_fields_2024_v2.xlsx\", index=False)\r\n    df2.to_excel('gradient_grower_fields_2024_v2.xlsx', engine='xlsxwriter', index=False)\r\n\r\n\r\n\r\ndef update_grower_field_gpm_and_irrigation_set_acres(grower_name: str, field_name: str, gpm = None, irrigation_set_acres = None):\r\n    growers = open_pickle()\r\n    for grower in growers:\r\n        if grower.name == grower_name:\r\n            for field in grower.fields:\r\n                if field.name == field_name:\r\n                    for logger in field.loggers:\r\n                        if gpm is not None:\r\n                            logger.gpm = float(gpm)\r\n                        if irrigation_set_acres is not None:\r\n                            logger.irrigation_set_acres = float(irrigation_set_acres)\r\n    write_pickle(growers)\r\n\r\n\r\n\r\n# generate_gradient_grower_fields_report()\r\n# show_pickle()\r\n\r\n# growers = open_pickle()\r\n#\r\n# for grower in growers:\r\n#     if grower.name == 'Riley Chaney Farms':\r\n#         for field in grower.fields:\r\n#             print(f'Field: {field.name}')\r\n#             for logger in field.loggers:\r\n#                 print(f'\\tLogger: {logger.name}  |  GPM: {logger.gpm}  |  Irrigation Set Acres: {logger.irrigation_set_acres}')\r\n#\r\n# update_grower_field_gpm_and_irrigation_set_acres('Riley Chaney Farms', 'Riley Chaney Farms18', gpm=380)\r\n# update_grower_field_gpm_and_irrigation_set_acres('Riley Chaney Farms', 'Riley Chaney Farms4W', gpm=520)\r\n# update_grower_field_gpm_and_irrigation_set_acres('Riley Chaney Farms', 'Riley Chaney Farms16', gpm=650)\r\n# update_grower_field_gpm_and_irrigation_set_acres('Riley Chaney Farms', 'Riley Chaney Farms4E', gpm=1000)\r\n# update_grower_field_gpm_and_irrigation_set_acres('Riley Chaney Farms', 'Riley Chaney Farms27', gpm=1270)\r\n#\r\n#\r\n# for grower in growers:\r\n#     if grower.name == 'Riley Chaney Farms':\r\n#         for field in grower.fields:\r\n#             print(f'Field: {field.name}')\r\n#             for logger in field.loggers:\r\n#                 print(f'\\tLogger: {logger.name}  |  GPM: {logger.gpm}  |  Irrigation Set Acres: {logger.irrigation_set_acres}')\r\n\r\n# show_pickle()\r\n# growers = open_pickle()\r\n# for grower in growers:\r\n#     if grower.name == 'Knight Farms':\r\n#         for field in grower.fields:\r\n#             if field.name == 'Knight FarmsNC1':\r\n#                 for logger in field.loggers:\r\n#                     logger.ir_active = True\r\n# write_pickle(growers)\r\n\r\n# remove_grower('Riley Chaney Farms')\r\n# remove_field('Riley Chaney Farms', 'Riley Chaney Farms13')\r\n# remove_field('Riley Chaney Farms', 'Riley Chaney Farms3')\r\n# remove_field('Riley Chaney Farms', 'Riley Chaney Farms15')\r\n# remove_field('Riley Chaney Farms', 'Riley Chaney Farms24')\r\n# remove_field('Riley Chaney Farms', 'Riley Chaney Farms27')\r\n\r\n# generate_gradient_grower_fields_report()\r\n\r\n# setup_weather_stations()\r\n\r\n# pickle_file_name = 'weather_stations_2023.pickle'\r\n# # old_pickle_file_name = 'weather_stations_2023 OLD BEFORE CLASS CHANGE.pickle'\r\n# pickle_file_path = 'H:\\\\Shared drives\\\\Stomato\\\\HeatUnits\\\\Pickles\\\\'\r\n# # old_weather_stations = open_pickle(filename=old_pickle_file_name, specific_file_path=pickle_file_path)\r\n# weather_stations = open_pickle(filename=pickle_file_name, specific_file_path=pickle_file_path)\r\n# for station in weather_stations:\r\n#     if station.name == 'Ben Fast':\r\n#         station.id = 'z6-15905'\r\n# write_pickle(weather_stations, filename=pickle_file_name, specific_file_path=pickle_file_path)\r\n# print()\r\n\r\n# varieties = Varieties()\r\n# # varieties.show_all_varieties()\r\n# print(varieties.get_variety_days_to_harvest('1996'))\r\n\r\n# temp_ai_application()\r\n\r\n# cimis = CIMIS()\r\n# cimis_station = CimisStation()\r\n# inactive_cimis_station_list = cimis_station.return_inactive_cimis_stations_list()\r\n# closest_cimis_station = cimis.get_closest_cimis_station(37.8806321, -121.1559947, [])\r\n# print(closest_cimis_station)\r\n\r\n# compare_new_psi_algo_vs_()\r\n# show_pickle()\r\n# only_certain_growers_update(['Riley Chaney Farms'], get_weather=True, get_data=True, write_to_db=True, write_to_portal=True)\r\n# only_certain_growers_field_logger_update('Riley Chaney Farms', 'Riley Chaney Farms3', 'RC-3-NE', write_to_db=True)\r\n\r\n# show_grower('Riley Chaney Farms')\r\n\r\n# growers = open_pickle()\r\n# for grower in growers:\r\n#     if grower.name == 'Riley Chaney Farms':\r\n#         print(grower.name, grower.active)\r\n#         for field in grower.fields:\r\n#             # print('\\t', field.name, 'Active: ', field.active)\r\n#             print(field)\r\n#             for logger in field.loggers:\r\n#                 print('\\t',logger.id, logger.name)\r\n\r\n# growers = open_pickle()\r\n# for grower in growers:\r\n#     if grower.name == 'Riley Chaney Farms':\r\n#         for field in grower.fields:\r\n#             if field.name == 'Riley Chaney Farms3':\r\n#                 for logger in field.loggers:\r\n#                     if logger.id == 'z6-11928':\r\n#                         print(logger.id, logger.password)\r\n#                         logger.password = '12608-28613'\r\n#                         print(logger.id, logger.password)\r\n# write_pickle(growers)\r\n\r\n# new_year_pickle_cleanup()\r\n\r\n# test_switch_cases()\r\n# temp_ai_application()\r\n\r\n\r\n\r\n# setup_ai_game_data(\r\n#     \"2022_pickle.pickle\",\r\n#     \"H:\\\\Shared drives\\\\Stomato\\\\2022\\\\Pickle\\\\\",\r\n#     \"2022_ai_game_data.pickle\",\r\n#     \"H:\\\\Shared drives\\\\Stomato\\\\AIIrrigationGame\\\\\"\r\n# )\r\n\r\n# temp_ai_application()\r\n\r\n# show_pickle()\r\n# growers = open_pickle()\r\n# for grower in growers:\r\n#     print(grower.name)\r\n# print()\r\n# remove_grower('Dwelley Farms')\r\n\r\n# show_pickle()\r\n\r\n# show_pickle()\r\n# cimis_stations_pickle = open_pickle(filename=\"cimisStation.pickle\")\r\n# print()\r\n\r\n# growers = open_pickle()\r\n# for grower in growers:\r\n#     print(grower.name)\r\n\r\n# only_certain_growers_update(['Dwelley Farms'], get_weather=True, get_data=True, write_to_portal=True, write_to_db=True)\r\n# only_certain_growers_field_update('Dwelley Farms', 'Dwelley FarmsTKs', get_weather=True, get_data=True, write_to_portal=True, write_to_db=True)\r\n\r\n\r\n# show_grower('Riley Chaney Farms')\r\n\r\n# growers = open_pickle()\r\n# for grower in growers:\r\n#     if grower.name == 'Riley Chaney Farms':\r\n#         print()\r\n#         for field in grower.fields:\r\n#             if field.name == 'Riley Chaney FarmsRN 2 LLC':\r\n#                 print(field.name, field.nickname)\r\n#                 field.nickname = 'R&N 2 LLC'\r\n#                 print(field.name, field.nickname)\r\n# write_pickle(growers)\r\n\r\n# only_certain_growers_field_update('Barrios Farms', 'Barrios Farms22', get_weather=True, get_data=True, write_to_db=True)\r\n\r\n# growers = open_pickle()\r\n# for grower in growers:\r\n#     if grower.name == 'Barrios Farms':\r\n#         print()\r\n        # for field in grower.fields:\r\n        #     if field.name == 'Riley Chaney FarmsRN 2 LLC':\r\n        #         print(field.name, field.nickname)\r\n        #         field.nickname = 'R&N 2 LLC'\r\n        #         print(field.name, field.nickname)\r\n\r\ndef update_historical_et_stations():\r\n    \"\"\"\r\n    Update the Historical ET table for each station with the new years etos from our BQ\r\n    and recalculate average\r\n    In this iteration going to try\r\n    1. Pull Hist Et Table from BQ -> dict\r\n    2. Pull CIMIS data and add to Hist Et dict\r\n    3. Add new key/column to dict and recalculate average\r\n    4. Write back to the DB\r\n    \"\"\"\r\n    db = DBWriter()\r\n    cimis = CIMIS()\r\n    project = 'stomato-info'\r\n    hist_dataset_id = 'Historical_ET'\r\n    # TODO: some Hist ET tables have different years than others, need to take that in account with the prev_year, maybe a retry on the grab_all_table_data changing the order by each time\r\n    current_year = datetime.now().year\r\n    prev_year = current_year - 1\r\n    start_of_year = date(2023, 1, 1)\r\n    end_of_year = date(2023, 12, 31)\r\n\r\n    hist_et_tables = db.get_tables(hist_dataset_id, project=project)\r\n\r\n    for table in hist_et_tables:\r\n\r\n        station_number = table.table_id\r\n        print(f\"Updating Historical ET for station {station_number}\")\r\n        # Get historical ET table from BigQuery and convert to a dictionary\r\n        # Retry mechanism\r\n        while True:\r\n            try:\r\n                bq_table, table_info = db.grab_all_table_data(hist_dataset_id, station_number, project,\r\n                                                              order_by=f'Year_{prev_year}')\r\n                if bq_table is not None:\r\n                    break  # Exit loop if data is successfully fetched\r\n            except Exception as e:\r\n                print(f\"Error fetching data for Year_{prev_year - 1}: {e}\")\r\n                prev_year -= 1  # Decrement year and retry\r\n            continue\r\n\r\n        # Pull CIMIS data\r\n        et_data = cimis.getDictForStation(station_number, start_of_year, end_of_year)\r\n\r\n        # Add new columns to the dictionary\r\n        for i, row in enumerate(bq_table):\r\n            row[f'Year_{current_year}'] = et_data[\"dates\"][i]\r\n            row[f'Year_{current_year}_ET'] = et_data[\"eto\"][i]\r\n\r\n            # Calculate average if needed\r\n            # row['Average'] = (row.get(f'Year_{prev_year}_ET', 0) + row[f'Year_{current_year}_ET']) / 2\r\n\r\n        column_based_data = {key: [row[key] for row in bq_table] for key in bq_table[0]}\r\n\r\n        averages = cimis.get_average_et(column_based_data)\r\n\r\n        for i, row in enumerate(bq_table):\r\n            row['Average'] = averages[i]\r\n        # Write the modified data back to a CSV file\r\n        filename = f'new_hist_et_{station_number}.csv'\r\n        with open(filename, 'w', newline='') as outfile:\r\n            writer = csv.DictWriter(outfile, fieldnames=bq_table[0].keys())\r\n            writer.writeheader()\r\n            writer.writerows(bq_table)\r\n\r\n        # Update schema to include new columns\r\n        new_schema = table_info.schema + [\r\n            bigquery.SchemaField(f'Year_{current_year}', 'DATE'),\r\n            bigquery.SchemaField(f'Year_{current_year}_ET', 'FLOAT'),\r\n        ]\r\n\r\n        # Write the new table back to BigQuery\r\n        print(\"Writing to table from csv\")\r\n        db.write_to_table_from_csv(hist_dataset_id + '_test', station_number + '_test', filename, new_schema, project)\r\n\r\n    pass\r\n\r\n\r\n\r\n\r\n# show_pickle()\r\n\r\n# growers = open_pickle()\r\n# for grower in growers:\r\n#     for field in grower.fields:\r\n#         for logger in field.loggers:\r\n#             old_opt_low = logger.soil.optimum_lower\r\n#             old_opt_high = logger.soil.optimum_upper\r\n#             new_soil = Soil(field_capacity=logger.soil.field_capacity, wilting_point=logger.soil.wilting_point)\r\n#             logger.soil = new_soil\r\n#             new_soil_low = new_soil.optimum_lower\r\n#             if old_opt_low != new_soil_low:\r\n#                 print(f'Old optimum: {old_opt_low} - {old_opt_high}')\r\n#                 print(f'New optimum: {new_soil_low} - {new_soil.optimum_upper}')\r\n#                 print()\r\n#\r\n# write_pickle(growers)\r\n\r\n\r\n\r\n\r\n# show_pickle()\r\n\r\n\r\n# new_year_pickle_cleanup()\r\n# remove_inactive_fields_from_growers_from_pickle()\r\n\r\n\r\n# growers = open_pickle()\r\n# for grower in growers:\r\n#     for field in grower.fields:\r\n#         for logger in field.loggers:\r\n#             if logger.name == 'LM-17J-S':\r\n#                 logger.show_irrigation_ledger()\r\n#                 for date, switch_list in logger.irrigation_ledger.items():\r\n#                     for ind, switch_val in enumerate(switch_list):\r\n#                         if switch_val == 15:\r\n#                             logger.irrigation_ledger[date][ind] = 60\r\n#                 print()\r\n#                 logger.show_irrigation_ledger()\r\n# write_pickle(growers)\r\n# print()\r\n\r\n\r\n# growers = open_pickle()\r\n# for grower in growers:\r\n#     print(grower)\r\n#\r\n# cimis_stations_pickle = open_pickle(filename=\"cimisStation.pickle\")\r\n#\r\n# pickle_file_name = 'weather_stations_2023.pickle'\r\n# pickle_file_path = 'H:\\\\Shared drives\\\\Stomato\\\\HeatUnits\\\\'\r\n# weather_stations = open_pickle(filename=pickle_file_name, specific_file_path=pickle_file_path)\r\n\r\n\r\n\r\n\r\n\r\n# weather_stations = open_pickle(filename=pickle_file_name, specific_file_path=pickle_file_path)\r\n\r\n# only_certain_growers_field_logger_update('Barrios Farms', 'Barrios Farms25E', 'BF-25E-C', write_to_db=True, subtract_from_mrid=62)\r\n\r\n# dbwriter = DBWriter()\r\n# db_dates = dbwriter.grab_specific_column_table_data('Barrios_Farms25E', 'BF-25E-C', 'stomato-2023', 'date')\r\n# db_dates_list = [row[0] for row in db_dates]\r\n# print()\r\n\r\n# reset_updated_all()\r\n# update_information(get_et=True, write_to_db=True)\r\n\r\n\r\n\r\n# for grower in growers:\r\n    # print()\r\n# for grower in growers:\r\n#     for field in grower.fields:\r\n#         if field.name == 'Lucero Rio VistaB 17-22':\r\n#             for logger in field.loggers:\r\n#                 print(type(logger.daily_switch))\r\n#                 logger.gpm = 4488\r\n#                 logger.irrigation_set_acres = 215\r\n# #     print(logger.name, logger.soil.soil_type)\r\n#             #     logger.soil.set_soil_type('Sandy Clay Loam')\r\n#             #     print(logger.name, logger.soil.soil_type)\r\n# write_pickle(growers)\r\n\r\n# cimis = CIMIS()\r\n# # # cimisStation = CimisStation()\r\n# # # stations = get_all_current_cimis_stations()\r\n# # # stations = cimisStation.return_list_of_stations()\r\n# cimis_stations_pickle = open_pickle(filename=\"cimisStation.pickle\")\r\n# # Convert the integers to strings\r\n# cimis_station_string_list = [str(station.station_number) for station in cimis_stations_pickle]\r\n#\r\n# # Join the strings with commas\r\n# cimis_stations_list_string = ', '.join(cimis_station_string_list)\r\n# print(cimis_stations_list_string)\r\n#\r\n# start_date = str(date.today() - timedelta(3))\r\n# end_date = str(date.today() - timedelta(1))\r\n# all_cimis_station_et = cimis.get_eto(cimis_stations_list_string, start_date, end_date)\r\n# print()\r\n# pprint.pprint(all_cimis_station_et)\r\n\r\n# show_pickle()\r\n\r\n# growers = open_pickle()\r\n# for grower in growers:\r\n#     print(grower.name)\r\n\r\n# Testing new single CIMIS API call\r\n# dict_of_stations = {}\r\n# all_current_cimis_stations = open_pickle(filename=\"cimisStation.pickle\")\r\n# for station in all_current_cimis_stations:\r\n#     dict_of_stations[station.station_number] = {'station': station, 'dates': [], 'eto': []}\r\n# print()\r\n\r\n# ########################################\r\n# ### TESTING CIMIS CALL AT 12\r\n# cimis = CIMIS()\r\n# cimis_stations_pickle = open_pickle(filename=\"cimisStation.pickle\")\r\n# print()\r\n# start_date = str(date.today() - timedelta(1))\r\n# end_date = str(date.today() - timedelta(1))\r\n# for stations in cimis_stations_pickle:\r\n#     stations.updated = False\r\n# all_cimis_station_et = cimis.get_all_stations_et_data(cimis_stations_pickle, start_date, end_date)\r\n# print()\r\n# #########################################\r\n# write_all_et_values_to_db(all_cimis_station_et)\r\n\r\n# print()\r\n\r\n# project = 'stomato-info'\r\n# dataset_id = f\"{project}.ET.105\"\r\n# dataset_id = \"`\" + dataset_id + \"`\"\r\n# #\r\n# dbwriter = DBWriter()\r\n# dml_statement = f\"SELECT * FROM {dataset_id} WHERE date BETWEEN DATE(\\'2023-03-01\\') and DATE(\\'2023-03-25\\') ORDER BY date ASC\"\r\n# result = dbwriter.run_dml(dml_statement, project=project)\r\n#\r\n# date_data = []\r\n# eto_data = []\r\n#\r\n# for row in result:\r\n#     date_data.append(row['date'])\r\n#     eto_data.append(row['eto'])\r\n# print()\r\n\r\n# dml = f\"UPDATE `test.test.test`\" \\\r\n#       + f\" SET daily_switch = 55\"\\\r\n#       + f\", daily_hours = 0.8\"\\\r\n#       + f\", daily_inches = 1.5 WHERE date = 'date'\"\r\n#\r\n# print(dml)\r\n\r\n\r\n# growers = open_pickle()\r\n# for grower in growers:\r\n#     for field in grower.fields:\r\n#         for logger in field.loggers:\r\n#             logger.irrigation_ledger = {}\r\n# write_pickle(growers)\r\n# print()\r\n\r\n# for each_dict in dict_of_stations:\r\n#     print(f\"Station: {dict_of_stations[each_dict]['station'].station_number}\")\r\n# print()\r\n\r\n# my_list = ['pear']\r\n# if 'apple' not in my_list:\r\n#     print(\"apple not in list\")\r\n\r\n# all_cimis_station_et = cimis.get_all_stations_et_data(cimis_stations_pickle, start_date, end_date)\r\n\r\n\r\n# show_pickle()\r\n# temp_ai_application()\r\n# Testing writing a html doc for notifications instead of a txt\r\n# lat = 37.0544503152434\r\n# long = -120.80964223605376\r\n# saulisms = Saulisms()\r\n# saying, saying_date = saulisms.get_random_saulism()\r\n# file_path = \"C:\\\\Users\\\\javie\\\\Desktop\\\\notification_html_test.html\"\r\n# with open(file_path, 'a') as the_file:\r\n#     the_file.write(\"<!DOCTYPE html>\\n\")\r\n#     the_file.write(\"<html>\\n\")\r\n#     the_file.write(\"<body>\\n\")\r\n#     the_file.write(\"<h2>SENSOR ERRORS</h2>\\n\")\r\n#     the_file.write(f\"<h2 style='font-style: italic; font-size:150%;'>\\\"{saying}\\\", {saying_date}</h2>\")\r\n#     the_file.write(\"<h3>=== New Grower ===</h3>\\n\")\r\n#     the_file.write(\"<p>-------------------</p>\\n\")\r\n#     the_file.write(\"<p>Field: Lucero Rio Vista74, 75</p>\\n\")\r\n#     the_file.write(\"<p>Logger: RV-74_75-SE</p>\\n\")\r\n#     the_file.write(\"<p>Logger ID: z6-12306</p>\\n\")\r\n#     the_file.write(\"<p>Date: 06/26/23</p>\\n\")\r\n#     the_file.write(\"<p>Sensor: Canopy Temp</p>\\n\")\r\n#     the_file.write(\"<p>-> Canopy Temp is showing None at some point in the day (Not necessarily at the hottest time). Connection issue?</p>\\n\")\r\n#     the_file.write(f\"<a href='https://www.google.com/maps/search/?api=1&query={lat},{long}' target='_blank'>Location</a>\\n\")\r\n#     the_file.write(\"<p>-------------------</p>\\n\")\r\n#     the_file.write(\"</body>\\n\")\r\n#     the_file.write(\"</html>\\n\")\r\n\r\n# show_pickle()\r\n\r\n# Change Soil Type for a Logger\r\n# growers = open_pickle()\r\n# for grower in growers:\r\n#     for field in grower.fields:\r\n#         for logger in field.loggers:\r\n#             if logger.name == 'RC-24-SW':\r\n#                 print(logger.name, logger.soil.soil_type, logger.soil.field_capacity, logger.soil.wilting_point)\r\n#                 logger.soil.set_soil_type('Clay Loam')\r\n#                 print(logger.name, logger.soil.soil_type, logger.soil.field_capacity, logger.soil.wilting_point)\r\n#                 print('---------------------')\r\n#             if logger.name == 'RN-2LLC-E':\r\n#                 print(logger.name, logger.soil.soil_type, logger.soil.field_capacity, logger.soil.wilting_point)\r\n#                 logger.soil.set_soil_type('Clay Loam')\r\n#                 print(logger.name, logger.soil.soil_type, logger.soil.field_capacity, logger.soil.wilting_point)\r\n#                 print('---------------------')\r\n#             if logger.name == 'RN-2LLC-N':\r\n#                 print(logger.name, logger.soil.soil_type, logger.soil.field_capacity, logger.soil.wilting_point)\r\n#                 logger.soil.set_soil_type('Sandy Clay Loam')\r\n#                 print(logger.name, logger.soil.soil_type, logger.soil.field_capacity, logger.soil.wilting_point)\r\n#                 print('---------------------')\r\n#             if logger.name == 'RN-2LLC-W':\r\n#                 print(logger.name, logger.soil.soil_type, logger.soil.field_capacity, logger.soil.wilting_point)\r\n#                 logger.soil.set_soil_type('Sandy Clay Loam')\r\n#                 print(logger.name, logger.soil.soil_type, logger.soil.field_capacity, logger.soil.wilting_point)\r\n#                 print('---------------------')\r\n#             if logger.name == 'RC-16-E':\r\n#                 print(logger.name, logger.soil.soil_type, logger.soil.field_capacity, logger.soil.wilting_point)\r\n#                 logger.soil.set_soil_type('Clay Loam')\r\n#                 print(logger.name, logger.soil.soil_type, logger.soil.field_capacity, logger.soil.wilting_point)\r\n#                 print('---------------------')\r\n#             if logger.name == 'RC-13-E':\r\n#                 print(logger.name, logger.soil.soil_type, logger.soil.field_capacity, logger.soil.wilting_point)\r\n#                 logger.soil.set_soil_type('Clay Loam')\r\n#                 print(logger.name, logger.soil.soil_type, logger.soil.field_capacity, logger.soil.wilting_point)\r\n#                 print('---------------------')\r\n#             if logger.name == 'RC-4E-C':\r\n#                 print(logger.name, logger.soil.soil_type, logger.soil.field_capacity, logger.soil.wilting_point)\r\n#                 logger.soil.set_soil_type('Clay Loam')\r\n#                 print(logger.name, logger.soil.soil_type, logger.soil.field_capacity, logger.soil.wilting_point)\r\n#                 print('---------------------')\r\n#             if logger.name == 'RC-3-NE':\r\n#                 print(logger.name, logger.soil.soil_type, logger.soil.field_capacity, logger.soil.wilting_point)\r\n#                 logger.soil.set_soil_type('Sandy Loam')\r\n#                 print(logger.name, logger.soil.soil_type, logger.soil.field_capacity, logger.soil.wilting_point)\r\n#                 print('---------------------')\r\n#\r\n#\r\n# write_pickle(growers)\r\n\r\n# # Testing some notification generation and writing\r\n# growers = open_pickle()\r\n# techs = get_all_technicians(growers)\r\n# notifications_setup(growers, techs, file_type='html')\r\n# reset_updated_all()\r\n# update_information(get_data=True, check_for_notifications=True)\r\n\r\n# only_certain_growers_field_logger_update('Saul', 'Meza', 'Development-C', check_for_notifications=True, subtract_from_mrid=24)\r\n\r\n# growers = open_pickle()\r\n# for grower in growers:\r\n#     for field in grower.fields:\r\n#         for logger in field.loggers:\r\n#             print()\r\n#             print(f\"https://www.google.com/maps/search/?api=1&query={logger.lat},{logger.long}\")\r\n#             print()\r\n#         if field.crop_type not in ['Tomatoes', 'tomatoes', 'tomato', 'Tomato']:\r\n#             print(field.name, field.crop_type)\r\n#         if field.loggers[0].rnd:\r\n#             field.field_type = 'R&D'\r\n#         else:\r\n#             field.field_type = 'Commercial'\r\n# print(f'{field.name} - {field.field_type}')\r\n# write_pickle(growers)\r\n\r\n# print(field.name)\r\n# for logger in field.loggers:\r\n#     print(f'\\t{logger.name} - {logger.rnd}')\r\n\r\n# df = pd.DataFrame(columns=['Gradient Grower', 'Gradient Grower-Field', 'Gradient Field', 'Gradient Planting Date', 'Gradient Uninstall Date'])\r\n#\r\n# pickle_file_name = \"2022_pickle.pickle\"\r\n# pickle_file_path = \"H:\\\\Shared drives\\\\Stomato\\\\2022\\\\Pickle\\\\\"\r\n# growers = open_pickle(filename=pickle_file_name, specific_file_path=pickle_file_path)\r\n# for grower in growers:\r\n#     for field in grower.fields:\r\n#         if field.crop_type in ['Tomatoes', 'tomatoes', 'tomato', 'Tomato']:\r\n#             df = df.append({'Gradient Grower': grower.name, 'Gradient Grower-Field': field.name, 'Gradient Field': field.nickname, 'Gradient Planting Date': field.loggers[0].planting_date, 'Gradient Uninstall Date': field.loggers[0].uninstall_date}, ignore_index=True)\r\n#\r\n# print()\r\n# print(df)\r\n# df.to_excel('growers_fields.xlsx', index=False)\r\n\r\n# pickle_file_name = \"ai_game_2023_data_errors.pickle\"\r\n# pickle_file_path = \"C:\\\\Users\\\\javie\\\\Desktop\\\\AI Game v3 Distribution\\\\\"\r\n# ai_data = open_pickle(filename=pickle_file_name, specific_file_path=pickle_file_path)\r\n# print()\r\n# ai_data = []\r\n#\r\n# write_pickle(ai_data, filename=pickle_file_name, specific_file_path=pickle_file_path)\r\n\r\n\r\n# pickle_file_name = \"2022_pickle.pickle\"\r\n# pickle_file_path = \"H:\\\\Shared drives\\\\Stomato\\\\2022\\\\Pickle\\\\\"\r\n# growers = open_pickle(filename=pickle_file_name, specific_file_path=pickle_file_path)\r\n# for grower in growers:\r\n#     for field in grower.fields:\r\n#         if field.loggers[0].rnd:\r\n#             field.field_type = 'R&D'\r\n#         else:\r\n#             field.field_type = 'Commercial'\r\n#         print(f'{field.name} - {field.field_type}')\r\n# write_pickle(growers, filename=pickle_file_name, specific_file_path=pickle_file_path)\r\n\r\n# pickle_file_name = \"2022_pickle.pickle\"\r\n# pickle_file_path = \"H:\\\\Shared drives\\\\Stomato\\\\2022\\\\Pickle\\\\\"\r\n# graph_yields(pickle_file_name, pickle_file_path, 'Paid')\r\n\r\n# pickle_file_name = \"2022_pickle.pickle\"\r\n# pickle_file_path = \"H:\\\\Shared drives\\\\Stomato\\\\2022\\\\Pickle\\\\\"\r\n# get_tomato_yield_data(pickle_file_name, pickle_file_path, '2022 Tomato Yield.xlsx', 0, 263)\r\n# show_specific_pickle(pickle_file_name, specific_file_path=pickle_file_path)\r\n\r\n# pickle_file_name = \"2022_pickle.pickle\"\r\n# pickle_file_path = \"H:\\\\Shared drives\\\\Stomato\\\\2022\\\\Pickle\\\\\"\r\n# growers = open_specific_pickle(pickle_file_name, specific_file_path=pickle_file_path)\r\n# for grower in growers:\r\n#     for field in grower.fields:\r\n#         print(field.name, field.loggers[0].uninstall_date)\r\n#         field.crop_type = field.loggers[0].cropType\r\n#         field.net_yield = None\r\n#         field.paid_yield = None\r\n# write_specific_pickle(growers, pickle_file_name, specific_file_path=pickle_file_path)\r\n# show_specific_pickle(pickle_file_name, specific_file_path=pickle_file_path)\r\n\r\n# pickle_file_name = \"2022_pickle.pickle\"\r\n# pickle_file_path = \"H:\\\\Shared drives\\\\Stomato\\\\2022\\\\Pickle\\\\\"\r\n# growers = open_specific_pickle(pickle_file_name, specific_file_path=pickle_file_path)\r\n# for grower in growers:\r\n#     for field in grower.fields:\r\n#         for logger in field.loggers:\r\n#             if not hasattr(logger, 'rnd'):\r\n#                 logger.crop_type = logger.cropType\r\n#                 print(logger.grower.name, logger.field.name, logger.name, logger.crop_type)\r\n#                 logger.rnd = False\r\n#                 if logger.name == 'Development-C':\r\n#                     print(logger.grower.name, logger.field.name, logger.name, logger.crop_type)\r\n#                     logger.rnd = True\r\n#                 # logger.rnd = True\r\n# write_specific_pickle(growers, pickle_file_name, specific_file_path=pickle_file_path)\r\n# print(datetime.now().date())\r\n\r\n# growers = open_pickle()\r\n# for grower in growers:\r\n#     for field in grower.fields:\r\n#         for logger in field.loggers:\r\n#             if logger.name == 'MB-52-NE':\r\n#                 logger.consecutive_ir_values = deque()\r\n#                 logger.consecutive_ir_values.append((8.212098906263984, 26.622))\r\n#             elif logger.name == 'MB-52-SW':\r\n#                 logger.consecutive_ir_values = deque()\r\n#                 logger.consecutive_ir_values.append((4.40852468436712, 10.89))\r\n# write_pickle(growers)\r\n\r\n# show_pickle()\r\n\r\n# gpm = '1,200'\r\n# print(float(gpm))\r\n\r\n\r\n# def clean_list(list, remove_start, remove_end):\r\n#     clean = list[remove_start:len(list) - remove_end]\r\n#     print(clean)\r\n#\r\n# list = [1,2,3,4,5,6,7,8,9,10]\r\n# clean_list(list, 0, 0)\r\n\r\n# show_pickle()\r\n\r\n# only_certain_growers_field_logger_update('Matteoli Brothers', 'Matteoli Brothers50', 'MB-50-C', write_to_db=True)\r\n\r\n# growers = open_pickle()\r\n# for grower in growers:\r\n#     for field in grower.fields:\r\n#         if field.name == 'Schreiner115':\r\n#             for logger in field.loggers:\r\n#                 print(logger.soil.soil_type)\r\n#                 print(logger.soil.field_capacity)\r\n#                 print(logger.soil.wilting_point)\r\n#                 logger.soil.set_soil_type('Silt Loam')\r\n#                 print(logger.soil.soil_type)\r\n#                 print(logger.soil.field_capacity)\r\n#                 print(logger.soil.wilting_point)\r\n# write_pickle(growers)\r\n# logger.to_string()\r\n# logger.ir_active = True\r\n# logger.to_string()\r\n# write_pickle(growers)\r\n# pickle_file_name = \"2022_pickle.pickle\"\r\n# pickle_file_path = \"H:\\\\Shared drives\\\\Stomato\\\\2022\\\\Pickle\\\\\"\r\n# field_capacities = []\r\n# field_capacities_data = []\r\n# growers = open_specific_pickle(pickle_file_name, specific_file_path=pickle_file_path)\r\n# for grower in growers:\r\n#     for field in grower.fields:\r\n#         for logger in field.loggers:\r\n#             logger.field_capacity = logger.fieldCapacity\r\n#             if logger.field_capacity not in field_capacities:\r\n#                 percent = 0.15\r\n#                 fc_upper = round(logger.field_capacity + (logger.field_capacity * percent), 1)\r\n#                 fc_lower = round(logger.field_capacity - (logger.field_capacity * percent), 1)\r\n#                 field_capacities.append(logger.field_capacity)\r\n#                 field_capacities_data.append((logger.field_capacity, fc_upper, fc_lower))\r\n#\r\n# for fc_data in field_capacities_data:\r\n#     fc, fc_u, fc_l = fc_data\r\n#     print(f'Upper: {fc_u}')\r\n#     print(f'FC: {fc}')\r\n#     print(f'Lower: {fc_l}')\r\n#     print()\r\n# print(field_capacities)\r\n\r\n# all_acres = []\r\n# total_acres = 0\r\n# number_of_fields = 0\r\n# price_per_station = []\r\n# growers = open_pickle()\r\n# for grower in growers:\r\n#     for field in grower.fields:\r\n#         if field.crop_type in ['Tomatoes', 'Tomato', 'tomatoes', 'tomato']:\r\n#             if not field.loggers[0].rnd:\r\n#                 # print(type(field.acres))\r\n#                 total_acres += float(field.acres)\r\n#                 number_of_fields += 1\r\n#                 price_per_station.append(50 * float(field.acres) / len(field.loggers))\r\n#\r\n# # print(total_acres/number_of_fields)\r\n# print(price_per_station)\r\n# print(number_of_fields)\r\n#\r\n# print(sum(price_per_station)/len(price_per_station))\r\n\r\n\r\n# pickle_file_name = \"2022_pickle.pickle\"\r\n# pickle_file_path = \"H:\\\\Shared drives\\\\Stomato\\\\2022\\\\Pickle\\\\\"\r\n# # field_capacities = []\r\n# # field_capacities_data = []\r\n# growers = open_specific_pickle(pickle_file_name, specific_file_path=pickle_file_path)\r\n# all_acres = []\r\n# total_acres = 0\r\n# number_of_fields = 0\r\n# price_per_station = []\r\n# # growers = open_pickle()\r\n# for grower in growers:\r\n#     for field in grower.fields:\r\n#         if field.crop_type in ['Tomatoes', 'Tomato', 'tomatoes', 'tomato']:\r\n#             if not field.loggers[0].rnd:\r\n#                 total_acres += float(field.acres)\r\n#                 number_of_fields += 1\r\n#                 price_per_station.append(50 * float(field.acres) / len(field.loggers))\r\n#\r\n# # print(total_acres/number_of_fields)\r\n# print(price_per_station)\r\n# total_price = 0\r\n# for station in price_per_station:\r\n#     total_price += station\r\n# print(total_price/len(price_per_station))\r\n# growers = open_pickle()\r\n# for grower in growers:\r\n#     for field in grower.fields:\r\n#         for logger in field.loggers:\r\n#             st = Soil(field_capacity=logger.soil.field_capacity, wilting_point=logger.soil.wilting_point)\r\n#             logger.soil = st\r\n#\r\n#             print(field.name, logger.name)\r\n#             print(f'Soil type: {logger.soil.soil_type}')\r\n#             print(f'Ranges: {logger.soil.bounds}')\r\n# write_pickle(growers)\r\n# print(logger.field.name, logger.name, logger.field_capacity, logger.wilting_point)\r\n\r\n# pickle_file_name = \"2022_pickle.pickle\"\r\n# pickle_file_path = \"H:\\\\Shared drives\\\\Stomato\\\\2022\\\\Pickle\\\\\"\r\n# growers = open_pickle(filename=pickle_file_name, specific_file_path=pickle_file_path)\r\n# for grower in growers:\r\n#     for field in grower.fields:\r\n#         for logger in field.loggers:\r\n#             # logger.field_capacity = logger.fieldCapacity\r\n#             # logger.wilting_point = logger.wiltingPoint\r\n#             st = Soil(field_capacity=logger.soil.field_capacity, wilting_point=logger.soil.wilting_point)\r\n#             logger.soil = st\r\n# #\r\n#             print(field.name, logger.name)\r\n#             print(f'Soil type: {logger.soil.soil_type}')\r\n#             print(f'Ranges: {logger.soil.bounds}')\r\n# #             print()\r\n# write_pickle(growers, filename=pickle_file_name, specific_file_path=pickle_file_path)\r\n\r\n# growers = open_pickle()\r\n# for grower in growers:\r\n#     for field in grower.fields:\r\n#         for logger in field.loggers:\r\n#             soil_type = logger.soil.soil_type\r\n#             new_soil = Soil(soil_type=soil_type)\r\n#             logger.soil = new_soil\r\n# write_pickle(growers)\r\n\r\n# show_pickle()\r\n\r\n# pickle_file_name = \"2022_pickle.pickle\"\r\n# pickle_file_path = \"H:\\\\Shared drives\\\\Stomato\\\\2022\\\\Pickle\\\\\"\r\n# growers = open_specific_pickle(pickle_file_name, specific_file_path=pickle_file_path)\r\n# total_2022_fields = 0\r\n# total_2022_loggers = 0\r\n# for grower in growers:\r\n#     for field in grower.fields:\r\n#         total_2022_fields += 1\r\n#         for logger in field.loggers:\r\n#             total_2022_loggers += 1\r\n# print(f'Field #: {total_2022_fields}')\r\n# print(f'Logger #: {total_2022_loggers}')\r\n\r\n# pickle_file_name = \"2023_pickle.pickle\"\r\n# pickle_file_path = \"H:\\\\Shared drives\\\\Stomato\\\\2023\\\\Pickle\\\\\"\r\n# pickle_file_name = \"entry.pickle\"\r\n# pickle_file_path = \"H:\\\\Shared drives\\\\Stomato\\\\2021\\\\Pickle\\\\\"\r\n# pickle_file_name = \"2022_pickle.pickle\"\r\n# pickle_file_path = \"H:\\\\Shared drives\\\\Stomato\\\\2022\\\\Pickle\\\\\"\r\n# growers = open_specific_pickle(pickle_file_name, specific_file_path=pickle_file_path)\r\n# for grower in growers:\r\n#     for field in grower.fields:\r\n#         for logger in field.loggers:\r\n#             logger.install_date = None\r\n#             logger.broken = False\r\n#         field.crop_type = field.loggers[0].crop_type\r\n#         field.net_yield = None\r\n#         field.paid_yield = None\r\n# print('Done')\r\n# write_specific_pickle(growers, pickle_file_name, specific_file_path=pickle_file_path)\r\n# show_specific_pickle(pickle_file_name, specific_file_path=pickle_file_path)\r\n# show_pickle()\r\n# pickle_file_name = \"entry.pickle\"\r\n# pickle_file_path = \"H:\\\\Shared drives\\\\Stomato\\\\2021\\\\Pickle\\\\\"\r\n# info = {}\r\n# growers = open_specific_pickle(pickle_file_name, specific_file_path=pickle_file_path)\r\n# for grower in growers:\r\n#     for field in grower.fields:\r\n#         for logger in field.loggers:\r\n#             if 'z6' in logger.id:\r\n#                 if logger.id not in info:\r\n#                     info[logger.id] = []\r\n#                 info[logger.id].append(logger.field.name + ' -> ' + logger.name)\r\n# pprint.pprint(info)\r\n# write_specific_pickle(growers, pickle_file_name, specific_file_path=pickle_file_path)\r\n# show_specific_pickle(pickle_file_name, specific_file_path=pickle_file_path)\r\n\r\n\r\n# turn_ai_game_data_into_csv(\"ai_game_data2.pickle\", \"G:\\\\My Drive\\\\S-TOMAto\\\\2021\\\\Pickle\\\\\", 'ai_game_data_2022')\r\n# grower_pickle_file_name = \"2022_data.pickle\"\r\n# grower_pickle_file_path = \"H:\\\\Shared drives\\\\Stomato\\\\2022\\\\Pickle\\\\\"\r\n# growers = open_specific_pickle(grower_pickle_file_name, specific_file_path=grower_pickle_file_path)\r\n# print()\r\n# setup_ai_game_data(\r\n#     \"2022_pickle.pickle\",\r\n#     \"H:\\\\Shared drives\\\\Stomato\\\\2022\\\\Pickle\\\\\",\r\n#     \"2022_data.pickle\",\r\n#     \"H:\\\\Shared drives\\\\Stomato\\\\2023\\\\Pickle\\\\\"\r\n# )\r\n\r\n\r\n# growers = open_specific_pickle('2022_pickle.pickle', 'H:\\\\Shared drives\\\\Stomato\\\\2022\\\\Pickle\\\\')\r\n# print(growers)\r\n\r\n# growers = open_specific_pickle('entry.pickle', 'H:\\\\Shared drives\\\\Stomato\\\\2021\\\\Pickle\\\\')\r\n# num_of_fields = 0\r\n# for grower in growers:\r\n#     for field in grower.fields:\r\n#         num_of_fields += 1\r\n# print(num_of_fields)\r\n#     grower.to_string()\r\n# print()\r\n\r\n# show_pickle()\r\n\r\n# growers = open_pickle()\r\n# for grower in growers:\r\n#     tech = grower.technician\r\n#     print(tech.name)\r\n#     if tech.name == 'Vanessa' and len(tech.email) == 1:\r\n#         tech.email.append('gjazo@morningstarco.com')\r\n#     elif tech.name == 'Exsaelth' and len(tech.email) == 2:\r\n#         tech.email.append('jjimenez@morningstarco.com')\r\n#     print(tech.email)\r\n# write_pickle(growers)\r\n#     if isinstance(tech.email, str):\r\n#         tech.email = tech.email.split('; ')\r\n#     print(tech.email)\r\n# write_pickle(growers)\r\n# if tech.name == 'Development Test Tech':\r\n#     print(tech.email)\r\n#     tech.email = tech.email.split('; ')\r\n#     print(tech.email)\r\n\r\n\r\n#         grower.to_string()\r\n# recipients = ['jgarrido@morningstarco.com','jsalcedo@morningstarco.com']\r\n# print(', '.join(recipients))\r\n\r\n\r\n# dbw = DBWriter()\r\n# dbw.grab_bq_client(my_project='')\r\n# print()\r\n# cleanup_cimis_stations_pickle()\r\n# test_switch_cases()\r\n\r\n# update_information(get_data=True, write_to_db=True, write_to_portal=True)\r\n# show_pickle()\r\n# only_certain_growers_update(['Bullseye'])\r\n# only_certain_growers_field_update('Bullseye Farms', 'Bullseye FarmsYO2E', get_data=True, write_to_db=True, write_to_portal=True)\r\n# cimis_pickle = open_specific_pickle(\"cimisStation.pickle\")\r\n# print()\r\n\r\n# yesterdayRaw = date.today() - timedelta(1)\r\n# cimis = CIMIS()\r\n# print(cimis.get_list_of_active_eto_stations())\r\n# cimis.get_eto('169', str(yesterdayRaw), str(yesterdayRaw))\r\n\r\n# stations = {}\r\n# show_pickle()\r\n# dbw = DBWriter()\r\n# cwsi = CwsiProcessor()\r\n\r\n# growers = open_pickle()\r\n# for grower in growers:\r\n#     for field in grower.fields:\r\n#         for logger in field.loggers:\r\n#             if logger.ports:\r\n#                 for port in logger.ports:\r\n#                     sensor_type = logger.sensor_name(logger.ports[port])\r\n#                     logger.ports[port] = sensor_type\r\n#                 print(logger.ports)\r\n#             # ports[ind[\"port\"]] = ind[\"sensor_number\"]\r\n#             # ports[ind[\"port\"]] = sensor_type\r\n# write_pickle(growers)\r\n\r\n# show_pickle()\r\n# show_grower('Saul')\r\n\r\n# only_certain_growers_field_logger_update('Saul', 'Meza', 'Development-C', subtract_from_mrid=24)\r\n# only_certain_growers_field_update('Saul', 'Meza', get_weather=True)\r\n\r\n# weather = WeatherProcessor(37.052433, -120.811616)\r\n# weather_data = weather.time_machine_forecast(weather.lat, weather.long, datetime(2021, 1, 1), datetime(2021, 1, 10))\r\n# def print_pretty_weather_data(weather_data):\r\n#     print(\"Weather forecast:\\n\")\r\n#     for data in weather_data:\r\n#         date = data['datetime']\r\n#         temp_min = data['tempmin']\r\n#         temp_max = data['tempmax']\r\n#         conditions = data['conditions']\r\n#\r\n#         print(f\"{date}:\")\r\n#         print(f\"  Temperature: {temp_min}°F - {temp_max}°F\")\r\n#         print(f\"  Conditions: {conditions}\")\r\n#         print()\r\n# print_pretty_weather_data(weather_data['days'])\r\n\r\n# show_pickle()\r\n\r\n# print((logger.ports))\r\n#\r\n#             # print(f'GPM: {logger.gpm}')\r\n#             if not hasattr(logger, 'rnd'):\r\n#                 print(f'Logger: {logger.name}')\r\n#                 logger.rnd = False\r\n#             # print(f'R&D: {logger.rnd}')\r\n#             # logger.to_string()\r\n# write_pickle(growers)\r\n# show_pickle()\r\n# hi = 'LAKJ:LAKJ:LKJ:LKJA'\r\n# print(f'hi: {hi:5}')\r\n# growers = open_pickle()\r\n# for grower in growers:\r\n#     print('************************GROWER***********************')\r\n#     pprint(vars(grower))\r\n#     print('***********************************************')\r\n#     for field in grower.fields:\r\n#         if hasattr(field, 'cimis'):\r\n#             delattr(field, 'cimis')\r\n#         print('===================FIELD=========================')\r\n#         pprint(vars(field))\r\n#         print('============================================')\r\n#         for logger in field.loggers:\r\n#             # logger.irrigation_set_acres = logger.acres\r\n#             if hasattr(logger, 'filename'):\r\n#                 delattr(logger, 'filename')\r\n#             if hasattr(logger, 'filepath'):\r\n#                 delattr(logger, 'filepath')\r\n#             print('--------------------LOGGER----------------------')\r\n#             pprint(vars(logger))\r\n#             print('------------------------------------------')\r\n# if hasattr(field, 'failed_cimis_update'):\r\n#     del field.failed_cimis_update\r\n# write_pickle(growers)\r\n# show_pickle()\r\n#         print(f'')\r\n#\r\n# reset_updated_all()\r\n# update_information(get_weather=True, write_to_db=True)\r\n# dbw = DBWriter()\r\n# cwsi = CwsiProcessor()\r\n# growers = open_pickle()\r\n# for grower in growers:\r\n#     for field in grower.fields:\r\n#         print(type(field.acres))\r\n#         print()\r\n#         if field.name == 'Bone Farms LLCF7' or field.name == 'Bone Farms LLCN42 N43':\r\n#             for logger in field.loggers:\r\n#                 result = dbw.add_up_whole_column('gdd', logger)\r\n#                 print(f'Name {logger.name}')\r\n#                 print(f'Gdd total: {result}')\r\n#                 logger.gdd_total = result\r\n#                 crop_stage = cwsi.get_crop_stage(result)\r\n#                 logger.crop_stage = crop_stage\r\n#                 print(f'Crop Stage: {crop_stage}')\r\n# write_pickle(growers)\r\n# print(logger.to_string())\r\n#             logger.crop_stage = 'NA'\r\n# write_pickle(growers)\r\n#         # field.cimis_station = field.cimisStation\r\n#         # if field.name == 'Bone Farms LLCF7':\r\n#         #     print()\r\n#         print(field.name, len(vars(field)))\r\n#         print()\r\n# print(field.name)\r\n# print()\r\n#         if field.cimisStation not in stations:\r\n#             stations[field.cimisStation] = 1\r\n#         else:\r\n#             stations[field.cimisStation] += 1\r\n# print(stations)\r\n\r\n# growers = open_pickle()\r\n# for grower in growers:\r\n#     for field in grower.fields:\r\n#         print(f'Field: {field.name} - Station: {field.cimisStation}')\r\n\r\n\r\n# onlyCertainGrowersUpdate(['KTN JV'], get_weather=True, write_to_db=True)\r\n# show_pickle()\r\n# write_pickle(growers)\r\n# show_pickle()\r\n# only_certain_growers_field_update('Bullseye Farms', 'Bullseye FarmsYO2E', write_to_portal=True, get_data=True)\r\n# remove_grower('Jesus')\r\n# growers = open_pickle()\r\n# for grower in growers:\r\n#     if hasattr(grower, 'portalGSheetURL'):\r\n#         del grower.portalGSheetURL\r\n#     grower.email = ''\r\n# write_pickle(growers)\r\n#     print(grower.name, 'Email: ', grower.email)\r\n# for field in grower.fields:\r\n#     if hasattr(field, 'gSheetEtName'):\r\n#         del field.gSheetEtName\r\n#     if hasattr(field, 'gSheetIrrigationSchedulingName'):\r\n#         del field.gSheetIrrigationSchedulingName\r\n#     if hasattr(field, 'gSheetUrl'):\r\n#         del field.gSheetUrl\r\n#     if hasattr(field, 'gSheetWeatherIconName'):\r\n#         del field.gSheetWeatherIconName\r\n#     if hasattr(field, 'gSheetWeatherName'):\r\n#         del field.gSheetWeatherName\r\n#     if hasattr(field, 'portalGSheetName'):\r\n#         del field.portalGSheetName\r\n#     print()\r\n# for logger in field.loggers:\r\n#     if hasattr(logger, 'almondkc'):\r\n#         del logger.almondkc\r\n#     if hasattr(logger, 'pepperkc'):\r\n#         del logger.pepperkc\r\n#     if hasattr(logger, 'pistachiokc'):\r\n#         del logger.pistachiokc\r\n#     if hasattr(logger, 'tomatokc'):\r\n#         del logger.tomatokc\r\n#     if hasattr(logger, 'gsheetid'):\r\n#         del logger.gsheetid\r\n# write_pickle(growers)\r\n#         for logger in field.loggers:\r\n#             logger.logger_direction = logger.loggerDirection\r\n#             del logger.loggerDirection\r\n# logger.crop_type = logger.cropType\r\n# del logger.cropType\r\n# logger.field_capacity = logger.fieldCapacity\r\n# del logger.fieldCapacity\r\n# logger.wilting_point = logger.wiltingPoint\r\n# del logger.wiltingPoint\r\n\r\n# write_pickle(growers)\r\n# check_technician_clones()\r\n#         field.weather_crashed = False\r\n# remove_field()\r\n# update_information(get_weather=True, write_to_db=True)\r\n# growers = open_pickle()\r\n# for grower in growers:\r\n#     new_fields = []\r\n#     print('Old Fields')\r\n#     for field in grower.fields:\r\n#         print(field.name, ' - ', field.active)\r\n#         if field.active:\r\n#             new_fields.append(field)\r\n#     print()\r\n#     print('New Fields')\r\n#     for f in new_fields:\r\n#         print(f.name, ' - ', f.active)\r\n#     print()\r\n#     grower.fields = new_fields\r\n# write_pickle(growers)\r\n\r\n\r\n# new_year_pickle_cleanup()\r\n# deactivate_grower('Javier')\r\n#     has_active_fields = False\r\n#     for field in grower.fields:\r\n#         if field.active:\r\n#             has_active_fields = True\r\n#     if has_active_fields:\r\n#         new_2023_growers.append(grower)\r\n#         # print(field.name, ' - ', field.active)\r\n# write_pickle(new_2023_growers)\r\n# print('Active Growers:')\r\n# for grower in new_2023_growers:\r\n#     print(grower.name)\r\n\r\n# for data in dict1:\r\n#     print(f'Length for {data} = {len(dict1[data])}')\r\n# print()\r\n# for data in dict2:\r\n#     print(f'Length for {data} = {len(dict2[data])}')\r\n\r\n\r\n# for data in multiple_days:\r\n#     print(f'Length for {data} = {len(multiple_days[data])}')\r\n\r\n# setup_ai_game_data()\r\n# cwsi = CwsiProcessor()\r\n# print('PSI ver1: ', cwsi.get_old_tomatoes_cwsi(-12.6,29.9,4.2,97.3,True))\r\n# print('PSI ver2: ', cwsi.get_old_tomatoes_cwsi_2(-12.6,29.9,4.2,97.3,True))\r\n# writeUninstallationProgresstoDB()\r\n\r\n# write_pickle(growers)\r\n# onlyCertainGrowersFieldUpdate('Bullseye Farms', 'Bullseye FarmsYO2W', get_data=True)\r\n# show_pickle()\r\n# onlyCertainGrowersFieldUpdate('Andrew', 'Andrew3107', get_weather=True, write_to_db=True)\r\n# reset_updated_all()\r\n# update_information(get_weather=True, write_to_db=True)\r\n# show_pickle()\r\n# growers = open_pickle()\r\n# for grower in growers:\r\n#     if grower.active:\r\n#         for field in grower.fields:\r\n#             if field.active:\r\n#                 print(field.grower.name)\r\n#                 print(field.name)\r\n#                 print()\r\n# update_information(get_data=True, write_to_portal=True)\r\n\r\n# today = datetime.today()\r\n# print((today + timedelta(days=60)).month)\r\n# growers = open_pickle()\r\n# for g in growers:\r\n#     for f in g.fields:\r\n#         for l in f.loggers:\r\n#             print(l.name, l.model)\r\n\r\n# url = \"https://api.openweathermap.org/data/3.0/onecall?lat=37.053841&lon=-120.809556&appid=23e1e01a3d66fbd41154957a11dbe696\"\r\n#\r\n# headers = {\"accept\": \"application/json\"}\r\n# response = requests.get(url, headers=headers)\r\n# print(response.text)\r\n\r\n\r\n# ///////////////////////\r\n# api_key = \"91abc03a7e1748b767c16fccd701c7ec\"\r\n# lat = \"37.053841\"\r\n# lon = \"-120.809556\"\r\n# url = \"https://api.openweathermap.org/data/2.5/onecall?lat=%s&lon=%s&appid=%s&units=imperial&exclude=minutely,hourly\" % (lat, lon, api_key)\r\n#\r\n# response = requests.get(url)\r\n# data = json.loads(response.text)\r\n#\r\n# daily = data[\"daily\"]\r\n# for entry in daily:\r\n#     time = datetime.utcfromtimestamp(entry[\"dt\"])\r\n#     # time = datetime.fromtimestamp(entry[\"dt\"])\r\n#     date_string_format = time.strftime(\"%Y-%m-%d\")\r\n#     date_day = time.strftime('%a')\r\n#     maxTemp = entry[\"temp\"][\"max\"]\r\n#     humidity = entry[\"humidity\"]\r\n#     tempC = (maxTemp - 32) * 5.0 / 9.0\r\n#     saturation_vapor_pressure = 610.7 * 10 ** ((7.5 * tempC) / (237.7 + tempC))\r\n#     vpd = (((1 - humidity/100) * saturation_vapor_pressure) * 0.001)\r\n#     vpd = round(vpd, 1)\r\n#     icon = entry[\"weather\"][0][\"main\"]\r\n#     print(date_string_format, date_day, maxTemp, humidity, vpd, icon)\r\n# print(data)\r\n# ///////////////////////////////\r\n\r\n# show_pickle()\r\n# reset_updated_all()\r\n# update_information(get_weather=True, get_data=True)\r\n# onlyCertainGrowersUpdate(['Saul'], get_weather=True, get_data=True, write_to_db=True, write_to_portal=True)\r\n# only_certain_growers_field_logger_update('Saul', 'Meza', 'Development-C', subtract_from_mrid=48)\r\n\r\n# show_pickle()\r\n# growers = open_pickle()\r\n# for grower in growers:\r\n#     for field in grower.fields:\r\n\r\n#         if field.name == 'Andrew3101A':\r\n#             for logger in field.loggers:\r\n#                 print('HI')\r\n#                 logger.field_capacity = 18\r\n#                 logger.wilting_point = 8\r\n# write_pickle(growers)\r\n\r\n# show_pickle()\r\n\r\n# growers = open_pickle()\r\n# for grower in growers:\r\n#     for field in grower.fields:\r\n#         if field.name == 'RG Farms LLC2':\r\n#             print()\r\n#             print(field.acres)\r\n#             field.acres = 30.35\r\n#             print(field.acres)\r\n# write_pickle(growers)\r\n\r\n# growers = open_pickle()\r\n# for grower in growers:\r\n#     for field in grower.fields:\r\n#         for logger in field.loggers:\r\n#             # print(f'Grower: {logger.grower.name} - Field: {logger.field.name}')\r\n#             # print(f'Logger name: {logger.name}')\r\n#             print(f'Logger ports: {logger.ports}')\r\n# print(f'Sensor data: {logger.get_sensor_index_data()}')\r\n# print(f'Logger keys: {logger.ports.keys()}')\r\n# print(f'Logger values: {logger.ports.values()}')\r\n# for key, value in sorted(logger.ports):\r\n#     print(f'Key: {key} - Value: {value}')\r\n#             irs = 0\r\n#             for ind, port in enumerate(logger.ports):\r\n# #                 port_sensor = logger.sensor_name(logger.ports[port])\r\n# #                 logger.ports[port] = port_sensor\r\n#                 print(f'Port: {port} - Sensor: {logger.ports[port]}')\r\n#                 if logger.ports[port] == 'Infra Red':\r\n#                     irs += 1\r\n#             print(f'IRs: {irs}')\r\n#             if irs > 1:\r\n#                 break\r\n#             print(f'Logger ports: {logger.ports}')\r\n# write_pickle(growers)\r\n# print()\r\n\r\n\r\n# setup_grower('Javier Test', 'a', 'a', 'North', True)\r\n\r\n# show_pickle()\r\n#         if field.active:\r\n#             print(field.name)\r\n# print(field.loggers[0].crop)\r\n# show_pickle()\r\n# ///////////////////////////////\r\n# METER API v1 Testing\r\n# GET READINGS\r\n# import requests\r\n# import json\r\n# import pprint\r\n# import pandas as pd\r\n# device_sn = \"z6-16138\"\r\n# device_pw = '65607-66874'\r\n# token = \"Token {TOKEN}\".format(TOKEN=\"6d1f65835ec6fd0c7ec77266d6a74d7f5a7be6b1\")\r\n# url = \"https://zentracloud.com/api/v1/readings\"\r\n# headers = {'content-type': 'application/json', 'Authorization': token}\r\n# output_format = \"json\"\r\n#\r\n# params = {'user': 'jgarrido@morningstarco.com', 'user_password': 'Mexico1012', 'sn': device_sn,\r\n#           'device_password': device_pw, 'start_mrid': 0}\r\n# # payload_str = urllib.parse.urlencode(params, safe='@')\r\n# response = requests.get(url, params=params)\r\n# # print(response.url)\r\n# print(f'Response ok: {response.ok}')\r\n# content = json.loads(response.content)\r\n# pprint.pprint(content)\r\n# df = pd.read_json(content['data'], convert_dates=True, orient='split')\r\n# print(df)\r\n# -->> Request URL: https://zentracloud.com/api/v1/readings?user=jgarrido@morningstarco.com&user_password=Mexico1012&sn=z6-07220&device_password=82901-36182&start_mrid=22204\r\n#                  'https://zentracloud.com/api/v1/readings/?user=jgarrido@morningstarco.com&user_password=Mexico1012&sn=z6-07275&device_password=65299-46534&start_mrid=0'\r\n# ///////////////////////////////\r\n\r\n\r\n\r\n# ///////////////////////////////\r\n# METER API v4 Testing\r\n# GET READINGS\r\n# import requests\r\n# import json\r\n# import pprint\r\n# import pandas as pd\r\n# device_sn = \"z6-16138\"\r\n# token = \"Token {TOKEN}\".format(TOKEN=\"8c5815c78e7dd349d77da4821db5e3ec2087c4a2\")\r\n# url = \"https://zentracloud.com/api/v4/get_readings/\"\r\n# headers = {'content-type': 'application/json', 'Authorization': token}\r\n# output_format = \"json\"\r\n# page_num = 1\r\n#\r\n# params = {'device_sn': device_sn, 'output_format': output_format, 'per_page': 2000,\r\n#           'sort_by': 'ascending'} #, 'start_date': '1-1-2023', 'end_date': '7-10-2023'}\r\n# response1 = requests.get(url, params=params, headers=headers)\r\n# content1 = json.loads(response1.content)\r\n# print()\r\n#\r\n# next_url = content1['pagination']['next_url']\r\n# print(next_url)\r\n# response2 = requests.get(next_url, headers=headers)\r\n# content2 = json.loads(response2.content)\r\n# print()\r\n# pprint.pprint(content)\r\n# df = pd.read_json(content['data'], convert_dates=True, orient='split')\r\n# print(df)\r\n# ///////////////////////////////\r\n\r\n\r\n\r\n# ///////////////////////////////\r\n# GET ENVIRONMENTAL\r\n# device_sn = \"z6-12376\"\r\n# token = \"Token {TOKEN}\".format(TOKEN=\"6d1f65835ec6fd0c7ec77266d6a74d7f5a7be6b1\")\r\n# url = \"https://zentracloud.com/api/v3/get_env_model_data/\"\r\n# headers = {'content-type': 'application/json', 'Authorization': token}\r\n# output_format = \"json\"\r\n#\r\n# params = {'device_sn': device_sn, 'model_type': 'ETo', 'port_num': 2, 'inputs': {\"elevation\": 3, \"latitude\": 38.6120803, \"wind_measurement_height\": 2}}\r\n# response = requests.get(url, params=params, headers=headers)\r\n# content = json.loads(response.content)\r\n# pprint.pprint(content)\r\n# df = pd.read_json(content['data'], convert_dates=True, orient='split')\r\n# print(df)\r\n# ///////////////////////////////\r\n\r\n\r\n# show_pickle()\r\n\r\n# apply_AI_recommendation_to_logger('Nees_AI', 'IRR-1-80-AI')\r\n# num_of_fields = 0\r\n# growers = open_pickle()\r\n# for grower in growers:\r\n#     for field in grower.fields:\r\n#         for logger in field.loggers:\r\n#             crop = logger.crop_type.lower()[0:4]\r\n#             print(crop)\r\n#         num_of_fields += 1\r\n#\r\n# print(num_of_fields)\r\n\r\n# show_pickle()\r\n# growers = open_pickle()\r\n# techs = get_all_technicians(growers)\r\n# for tech in techs:\r\n#     if tech.name == 'Exsaelth':\r\n#         tech.email = 'ejimenez@morningstarco.com; cvalcheck@morningstarco.com'\r\n#     print(tech.name)\r\n#     print(tech.email)\r\n# write_pickle(growers)\r\n# show_pickle()\r\n# tech = Technician('Vanessa_and_Exsaelth', 'vcastillo@morningstarco.com; ejimenez@morningstarco.com')\r\n#\r\n# growers = open_pickle()\r\n# for grower in growers:\r\n#     print()\r\n#     print(grower.name)\r\n#     for field in grower.fields:\r\n#         print('\\t', field.name)\r\n#         for logger in field.loggers:\r\n#             print('\\t\\t', logger.name, ' R&D: ', logger.rnd)\r\n#     if grower.name == 'Lucero Watermark' or grower.name == 'Lucero Rio Vista' or grower.name == 'Lucero Morada':\r\n#         grower.technician = tech\r\n#         print(grower.technician.name)\r\n# write_pickle(growers)\r\n# update_information(get_data=True, check_for_notifications=True)\r\n# onlyCertainGrowersFieldLoggerUpdate('Saul', 'Meza', 'Development-C', subtract_from_mrid=48)\r\n# update_information(get_et=True, write_to_db=True)\r\n# cimisStationsPickle = CimisStation.open_cimis_station_pickle(CimisStation)\r\n# for stations in cimisStationsPickle:\r\n#     print(stations.station_number, ' : ', stations.latest_eto_value, ' : ', stations.updated)\r\n# cimisStations.showCimisStations()\r\n# show_pickle()\r\n\r\n\r\n# yesterdayRaw = date.today() - timedelta(1)\r\n# try:\r\n#     all_et_data_dicts = pull_all_et_values(str(yesterdayRaw), str(yesterdayRaw))\r\n# except Exception as error:\r\n#     print('ERROR in get_et')  # Used to just print ERROR\r\n#     print(error)\r\n#\r\n# try:\r\n#     write_all_et_values_to_db(all_et_data_dicts)\r\n# except Exception as error:\r\n#     print('ERROR in write_et_to_db')  # Used to just print ERROR\r\n#     print(error)\r\n\r\n# show_pickle()\r\n# remove_field('DCB', 'DCBCP:13,15')\r\n# update_information(get_data=True, write_to_db=True)\r\n# check_for_new_cimis_stations()\r\n# CimisStation.showCimisStations(CimisStation)\r\n# temp_ai_application()\r\n# print('BEFORE')\r\n# show_pickle()\r\n# onlyCertainGrowersFieldUpdate('Saul', 'Meza', get_data=True)\r\n# onlyCertainGrowersFieldLoggerUpdate('Hughes', 'Hughes233-4', 'HU-233-SW')\r\n\r\n\r\n# temp_ai_application()\r\n# show_pickle()\r\n# dbw = DBWriter()\r\n# dbw.add_gdd_columns_to_all_tables()\r\n# show_pickle()\r\n# dbwriter = DBWriter()\r\n# dbwriter.merge_all_tables_for_gdd()\r\n# show_pickle()\r\n# growers = open_pickle()\r\n# for grower in growers:\r\n#     for field in grower.fields:\r\n#         for logger in field.loggers:\r\n#             print(logger.cropType)\r\n#             if logger.lat == '':\r\n#                 logger.lat = None\r\n#             if logger.long == '':\r\n#                 logger.long = None\r\n#             if logger.name == 'HU-228-SW':\r\n#                 logger.lat = 36.5111213\r\n#             if logger.lat:\r\n#                 logger.lat = float(logger.lat)\r\n#             if logger.long:\r\n#                 logger.long = float(logger.long)\r\n#\r\n#             print(f'{logger.name} ---- Lat {logger.lat}, Long {logger.long}')\r\n# write_pickle(growers)\r\n#             print(logger.__dict__)\r\n# print(dir(logger))\r\n#     if grower.name == 'Saul':\r\n#         grower.deactivate()\r\n#     if 'Lucero' in grower.name:\r\n#         print(grower.name)\r\n#         for field in grower.fields:\r\n#             for logger in field.loggers:\r\n#                 print('\\t* factor for ', field.name, ' -- ', logger.name)\r\n#                 # print('(449 * ', logger.acres, ') / ')\r\n#                 # print('(', logger.gpm, ' * 0.85)')\r\n#                 print('\\t', round(((449*logger.acres)/(logger.gpm*0.85)),1))\r\n#             print()\r\n#         print()\r\n# temp_ai_application()\r\n\r\n# growers = open_pickle()\r\n# for grower in growers:\r\n#     if grower.name == 'Javier':\r\n#         print(grower.fields[0].loggers[1].gpm)\r\n#         print(grower.fields[0].loggers[1].acres)\r\n# for field in grower.fields:\r\n#     for logger in field.loggers:\r\n#         logger.gpm = 3600\r\n#         logger.acres = 128\r\n#\r\n\r\n# show_pickle()\r\n\r\n# write_pickle(growers)\r\n# print(grower.fields[0].loggers[1].gpm)\r\n# print(grower.fields[0].loggers[1].acres)\r\n#     print('Grower Name: ', grower.name)\r\n#     print('Total Fields: ', len(grower.fields))\r\n#     for field in grower.fields:\r\n#         print('\\tField Name: ', field.name, ' - ')\r\n#         print('\\tTotal Loggers: ', len(field.loggers))\r\n#         for logger in field.loggers:\r\n#             print('\\t\\tLogger Name: ', logger.name)\r\n#     print()\r\n# show_pickle()\r\n\r\n# onlyCertainGrowersFieldLoggerUpdate('RnD','RnDStoler Rate Trial', 'CONTROL-W', write_to_db=True, subtract_from_mrid=500000)\r\n# onlyCertainGrowersFieldLoggerUpdate('RnD','RnDStoler Rate Trial', 'RATE-1-E', write_to_db=True, subtract_from_mrid=500000)\r\n# onlyCertainGrowersFieldLoggerUpdate('RnD','RnDStoler Rate Trial', 'RATE-1-W', write_to_db=True, subtract_from_mrid=500000)\r\n# onlyCertainGrowersFieldLoggerUpdate('RnD','RnDStoler Rate Trial', 'RATE-2-E', write_to_db=True, subtract_from_mrid=500000)\r\n# onlyCertainGrowersFieldLoggerUpdate('RnD','RnDStoler Rate Trial', 'RATE-2-W', write_to_db=True, subtract_from_mrid=500000)\r\n# onlyCertainGrowersFieldLoggerUpdate('Bullseye Farms','Bullseye FarmsYO2E', 'YO2E-WM2-S', write_to_db=True, subtract_from_mrid=500000)\r\n#\r\n# apply_AI_recommendation_to_logger('Nees_AI', 'IRR-1-80-AI')\r\n# apply_AI_recommendation_to_logger('Nees_AI', 'IRR-1-80-Ctrl')\r\n# apply_AI_recommendation_to_logger('Nees_AI', 'IRR-2-60-AI')\r\n# apply_AI_recommendation_to_logger('Nees_AI', 'IRR-2-60-Ctrl')\r\n# apply_AI_recommendation_to_logger('Nees_AI', 'IRR-3-80-AI')\r\n# apply_AI_recommendation_to_logger('Nees_AI', 'IRR-3-80-Ctrl')\r\n# apply_AI_recommendation_to_logger('Nees_AI', 'IRR-4-60-AI')\r\n# apply_AI_recommendation_to_logger('Nees_AI', 'IRR-4-60-Ctrl')\r\n# apply_AI_recommendation_to_logger('Nees_AI', 'IRR-5-80-AI')\r\n# apply_AI_recommendation_to_logger('Nees_AI', 'IRR-5-80-Ctrl')\r\n\r\n# check_technician_clones()\r\n#\r\n# temp_ai_application()\r\n# growers = open_pickle()\r\n# for grower in growers:\r\n#     for field in grower.fields:\r\n#         field.update_et_tables()\r\n#         for logger in field.loggers:\r\n#             print('Grower -> ', grower.name, '  ', logger.name, ' - R&D? ', logger.rnd)\r\n#     if grower.technician.name == 'Vanessa':\r\n#         print(grower.technician.email)\r\n#         grower.technician.email = 'vcastillo@morningstarco.com; jkent@morningstarco.com'\r\n#         # break\r\n# #\r\n# # for grower in growers:\r\n# #     if grower.technician.name == 'Adriana':\r\n# #         grower.technician = vane\r\n# write_pickle(growers)\r\n#\r\n# print('AFTER')\r\n# check_technician_clones()\r\n\r\n# ady = get_technician('Adriana')\r\n# print(round(5.5798723,2))\r\n\r\n# growers = open_pickle()\r\n# del growers[-1]\r\n# write_pickle(growers)\r\n# show_pickle()\r\n#\r\n# grower_javier = setupGrower('Javier', '', 'Development Test Tech')\r\n# field_javier = setupField('Nees AI', '', '36.8339015', '-120.7384265', 124)\r\n# field_javier.nickname = 'Nees AI'\r\n# logger1 = setup_logger('z6-12374', '39225-28564', 'IRR-1-80-AI', '', 'tomatoes', 42, 36, 1750, 152, 'N', planting_date='05/03/2022')\r\n# logger2 = setup_logger('z6-11584', '73659-11380', 'IRR-1-80-Ctrl', '', 'tomatoes', 42, 36, 1750, 152, 'N', planting_date='05/03/2022')\r\n# logger3 = setup_logger('z6-16148', '96777-21941', 'IRR-2-60-AI', '', 'tomatoes', 42, 36, 1750, 152, 'N', planting_date='05/03/2022')\r\n# logger4 = setup_logger('z6-16143', '41233-23875', 'IRR-2-60-Ctrl', '', 'tomatoes', 42, 36, 1750, 152, 'N', planting_date='05/03/2022')\r\n# logger5 = setup_logger('z6-13262', '76705-06865', 'IRR-3-80-AI', '', 'tomatoes', 42, 36, 1750, 152, 'N', planting_date='05/03/2022')\r\n# logger6 = setup_logger('z6-07231', '40779-55230', 'IRR-3-80-Ctrl', '', 'tomatoes', 42, 36, 1750, 152, 'N', planting_date='05/03/2022')\r\n# logger7 = setup_logger('z6-11553', '80480-15456', 'IRR-4-60-AI', '', 'tomatoes', 42, 36, 1750, 152, 'N', planting_date='05/03/2022')\r\n# logger8 = setup_logger('z6-01889', '24590-03289', 'IRR-4-60-Ctrl', '', 'tomatoes', 42, 36, 1750, 152, 'N', planting_date='05/03/2022')\r\n# logger9 = setup_logger('z6-12391', '57568-22189', 'IRR-5-80-AI', '', 'tomatoes', 42, 36, 1750, 152, 'N', planting_date='05/03/2022')\r\n# logger10 = setup_logger('z6-12368', '32338-34041', 'IRR-5-80-Ctrl', '', 'tomatoes', 42, 36, 1750, 152, 'N', planting_date='05/03/2022')\r\n\r\n# field_javier.addLoggers([logger1, logger2, logger3, logger4, logger5, logger6, logger7, logger8, logger9, logger10])\r\n# addFieldToGrower(grower_javier.name, field_javier)\r\n\r\n\r\n# grower = setupGrower('Saul', '1S8acM-DKwrMaxOZpEdndvHn59xij7OHxtB6ztErIxoY')\r\n# addGrowerToGrowers(grower)\r\n#\r\n# field = setupField('Meza', '1TtZQM1GU6h88P83U6EGEUojIResR5gSyBBC7SR6gQZc', '37.053941099999996', '-120.80917459999999', '56')\r\n# logger1 = setupLogger('z6-07275', '65299-46534', 'Development', 1665227864, 'Tomato', 36, 22, 1200, 100)\r\n# field.addLogger(logger1)\r\n# addFieldToGrower('Saul', field)\r\n\r\n# logger_list_javier = []\r\n# logger_list_jesus = open_specific_pickle(\"loggerList.pickle\")\r\n# growers = open_pickle()\r\n# for grower in growers:\r\n#     print(grower.name)\r\n\r\n# del growers[-1]\r\n# write_pickle(growers)\r\n# for field in grower.fields:\r\n# print(field.name)\r\n#         for logger in field.loggers:\r\n#             logger_list_javier.append(logger)\r\n#\r\n# for logger in logger_list_javier:\r\n#     if logger.id not in logger_list_jesus:\r\n#         print(logger.id, ' not in')\r\n#         print(logger.name)\r\n#         print(logger.field.name)\r\n#         print(logger.active)\r\n#         print()\r\n\r\n\r\n# print(tech_dict)\r\n#     if grower.name == 'Ryan Jones':\r\n#         og_vanessa = grower.technician\r\n#\r\n# for grower in growers:\r\n#     if grower.name == 'Maricopa Orchards':\r\n#         grower.technician = og_vanessa\r\n#\r\n# for grower in growers:\r\n#     print(grower.name, ' - ', grower.technician.name, ' - ', id(grower.technician))\r\n\r\n\r\n# all_technicians = get_all_technicians(growers)\r\n# for tech in all_technicians:\r\n#     print(tech.name, ' - ', id(tech))\r\n\r\n# updated_run_report()\r\n# show_pickle()\r\n# g = growers[0]\r\n# # Multiple emails to each person with their own individual file\r\n# # for path in files:\r\n# #     g.all_notifications.email_tech_notifications(tech_notif_folder / path, path)\r\n# all_technicians = get_all_technicians(growers)\r\n# list_of_notifcation_files = []\r\n# for tech in all_technicians:\r\n#     list_of_notifcation_files.append(tech.notification_file_path)\r\n#\r\n# g.all_notifications.email_tech_notifications(list_of_notifcation_files, 'Tech Notifications')\r\n#\r\n# g.all_notifications.email_error_notifications()\r\n# for grower in growers:\r\n#     for field in grower.fields:\r\n#         print(field.name, ' - ', field.preview_url)\r\n#             for logger in field.loggers:\r\n#                 if not logger.active:\r\n#                     print('Logger inactive', logger.id, ' - ', logger.name)\r\n# field.update(get_data=True, write_to_portal=True)\r\n# for grower in growers:\r\n#     if grower.name == 'Dougherty Bros':\r\n#         grower.to_string()\r\n#     if grower.technician is not None:\r\n#         print(grower.name, ' - ', grower.technician.name)\r\n#     # print(grower.name, ' - ', grower.technician.name)\r\n#     if grower.name == 'KTN JV':\r\n#         ex = grower.technician\r\n#     if grower.name == 'Mumma Bros':\r\n#         grower.technician = ex\r\n# if grower.name == 'Mumma Bros':\r\n#     grower.cwsi_processor = CwsiProcessor()\r\n# write_pickle(growers)\r\n#         for field in grower.fields:\r\n#             if field.name == 'La QuintaDates Block 36 DB':\r\n#                 field.loggers[0].update(subtract_from_mrid=48)\r\n# # #\r\n# for grower in growers:\r\n#     if grower.name == \"Mumma Bros\":\r\n#         print(grower.technician.name)\r\n#         vane = grower.technician\r\n#\r\n# for grower in growers:\r\n#     if grower.name == \"RKB\":\r\n#         grower.technician = vane\r\n#\r\n# #\r\n# # for grower in growers:\r\n# #     if grower.name == 'Maricopa Orchards':\r\n# #         grower.technician = ady\r\n# #\r\n# for grower in growers:\r\n#     if grower.technician.name == 'Vanessa':\r\n#         print(grower.name, '-', grower.technician.name, grower.technician)\r\n# techs = get_all_technicians(growers)\r\n# print()\r\n# for tech in techs:\r\n#     print(tech.name, '-', tech)\r\n# #\r\n# write_pickle(growers)\r\n\r\n# if os.path.exists('C:\\\\Users\\\\javie\\\\Projects\\\\S-TOMAto\\\\credentials.json'):\r\n#     os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"C:/Users/javie/Projects/S-TOMAto/credentials.json\"\r\n# elif os.path.exists('C:\\\\Users\\\\javie\\\\PycharmProjects\\\\Stomato\\\\credentials.json'):\r\n#     os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"C:/Users/javie/PycharmProjects/Stomato/credentials.json\"\r\n# elif os.path.exists('C:\\\\Users\\\\jsalcedo\\\\PycharmProjects\\\\Stomato\\\\credentials.json'):\r\n#     os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"C:/Users/jsalcedo/PycharmProjects/Stomato/credentials.json\"\r\n# elif os.path.exists('C:\\\\Users\\\\jesus\\\\PycharmProjects\\\\Stomato\\\\credentials.json'):\r\n#     os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"C:/Users/jesus/PycharmProjects/Stomato/credentials.json\"\r\n# os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"C:/Users/javie/PycharmProjects/Stomato/credentials.json\"\r\n# dbw = DBWriter()\r\n# dbw.create_dataset('test2', project='growers-2022')\r\n# print('Hi')\r\n# growers = open_pickle()\r\n# # # #\r\n# for grower in growers:\r\n#     for field in grower.fields:\r\n#         if field.cwsi_processor is None:\r\n#             print(field, ' has no cwsi processor')\r\n# #             if field.name == 'Dougherty BrosHB':\r\n# #                 field.update(get_data=True, write_to_portal=True)\r\n# for field in grower.fields:\r\n#     for logger in field.loggers:\r\n#         print(logger.loggerDirection)\r\n# if grower.name == 'Carvalho':\r\n#     grower.update(get_data=True, write_to_portal=True)\r\n# for field in grower.fields:\r\n#     for logger in field.loggers:\r\n#         print(logger.loggerDirection)\r\n# grower.to_string()\r\n# grower.update(get_data=True, write_to_portal=True)\r\n#         for field in grower.fields:\r\n#             if field.name == 'Carvalho302':\r\n#                 field.nickname = 'KC 302'\r\n#             if field.name == 'Carvalho307/307A':\r\n#                 field.nickname = 'KC 307/307A'\r\n# write_pickle(growers)\r\n#         field.update(get_data=True, write_to_portal=True)\r\n# grower.update(get_data=True, write_to_portal=True)\r\n\r\n#     for field in grower.fields:\r\n#         for logger in field.loggers:\r\n#             logger.fieldCapacity = int(logger.fieldCapacity)\r\n#             logger.wiltingPoint = int(logger.wiltingPoint)\r\n# write_pickle(growers)\r\n# if grower.name == 'Carvalho':\r\n# print('l;l')\r\n#     for field in grower.fields:\r\n#         if field.name == 'Carvalho302':\r\n#             field.update(get_data=True, write_to_portal=True)\r\n\r\n\r\n# grower.update(get_data=True, write_to_portal=True)\r\n# grower.fields[0].report_url = 'testing'\r\n# grower.to_string()\r\n# write_pickle(growers)\r\n\r\n# dbw = DBWriter()\r\n# dataset_id = 'growers-2022.Saul.field_averages'\r\n# dml = \"SELECT field FROM \" + dataset_id + \" WHERE field = 'Meza'\"\r\n# result = dbw.run_dml(dml, project='growers-2022')\r\n# if len(list(result)) >= 1:\r\n#     print('Found')\r\n# else:\r\n#     print('Not Found')\r\n\r\n#     # for field in grower.fields:\r\n#     #     # print(field.name)\r\n#     #     for logger in field.loggers:\r\n#     #         print(logger.name)\r\n#     # print(grower.name)\r\n#     if grower.name == 'David Santos':\r\n#         tech = grower.technician\r\n#     if grower.technician is None:\r\n#         print(grower.name, '-', 'NONE')\r\n#         grower.technician = tech\r\n#         print(grower.name, '-', grower.technician.name)\r\n\r\n# write_pickle(growers)\r\n# else:\r\n#     print(grower.name, '-',grower.technician.name)\r\n#     if grower.technician is not None:\r\n#         print(grower.name, '-', grower.technician.name)\r\n#     if grower.updated:\r\n#         pass\r\n#     else:\r\n#         for field in grower.fields:\r\n#             if field.updated:\r\n#                 pass\r\n#             else:\r\n#                 for logger in field.loggers:\r\n#                     if logger.updated:\r\n#                         pass\r\n#                     else:\r\n#                         print(grower.name, '-', field.name, '-', logger.name)\r\n#             if logger.crashed:\r\n#                 print(grower.name, '-', field.name, '-', logger.id)\r\n# # write_pickle(growers)\r\n\r\n# print(grower.technician.name)\r\n# print()\r\n# apply_cwsi_to_whole_table('Hughes301', 'z6-07274', 'pistachios')\r\n# apply_cwsi_to_whole_table('Bone_Farms_LLCR12_13', '5G129089', 'pistachios')\r\n# apply_cwsi_to_whole_table('Bone_Farms_LLCR12_13', '5G129217', 'pistachios')\r\n# apply_cwsi_to_whole_table('Bone_Farms_LLCR12_13', '5G129223', 'pistachios')\r\n\r\n# weather_schema = [\r\n#     bigquery.SchemaField(\"date\", \"DATE\"),\r\n#     bigquery.SchemaField(\"day\", \"STRING\"),\r\n#     bigquery.SchemaField(\"order\", \"FLOAT\"),\r\n#     bigquery.SchemaField(\"temp\", \"FLOAT\"),\r\n#     bigquery.SchemaField(\"rh\", \"FLOAT\"),\r\n#     bigquery.SchemaField(\"vpd\", \"FLOAT\"),\r\n#     bigquery.SchemaField(\"icon\", \"STRING\"),\r\n# ]\r\n#\r\n# dbw = DBWriter()\r\n# dbw.write_to_table_from_csv('OPC3_2', 'weather_forecast', 'Weather Forecast Test.csv', weather_schema, overwrite=True)\r\n# cimisStation = CimisStation()\r\n# print(cimisStation.return_list_of_stations())\r\n# cimisStationsPickle = cimisStation.open_cimis_station_pickle()\r\n# cimis = CIMIS()\r\n# print(cimis.get_closest_cimis_station(39.968252, -122.0956379))\r\n# for f in cimisStationsPickle:\r\n#     f.active = True\r\n# if f.station_number == '222':\r\n#     f.active = True\r\n# print(f\"{f.station_number}: {f.active}\")\r\n# write_specific_pickle(cimisStationsPickle, \"cimisStation.pickle\")\r\n# cimisStationsPickle = CimisStation.open_cimis_station_pickle(CimisStation)\r\n# for stations in cimisStationsPickle:\r\n#     stations.updated = False\r\n#     print(stations.station_number, \":\", stations.updated)\r\n# write_pickle(cimisStationsPickle, filename=\"cimisStation.pickle\")\r\n# d = pull_all_et_values('2022-01-01', '2022-12-01')\r\n# write_all_et_values_to_db(d)\r\n# print(d)\r\n# show_pickle()\r\n# deactivate_growers_with_all_inactive_fields()\r\n\r\n# write_et_values_specific_station('2022-01-01', '2023-01-01', '7')\r\n# count = 0\r\n# show_pickle()\r\n# growers = open_pickle()\r\n# removeGrower('a')\r\n# techs = get_all_technicians(growers)\r\n# print(techs)\r\n# for g in growers:\r\n#     for f in g.fields:\r\n#         for l in f.loggers:\r\n#             if l.cropType == 'pistachio' or l.cropType == 'Pistachios':\r\n#                 l.grower.to_string()\r\n#             if l.model == 'em50g':\r\n#                 count = count + 1\r\n#                 l.to_string()\r\n# print(count)\r\n\r\n# growers = open_pickle()\r\n# for g in growers:\r\n#     # if g.name == 'Andrew':\r\n#     for f in g.fields:\r\n#         for l in f.loggers:\r\n#                 # l.loggerD\r\n#     # irection = 'SE'\r\n#                 # print(l.loggerDirection)\r\n#                 # print(f.name)\r\n#                 # print(\"\\t\" + l.name)\r\n# write_pickle(growers)\r\n# showGrower('Andrew')\r\n# growers = open_pickle()\r\n# techs = get_all_technicians(growers)\r\n# print()\r\n# showGrower('Saul')\r\n# reassign_technician('Serafin', 'Vanessa')\r\n#\r\n# onlyCertainGrowersUpdate(['Saul'], get_weather=True, write_to_db=True)\r\n# onlyCertainGrowersFieldUpdate('Carvalho', 'Carvalho310/310A', get_weather=True, write_to_db=True)\r\n\r\n\r\n# show_pickle()\r\n\r\n# onlyCertainGrowersFieldLoggerUpdate('DCB', 'DCBHome', 'z6-06009', specific_mrid=12381, write_to_db=True)\r\n# count = 0\r\n# growers = open_pickle()\r\n# for g in growers:\r\n#     count = count + 1\r\n# print(count)\r\n#     if g.name == 'Saul':\r\n#         g.updated = False\r\n# write_pickle(growers)\r\n# onlyCertainGrowersUpdate(['Saul'], get_data=True, write_to_db=True)\r\n\r\n# onlyCertainGrowersFieldLoggerUpdate('La Quinta', 'La QuintaDates', 'z6-12396', subtract_from_mrid=36,\r\n# write_to_db=True)\r\n\r\n#         for f in g.fields:\r\n#             if f.name == 'Rossow13':\r\n#                 for l in f.loggers:\r\n#                     if l.id == 'z6-07261':\r\n#                         l.crashed = False\r\n# write_pickle(growers)\r\n# show_pickle()\r\n# total_growers = 0\r\n# update_information()\r\n# total_fields = 0\r\n# total_loggers = 0\r\n# #\r\n# growers = open_pickle()\r\n# for grower in growers:\r\n#     print('\\t {}'.format(grower.name))\r\n#     total_growers = total_growers + 1\r\n#     for field in grower.fields:\r\n#         total_fields = total_fields + 1\r\n#         for logger in field.loggers:\r\n#             total_loggers = total_loggers + 1\r\n# print('Total Growers: {}'.format(total_growers))\r\n# print('Total Fields: {}'.format(total_fields))\r\n# print('Total Loggers: {}'.format(total_loggers))\r\n# for field in grower.fields:\r\n#     if not field.updated and field.active:\r\n#         for logger in field.loggers:\r\n#             print('Grower - {}\\t\\tField - {}\\t\\tLogger - {}'.format(grower.name, field.name, logger.id))\r\n\r\n# technicians = []\r\n# growers = open_pickle()\r\n# for g in growers:\r\n#     # g.technician.growers.append(g)\r\n#     if g.technician not in technicians:\r\n#         technicians.append(g.technician)\r\n#\r\n# for t in technicians:\r\n#     # t.growers = []\r\n#     t.to_string()\r\n# write_pickle(growers)\r\n\r\n# show_pickle()\r\n# onlyCertainGrowersFieldUpdate('David Santos', 'David SantosFA1', get_et=False, get_weather=True, get_data=True,\r\n#                                   write_to_sheet=True, write_to_portal_sheet=False, write_to_db=True,\r\n#                                   check_for_notifications=False)\r\n# onlyCertainGrowersFieldLoggerUpdate('David Santos', 'David SantosFA1', 'z6-01995', write_to_db=True)\r\n# onlyCertainGrowersFieldUpdate('David Santos', 'David SantosFA1', True, True, True, False, True, True, False)\r\n# update_et_tables()\r\n\r\n# list_of_db_fields = []\r\n# db = DBWriter()\r\n# datasets, project = db.get_datasets()\r\n# for d in datasets:\r\n#     # d.dataset_id.replace('_', ' ')\r\n#     # print(d.dataset_id.replace('_', ' '))\r\n#     list_of_db_fields.append(d.dataset_id)\r\n# print(\"\\tDB Fields ({})\".format(len(list_of_db_fields)))\r\n# for f in list_of_db_fields:\r\n#     print(f)\r\n# print(\"\\tDB Fields-----------\")\r\n#\r\n# list_of_pickle_fields = []\r\n# growers = open_pickle()\r\n# for grower in growers:\r\n#     for field in grower.fields:\r\n#         # print(field.name)\r\n#         field_name = field.name\r\n#         field_name = field_name.replace('-', '_')\r\n#         field_name = field_name.replace(' ', '_')\r\n#         field_name = field_name.replace('/', '_')\r\n#         list_of_pickle_fields.append(field_name)\r\n# list_of_pickle_fields.sort()\r\n# print(\"\\tPickle Fields ({})\".format(len(list_of_pickle_fields)))\r\n# for f in list_of_pickle_fields:\r\n#     print(f)\r\n# print(\"\\tPickle Fields------------\")\r\n#\r\n# set_difference = set(list_of_pickle_fields).symmetric_difference(set(list_of_db_fields))\r\n# list_difference = list(set_difference)\r\n#\r\n# list_difference.sort()\r\n# print(\"\\tDifferences ({})\".format(len(list_difference)))\r\n# for d in list_difference:\r\n#     print(d)\r\n# print(\"\\tDifferences------------\")\r\n\r\n# for item_db, item_pickle, item_diff in zip(list_of_db_fields, list_of_pickle_fields, list_difference):\r\n#     print('\\t - {} \\t\\t - {} \\t\\t - {}'.format(item_db, item_pickle, item_diff))\r\n# # print(list_difference)\r\n\r\n# for logger in field.loggers:\r\n\r\n# if path.exists(\"G:\\\\My Drive\\\\S-TOMAto\\\\2021\\\\Pickle\\\\ai_game_data.pickle\"):\r\n#     with open(\"G:\\\\My Drive\\\\S-TOMAto\\\\2021\\\\Pickle\\\\ai_game_data.pickle\", 'rb') as f:\r\n#         ai_game_data_all = pickle.load(f)\r\n# ai_game_data_all.model_vs_human_comparison()\r\n\r\n# ai_game_data_all.old_ai_game_data = []\r\n# new_ai_game_data = []\r\n# count = 0\r\n# for i in ai_game_data_all.ai_game_data:\r\n#     print('Game Date: {}'.format(i.game_date))\r\n#     if  i.game_date < datetime(2021,10,20):\r\n#         print('OLD')\r\n#         # new_ai_game_data.append(i)\r\n#         # ai_game_data_all.old_ai_game_data.append(i)\r\n#         # ai_game_data_all.remove_data_point(i)\r\n# # ai_game_data_all.ai_game_data = new_ai_game_data\r\n# if path.exists(\"G:\\\\My Drive\\\\S-TOMAto\\\\2021\\\\Pickle\\\\ai_game_data.pickle\"):\r\n#     with open(\"G:\\\\My Drive\\\\S-TOMAto\\\\2021\\\\Pickle\\\\ai_game_data.pickle\", 'wb') as f:\r\n#         pickle.dump(ai_game_data_all, f)\r\n\r\n#     print('Planting Date: {}'.format(i.planting_date))\r\n#     print('Harvest Date: {}'.format(i.harvest_date))\r\n#     print('Crop Stage: {}'.format(i.crop_stage))\r\n#     print()\r\n\r\n# if path.exists(\"G:\\\\My Drive\\\\S-TOMAto\\\\2021\\\\Pickle\\\\ai_game_data.pickle\"):\r\n#     with open(\"G:\\\\My Drive\\\\S-TOMAto\\\\2021\\\\Pickle\\\\ai_game_data.pickle\", 'rb') as f:\r\n#         ai_game_data_all = pickle.load(f)\r\n# # ai_game_data.model_vs_human_comparison()\r\n# growers = open_pickle()\r\n# for dp in ai_game_data_all.ai_game_data:\r\n#     if not hasattr(dp, 'crop stage'):\r\n#         print('Field: {}            Logger: {}'.format(dp.field, dp.logger))\r\n#         print('\\t Grabbing planting date-------------')\r\n#         for g in growers:\r\n#             for f in g.fields:\r\n#                 if dp.field == 'MatteoliG10B':\r\n#                     dp.planting_date = date(2021, 4, 6)\r\n#                 if f.name == dp.field:\r\n#                     print('Found field: {}'.format(dp.field))\r\n#                     for l in f.loggers:\r\n#                         print('Planting date: {}'.format(l.planting_date))\r\n#                         dp.planting_date = l.planting_date\r\n#         print('\\t Grabbing harvest date-------------')\r\n#         dataset = dp.field\r\n#         logger_id = dp.logger\r\n#\r\n#         dataset = dataset.replace(' ', '_')\r\n#         dataset = dataset.replace('-', '_')\r\n#         dataset = dataset.replace('/', '_')\r\n#         dml = 'SELECT *' \\\r\n#               'FROM `stomato.' + dataset + '.' + logger_id + '` ' \\\r\n#               'WHERE et_hours is not NULL AND logger_id is not NULL ORDER BY date ASC'\r\n#         dbwriter = DBWriter()\r\n#         result = dbwriter.run_dml(dml)\r\n#         db_results = {\"logger_id\": [], \"date\": [], \"time\": [], \"canopy_temperature\": [], \"ambient_temperature\": [],\r\n#                       \"vpd\": [], \"vwc_1\": [], \"vwc_2\": [], \"vwc_3\": [], \"field_capacity\": [], \"wilting_point\": [],\r\n#                       \"daily_gallons\": [], \"daily_switch\": [], \"daily_hours\": [], \"daily_pressure\": [],\r\n#                       \"daily_inches\": [], \"psi\": [], \"psi_threshold\": [], \"psi_critical\": [],\r\n#                       \"sdd\": [], \"rh\": [], 'eto': [], 'kc': [], 'etc': [], 'et_hours': [],\r\n#                       \"phase1_adjustment\": [], \"phase1_adjusted\": [], \"phase2_adjustment\": [], \"phase2_adjusted\": []}\r\n#\r\n#         for r in result:\r\n#             logger_id = r[0]\r\n#             rdate = r[1]\r\n#         #     time = r[2]\r\n#         #     canopy_temperature = r[3]\r\n#         #     ambient_temperature = r[4]\r\n#         #     vpd = r[5]\r\n#         #     vwc_1 = r[6]\r\n#         #     vwc_2 = r[7]\r\n#         #     vwc_3 = r[8]\r\n#         #     field_capacity = r[9]\r\n#         #     wilting_point = r[10]\r\n#         #     daily_gallons = r[11]\r\n#         #     daily_switch = r[12]\r\n#         #     daily_hours = r[13]\r\n#         #     daily_pressure = r[14]\r\n#         #     daily_inches = r[15]\r\n#         #     psi = r[16]\r\n#         #     psi_threshold = r[17]\r\n#         #     psi_critical = r[18]\r\n#         #     sdd = r[19]\r\n#         #     rh = r[20]\r\n#         #     eto = r[21]\r\n#         #     kc = r[22]\r\n#         #     etc = r[23]\r\n#         #     et_hours = r[24]\r\n#         #\r\n#             db_results['logger_id'].append(logger_id)\r\n#             db_results['date'].append(rdate)\r\n#         #     db_results['time'].append(time)\r\n#         #     db_results['canopy_temperature'].append(canopy_temperature)\r\n#         #     db_results['ambient_temperature'].append(ambient_temperature)\r\n#         #     db_results['vpd'].append(vpd)\r\n#         #     db_results['vwc_1'].append(vwc_1)\r\n#         #     db_results['vwc_2'].append(vwc_2)\r\n#         #     db_results['vwc_3'].append(vwc_3)\r\n#         #     db_results['field_capacity'].append(field_capacity)\r\n#         #     db_results['wilting_point'].append(wilting_point)\r\n#         #     db_results['daily_gallons'].append(daily_gallons)\r\n#         #     db_results['daily_switch'].append(daily_switch)\r\n#         #     db_results['daily_hours'].append(daily_hours)\r\n#         #     db_results['daily_pressure'].append(daily_pressure)\r\n#         #     db_results['daily_inches'].append(daily_inches)\r\n#         #     db_results['psi'].append(psi)\r\n#         #     db_results['psi_threshold'].append(psi_threshold)\r\n#         #     db_results['psi_critical'].append(psi_critical)\r\n#         #     db_results['sdd'].append(sdd)\r\n#         #     db_results['rh'].append(rh)\r\n#         #     db_results['eto'].append(eto)\r\n#         #     db_results['kc'].append(kc)\r\n#         #     db_results['etc'].append(etc)\r\n#         #     db_results['et_hours'].append(et_hours)\r\n#\r\n#         dp.harvest_date = db_results['date'][-1]\r\n#         print('\\t Grabbing crop stage-------------')\r\n#         crop_stage = get_crop_stage(dp.date, dp.harvest_date, dp.planting_date)\r\n#         dp.crop_stage = crop_stage\r\n#         print('\\t DONE-----------------------------------')\r\n#         print()\r\n#\r\n# if path.exists(\"G:\\\\My Drive\\\\S-TOMAto\\\\2021\\\\Pickle\\\\ai_game_data.pickle\"):\r\n#     with open(\"G:\\\\My Drive\\\\S-TOMAto\\\\2021\\\\Pickle\\\\ai_game_data.pickle\", 'wb') as f:\r\n#         pickle.dump(ai_game_data_all, f)\r\n\r\n\r\n# show_pickle()\r\n\r\n#\r\n# data_fixed = 0\r\n# for data_point in ai_game_data.ai_game_data:\r\n#     ran = data_point.fix_recommendations()\r\n#     if ran:\r\n#         data_fixed = data_fixed + 1\r\n#\r\n# print('Data Fixed: {}'.format(data_fixed))\r\n#\r\n# if path.exists(\"G:\\\\My Drive\\\\S-TOMAto\\\\2021\\\\Pickle\\\\ai_game_data.pickle\"):\r\n#     with open(\"G:\\\\My Drive\\\\S-TOMAto\\\\2021\\\\Pickle\\\\ai_game_data.pickle\", 'wb') as f:\r\n#         pickle.dump(ai_game_data, f)\r\n\r\n# #\r\n# ai_game_data.add_to_north_pool('Dougherty BrosD3', 'z6-11982')\r\n# ai_game_data.add_to_north_pool('MatteoliG10B', 'z6-02039')\r\n# del ai_game_data.north_pool[-1]\r\n# ai_game_data.show_content()\r\n\r\n# if path.exists(\"G:\\\\My Drive\\\\S-TOMAto\\\\2021\\\\Pickle\\\\ai_game_data.pickle\"):\r\n#     with open(\"G:\\\\My Drive\\\\S-TOMAto\\\\2021\\\\Pickle\\\\ai_game_data.pickle\", 'wb') as f:\r\n#         pickle.dump(ai_game_data, f)\r\n\r\n# show_pickle()\r\n# growers = open_pickle()\r\n# for g in growers:\r\n#     if g.name == 'Bullseye_Farms':\r\n#         g.to_string()\r\n# for f in g.fields:\r\n#     if g.name == 'MatteoliG10B':\r\n\r\n\r\n# showGrower('TP')\r\n\r\n# for dp in data.ai_game_data:\r\n#     dp.human_p1 = float(dp.human_p1)\r\n#     dp.human_p2 = float(dp.human_p2)\r\n#     dp.human_p3 = float(dp.human_p3)\r\n#\r\n# data = None\r\n# if path.exists(\"G:\\\\My Drive\\\\S-TOMAto\\\\2021\\\\Pickle\\\\ai_game_data.pickle\"):\r\n#     with open(\"G:\\\\My Drive\\\\S-TOMAto\\\\2021\\\\Pickle\\\\ai_game_data.pickle\", 'rb') as f:\r\n#         pickle.dump(data, f)\r\n\r\n# data.show_content\r\n# data.clear_all_data()\r\n# data.show_content()\r\n#\r\n# if path.exists(\"G:\\\\My Drive\\\\S-TOMAto\\\\2021\\\\Pickle\\\\ai_game_data.pickle\"):\r\n#     with open(\"G:\\\\My Drive\\\\S-TOMAto\\\\2021\\\\Pickle\\\\ai_game_data.pickle\", 'wb') as f:\r\n#         pickle.dump(data, f)\r\n\r\n# show_pickle()\r\n\r\n# content.south_pool.remove(content.south_pool[-1])\r\n# print(content.south_pool)\r\n# for data_point in content.ai_game_data:\r\n#     print('Field-{}  Logger-{}  Game Date-{}'.format(data_point.field, data_point.logger, data_point.game_date))\r\n#     print('Data Point-{}'.format(data_point.date))\r\n\r\n# apply_cwsi_to_whole_table('LemonicaLemon_E', 'z6-02020', 'Lemons')\r\n# apply_cwsi_to_whole_table('LemonicaLemon_E', 'z6-07266', 'Lemons')\r\n# apply_cwsi_to_whole_table('LemonicaLemon_E', 'z6-07272', 'Lemons')\r\n# apply_cwsi_to_whole_table('LemonicaLemon_E', 'z6-12420', 'Lemons')\r\n# apply_cwsi_to_whole_table('Hughes301', 'z6-07165', 'Pistachios')\r\n# apply_cwsi_to_whole_table('Hughes301', 'z6-07274', 'Pistachios')\r\n# apply_cwsi_to_whole_table('Maricopa_Orchards1831', 'z6-12298', 'Pistachios')\r\n# apply_cwsi_to_whole_table('Maricopa_Orchards1831', 'z6-12209', 'Pistachios')\r\n# apply_cwsi_to_whole_table('Maricopa_Orchards1831', 'z6-12332', 'Pistachios')\r\n# apply_cwsi_to_whole_table('Maricopa_Orchards1831', 'z6-12338', 'Pistachios')\r\n# apply_cwsi_to_whole_table('Bullseye_FarmsMB6', 'z6-02181', 'Pistachios')\r\n# apply_cwsi_to_whole_table('Bullseye_FarmsMB6', 'z6-02018', 'Pistachios')\r\n# apply_cwsi_to_whole_table('Bullseye_FarmsMB6', 'z6-05983', 'Pistachios')\r\n# apply_cwsi_to_whole_table('Bullseye_FarmsMB6', 'z6-06004', 'Pistachios')\r\n# apply_cwsi_to_whole_table('Bullseye_FarmsYO2E_West', 'z6-11527', 'Pistachios')\r\n# apply_cwsi_to_whole_table('Bullseye_FarmsYO2E_West', 'z6-11528', 'Pistachios')\r\n# apply_cwsi_to_whole_table('Bullseye_FarmsYO2E_East', 'z6-11519', 'Pistachios')\r\n# apply_cwsi_to_whole_table('Bullseye_FarmsYO2E_East', 'z6-11516', 'Pistachios')\r\n# apply_cwsi_to_whole_table('Shiraz_RanchS6_Pistachios', 'z6-11487', 'Pistachios')\r\n# apply_cwsi_to_whole_table('Shiraz_RanchS6_Pistachios', 'z6-11520', 'Pistachios')\r\n\r\n# growers = open_pickle() for grower in growers: for field in grower.fields: for logger in field.loggers: print(\r\n# 'Grower {}\\tField {}\\tLogger {}\\tLat {}\\tLong {}'.format(grower.name, field.name, logger.id, field.lat,\r\n# field.long)) write_pickle(growers)\r\n\r\n\r\n# onlyCertainGrowersFieldLoggerUpdate('OPC', 'OPC3-3', 'z6-07268', specific_mrid=0)\r\n\r\n# growers = open_pickle() for grower in growers: for field in grower.fields: for logger in field.loggers: if\r\n# logger.cropType == 'Pistachios': print('Grower {}\\tField {}\\tLogger {}\\tLat {}\\tLong {}'.format(grower.name,\r\n# field.name, logger.id, field.lat, field.long)) write_pickle(growers)\r\n\r\n# growers = open_pickle() for grower in growers: for field in grower.fields: for logger in field.loggers: if\r\n# logger.id == 'z6-11936': file_name = logger.jesus_output_folder + logger.id + '.dxd' battery_level =\r\n# logger.read_battery_level(file_name) print('Grower-{}\\tField-{}\\tLogger-{}\\tBattery Level-{}'.format(grower.name,\r\n# field.name, logger.id, battery_level))\r\n\r\n\r\n# show_pickle()\r\n# showGrower('OPC Independece Project')\r\n# growers = open_pickle()\r\n# for grower in growers:\r\n#     for field in grower.fields:\r\n#         if field.name == ''\r\n#         for logger in field.loggers:\r\n\r\n# reset_updated_all() onlyCertainGrowersFieldUpdate('OPC', 'OPC3-2', get_weather=True, get_data=True,\r\n# write_to_db=True, write_to_portal_sheet=True, write_to_sheet=True, check_for_notifications=True)\r\n# update_information(get_et=True, write_to_db=True)\r\n\r\n# growers = open_pickle() for grower in growers: for field in grower.fields: for logger in field.loggers: print(\r\n# 'Grower {}\\tField {}\\tLogger {}\\tCrop {}'.format(grower.name, field.name, logger.id, logger.cropType))\r\n# write_pickle(growers)\r\n\r\n# updatedRunReport()\r\n# show_pickle()\r\n\r\n# growers = open_pickle()\r\n# for grower in growers:\r\n#     for field in grower.fields:\r\n#         if not field.active:\r\n#             print('Field active? {}'.format(field.active))\r\n#             # field.deactivate()\r\n#             for l in field.loggers:\r\n#                 print(l.active)\r\n# write_pickle(growers)\r\n# apply_AI_recommendation_to_logger('DCBDT', 'z6-05993')\r\n# growers = open_pickle()\r\n# for g in growers:\r\n#     if g.name == 'TP':  # or g.name == 'Dougherty' or g.name == 'Bullseye':\r\n#         for f in g.fields:\r\n#             if f.name == 'TPMM3':\r\n#                 for l in f.loggers:\r\n#                     print('Field Name - {} | Logger - {}'.format(f.name, l.id))\r\n#                     apply_AI_recommendation_to_logger(f.name, l.id)\r\n\r\n# growers = open_pickle()\r\n# for g in growers:\r\n#     g.active = True\r\n#     for f in g.fields:\r\n#         f.active = True\r\n#         for l in f.loggers:\r\n#             l.active = True\r\n# write_pickle(growers)\r\n\r\n# show_pickle()\r\n# showGrower('Bullseye')\r\n\r\n# dbwriter = DBWriter()\r\n# dbwriter.add_AI_columns_to_all_tables()\r\n\r\n# updatedRunReport()\r\n# showField('David SantosFA1')\r\n\r\n# show_pickle()\r\n\r\n# updatedRunReport() Technician lists Vanessa = [Zuckerman Farms, FS , Ryan Jones, JJB Farms, BT, Hughes, Carvalho,\r\n# Rossow, Andrew, Lucero Hanford, Lucero Morada, Lucero 8 Mile] Serafin = [OPC, David Santos, JHP, RKB Ranch, DCB,\r\n# Bone Farms LLC]\r\n\r\n# growers = open_pickle()\r\n# # # loggerCount = 0\r\n# # # # rndLoggerCount = 0\r\n# vanessasGrowerList = ['Zuckerman Farms', 'FS' , 'Ryan Jones', 'JJB Farms', 'BT', 'Hughes', 'Carvalho', 'Rossow',\r\n#                       'Andrew', 'Lucero Hanford', 'Lucero Morada', 'Lucero 8 Mile', 'Oasis Organics']\r\n# serafinsGrowerList = ['OPC', 'David Santos', 'JHP', 'RKB Ranch', 'DCB', 'Bone Farms LLC']\r\n# adrianasGrowerList = ['Maricopa Orchards', 'OPC Independece Project', 'David Santos Independece Project', 'Lemonica',\r\n#                       'La Quinta']\r\n# northGrowerList = ['Matteoli', 'Tim Kalfsbeek', 'Barrios Farms', 'CM Ochoa', 'Schreiner Bros', 'Sam Reynolds',\r\n#                    'Faxon Farms' , 'Mumma Bros', 'Bullseye Farms' , 'CICC', 'Sean Doherty', 'Sycamore Marsh Farms',\r\n#                    'KTN JV', 'Knight Farms' , 'Dougherty Bros', 'JK Vineyards' , 'Kidwell Farms',\r\n#                    'UC Davis', 'TP', 'Quad H', 'Muller Ag LLC', 'Shiraz Ranch']\r\n# developmentList = ['Saul']\r\n#\r\n# vanessa = Technician('Vanessa', 'vcastillo@morningstarco.com')\r\n# serafin = Technician('Serafin', 'scolse@morningstarco.com')\r\n# adriana = Technician('Adriana', 'garciaa@morningstarco.com')\r\n# exsaelth = Technician('Exsaelth', 'ejimenez@morningstarco.com')\r\n# development_test_tech = Technician('Development Test Tech', 'jgarrido@morningstarco.com')\r\n#\r\n# for g in growers:\r\n#     print(g.name)\r\n#     if g.name in vanessasGrowerList:\r\n#         print('Assigned to Vanessa')\r\n#         g.technician = vanessa\r\n#         g.region = 'South'\r\n# #\r\n#     elif g.name in serafinsGrowerList:\r\n#         print('Assigned to Serafin')\r\n#         g.technician = serafin\r\n#         g.region = 'South'\r\n#\r\n#     elif g.name in adrianasGrowerList:\r\n#         print('Assigned to Adriana')\r\n#         g.technician = adriana\r\n#         g.region = 'South'\r\n#\r\n#     elif g.name in northGrowerList:\r\n#         print('Assigned to Exsaelth')\r\n#         g.technician = exsaelth\r\n#         g.region = 'North'\r\n#\r\n#     elif g.name in developmentList:\r\n#         print('Assigned to Dev')\r\n#         g.technician = development_test_tech\r\n#         g.region = 'South'\r\n# #\r\n#     else:\r\n#         print(\"{0} has no tech assigned\".format(g.name))\r\n#     print()\r\n# write_pickle(growers)\r\n# counter = 0\r\n# growers = open_pickle()\r\n# for g in growers:\r\n#     for f in g.fields:\r\n#         for l in f.loggers:\r\n#             counter = counter + 1\r\n#     # print(f'Grower: {g.name}')\r\n#     # print(f'Tech: {g.technician}')\r\n#     # print()\r\n#     # counter = counter + 1\r\n# print(counter)\r\n#\r\n#     if g.name == 'Lucero 8 Mile':\r\n#         g.technician = 'Vanessa'\r\n# write_pickle(growers)\r\n\r\n\r\n# showGrower('OPC')\r\n\r\n\r\n# setCimisStation('DCBChapman', 148)\r\n# setPlantingDate('Lucero 8 Mile', 'Lucero 8 Mile12-13', 2021, 6, 30)\r\n# growers = open_pickle()\r\n# for g in growers:\r\n#     print(\"{0} is covered by {1}\".format(g.name, g.technician))\r\n#     print(\"{0} region is {1}\".format(g.name, g.region))\r\n\r\n\r\n#     for f in g.fields:\r\n#         for l in f.loggers:\r\n#             if hasattr(l, \"crashed\"):\r\n#                 if l.crashed == True:\r\n#                     print(\"Crashed: {0}\".format(l.id))\r\n#                     loggerCount = loggerCount + 1\r\n#             if l.updated == False:\r\n#                 print(\"Not updated: {0}\".format(l.id))\r\n#                 loggerCount = loggerCount + 1\r\n# showGrower('BT')\r\n# growers = open_pickle()\r\n# for g in growers:\r\n#     if g.name == 'La Quinta':\r\n#         for f in g.fields:\r\n#             print('Field name:{0}'.format(f.name))\r\n#             print('Cimis station: {0}'.format(f.cimisStation))\r\n#             f.cimisStation = '218'\r\n# #             for l in f.loggers:\r\n# #                 print('Crop type: {0}'.format(l.cropType))\r\n# write_pickle(growers)\r\n# get_all_current_cimis_stations()\r\n# writeETValuesSpecificStation('12/31/2021','12/31/2021','148')\r\n# write_all_et_values_to_db('07/20/2022', '08/15/2022')\r\n# startDate = '01/01/2021'\r\n# endDate = '05/17/2021'\r\n\r\n# removeField('BT', 'BTSL01 ')\r\n#\r\n\r\n# showGrower('BT')\r\n# show_pickle()\r\n# print('Total commercial loggers installed: {0}'.format(loggerCount))\r\n# print('Total R&D loggers installed: {0}'.format(rndLoggerCount))\r\n\r\n# list = [\r\n#     'z6-01857',\r\n#     'z6-07165',\r\n#     'z6-01900',\r\n#     'z6-02017',\r\n#     'z6-07112',\r\n#     'z6-03443',\r\n#     'z6-01860',\r\n#     'z6-06002',\r\n#     'z6-05999',\r\n#     'z6-01898',\r\n#     'z6-03539',\r\n#     'z6-07265',\r\n#     'z6-07171',\r\n#     'z6-07270',\r\n# ]\r\n# reset_updated_all()\r\n# showGrower(\"La Quinta\")\r\n# show_pickle()\r\n# show_pickle()\r\n\r\n# growers = open_pickle()\r\n# print('Test')\r\n# for g in growers:\r\n#     if g.name == \"David Santos\":\r\n#         for f in g.fields:\r\n#             if f.name == 'David SantosFA1':\r\n#                 for l in f.loggers:\r\n#                     print('here')\r\n\r\n# onlyCertainGrowersFieldLoggerUpdate(\"David Santos\", \"David SantosFA1\", \"z6-01995\", specific_mrid=10803)\r\n# onlyCertainGrowersFieldLoggerUpdate(\"Bullseye Farms\", \"Bullseye FarmsMB6\", \"z6-02018\",  write_to_sheet=True,\r\n# write_to_db=True, write_to_portal_sheet=True) onlyCertainGrowersUpdate(['La Quinta'], get_weather=True,\r\n# get_data=True, write_to_db=True, write_to_sheet=True, write_to_portal_sheet=True)\r\n#\r\n# updatePrevSwitch(list, 60)\r\n# get_all_current_cimis_stations()\r\n# show_pickle()\r\n# showGrower('Knight Farms')\r\n# growers = open_pickle()\r\n# for g in growers:\r\n#     if g.updated == False:\r\n#         print(\"Grower {0} updated: {1}\".format(g.name, g.updated))\r\n#     for f in g.fields:\r\n#         if f.updated == False:\r\n#             print(\"\\tField {0} updated: {1}\".format(f.name, f.updated))\r\n#         for l in f.loggers:\r\n#             if l.updated == False:\r\n#                 print(\"\\t\\tLogger {0} updated: {1}\".format(l.name, l.updated))\r\n\r\n# write_pickle(growers)\r\n#                 if hasattr(l, \"crashed\"):\r\n#                     print(\"Logger: {0} has crashed: {1}\".format(l.id, l.crashed))\r\n#                 else:\r\n#                     print(\"Logger: {0} has no crashed attribute\")\r\n#\r\n#\r\n# if g.name == 'Sean Doherty':\r\n#     print(\"Grower: {0}\".format(g.name))\r\n#     for f in g.fields:\r\n#         # if f.name == 'Sean DohertyE1':\r\n#         print(\"\\tField {0} updated successfully? {1}\".format(f.name, f.updated))\r\n#         print(\"\\tNumber of loggers: {0}\".format(len(f.loggers)))\r\n#         f.checkSuccessfulUpdatedLoggers()\r\n#         print(\"\\tField {0} updated successfully? {1}\".format(f.name, f.updated))\r\n#         print()\r\n#     print(\"Grower {0} updated successfully? {1}\".format(g.name, g.updated))\r\n#     print(\"Number of fields: {0}\".format(len(g.fields)))\r\n#     g.checkSuccessfulUpdatedFields()\r\n#     print(\"Grower {0} updated successfully? {1}\".format(g.name, g.updated))\r\n#     print()\r\n# write_pickle(growers)\r\n\r\n\r\n# f.set_updated(False)\r\n# for l in f.loggers:\r\n#     l.set_updated(False)\r\n# write_pickle(growers)\r\n\r\n# onlyCertainGrowersFieldUpdate(\"DCB\", \"DCBNees 7-8\", get_et=False, get_weather=True, get_data=True,\r\n#                                       write_to_sheet=True, write_to_portal_sheet=True, write_to_db=True)\r\n# show_pickle()\r\n# showField('Bullseye FarmsFY1')\r\n\r\n# write_new_csv_to_db(\"Historical_ET\", \"114\", \"historicalET.csv\")\r\n# write_new_historical_et_to_db(\"114\", \"historicalET.csv\")\r\n\r\n# getMRID(\"2021-05-07\")\r\n# getPreviousDataField(\"Carvalho\", \"Carvalho304\", \"2021-05-07\", write_to_sheet=True)\r\n\r\n# onlyCertainGrowersFieldLoggerUpdate('Hughes', 'Hughes309-4', 'z6-07220', write_to_sheet=True, write_to_db=True,\r\n# subtract_from_mrid=96)\r\n\r\n# showField('Hughes309-4')\r\n\r\n# removeField('Hughes', 'Hughes234-2') removeGrower('LA QUINTA') removeField('Rossow', 'RossowWeholt')\r\n# onlyCertainGrowersUpdate(['KTN JV','Knight Farms','Andrew','Dougherty Bros','JK Vineyards','Kidwell Farms',\r\n# 'UC Davis', 'Zuckerman Farms', ], get_weather=True, get_data=True, write_to_db=True, write_to_portal_sheet=True,\r\n# write_to_sheet=True) onlyCertainGrowersFieldLoggerUpdate('JK Vineyards', 'JK VineyardsGemmer North', 'z6-02143',\r\n# write_to_sheet=True, write_to_portal_sheet=True, write_to_db=True) onlyCertainGrowersFieldLoggerUpdate('JHP',\r\n# 'JHPOakland 4', 'z6-01871', write_to_sheet=True, write_to_portal_sheet=True, write_to_db=True,\r\n# subtract_from_mrid=260) onlyCertainGrowersFieldLoggerUpdate('JHP', 'JHPOakland 4', 'z6-02026', write_to_sheet=True,\r\n# write_to_portal_sheet=True, write_to_db=True, subtract_from_mrid=260) onlyCertainGrowersFieldLoggerUpdate('JHP',\r\n# 'JHPOakland 4', 'z6-05964', write_to_sheet=True, write_to_portal_sheet=True, write_to_db=True,\r\n# subtract_from_mrid=260) onlyCertainGrowersFieldLoggerUpdate('JHP', 'JHPOakland 4', 'z6-03419', write_to_sheet=True,\r\n# write_to_portal_sheet=True, write_to_db=True, subtract_from_mrid=260)\r\n\r\n# onlyCertainGrowersFieldLoggerUpdate('Rossow', 'RossowWeholt', 'z6-12308', write_to_sheet=True,\r\n# write_to_portal_sheet=True, write_to_db=True, subtract_from_mrid=340) onlyCertainGrowersFieldLoggerUpdate('Bone\r\n# Farms LLC', 'Bone Farms LLCF7', 'z6-03447', write_to_sheet=True, write_to_portal_sheet=True, write_to_db=True,\r\n# subtract_from_mrid=340)\r\n\r\n# onlyCertainGrowersFieldLoggerUpdate('Oasis Organics', 'Oasis OrganicsDessert Ranch 34', 'z6-01690',\r\n# write_to_sheet=True, write_to_portal_sheet=True, write_to_db=True, subtract_from_mrid=550)\r\n# onlyCertainGrowersFieldLoggerUpdate('Oasis Organics', 'Oasis OrganicsDessert Ranch 34', 'z6-02020',\r\n# write_to_sheet=True, write_to_portal_sheet=True, write_to_db=True, subtract_from_mrid=550)\r\n# onlyCertainGrowersFieldLoggerUpdate('OPC', 'OPC11-2', 'z6-07267', write_to_sheet=True, write_to_portal_sheet=True,\r\n# write_to_db=True, subtract_from_mrid=96)\r\n\r\n\r\n# getAcres('OPC4-1', 'z6-06002')\r\n\r\n# onlyCertainGrowersFieldUpdate(\"Schreiner Bros\", \"Schreiner Bros121\", get_weather=True, get_data=True,\r\n# write_to_sheet=True, write_to_portal_sheet=True, write_to_db=True) onlyCertainGrowersFieldLoggerUpdate(\"Andrew\",\r\n# \"Andrew3104\", \"z6-12300\", write_to_sheet=True, write_to_portal_sheet=True, write_to_db=True)\r\n# onlyCertainGrowersFieldLoggerUpdate(\"KTN JV\", \"KTN JVYA1\", \"z6-11489\", write_to_sheet=True)\r\n# onlyCertainGrowersFieldLoggerUpdate(\"KTN JV\", \"KTN JVYA1\", \"z6-11562\", write_to_sheet=True)\r\n# onlyCertainGrowersFieldLoggerUpdate(\"KTN JV\", \"KTN JVYA1\", \"z6-02049\", write_to_sheet=True)\r\n# onlyCertainGrowersFieldLoggerUpdate(\"KTN JV\", \"KTN JVYA1\", \"z6-01887\", write_to_sheet=True)\r\n# onlyCertainGrowersUpdate([\"Bullseye Farms\", \"CICC\", \"Sean Doherty\", \"Sycamore Marsh Farms\", \"FS\", \"KTN JV\",\r\n# \"Knight Farms\", \"Andrew\", \"Dougherty Bros\", \"JK Vineyards\", \"Kidwell Farms\", \"UC Davis\", \"Zuckerman Farms\"],\r\n# get_data=True,  write_to_sheet=True, write_to_db=True, get_weather=True, write_to_portal_sheet=True)\r\n# onlyCertainGrowersFieldUpdate('Knight Farms', 'ntosDS1', get_weather=True, get_data=True, write_to_db=True,\r\n# write_to_sheet=True, write_to_portal_sheet=True) write_new_csv_to_db() show_pickle()\r\n\r\n# onlyCertainGrowersFieldUpdate() onlyCertainGrowersFieldLoggerUpdate('OPC', \"OPC14-1\", \"z6-02024\",\r\n# write_to_portal_sheet=True, write_to_sheet=True, write_to_db=True)\r\n\r\n# grower = setupGrower('Saul', '1S8acM-DKwrMaxOZpEdndvHn59xij7OHxtB6ztErIxoY')\r\n# addGrowerToGrowers(grower)\r\n#\r\n# field = setupField('Meza', '1TtZQM1GU6h88P83U6EGEUojIResR5gSyBBC7SR6gQZc', '37.053941099999996',\r\n# '-120.80917459999999', '56') logger1 = setupLogger('z6-07275', '65299-46534', 'Development', 1665227864, 'Tomato',\r\n# 36, 22, 1200, 100) field.addLogger(logger1) addFieldToGrower('Saul', field)\r\n#\r\n# grower = setupGrower('Casey', '1S8acM-DKwrMaxOZpEdndvHn59xij7OHxtB6ztErIxoY')\r\n# addGrowerToGrowers(grower)\r\n#\r\n# field = setupField('Pistachios', '1TtZQM1GU6h88P83U6EGEUojIResR5gSyBBC7SR6gQZc', '37.053941099999996',\r\n# '-120.80917459999999', '56') logger1 = setupLogger('z6-02181', '18379-27319', 'Development2', 1096363282,\r\n# 'Pistachios', 36, 22) field.addLogger(logger1) addFieldToGrower('Casey', field)\r\n#\r\n# show_pickle()\r\n\r\n# c = CIMIS()\r\n# et = c.get_eto('15', '01/01/2021', '04/16/2021')\r\n# print(et)\r\n\r\n# update_information(get_weather=True, write_to_sheet=True)\r\n\r\n# removeGrower(\"Dougherty Bros\")\r\n# update_information()\r\n# removeGrower('Casey')\r\n# onlyCertainGrowersFieldsUpdate(['Hughes304-3'], get_et=True, write_to_db=True)\r\n# onlyCertainGrowersUpdate(['Saul'], get_data=True, write_to_sheet=True, write_to_db=True)\r\n# update_information(get_data=True, write_to_db=True)\r\n# update_information(get_et=True)\r\n#\r\n# show_pickle()\r\n# update_information(get_data=True, write_to_db=True)\r\n# growers = open_pickle()\r\n# for g in growers:\r\n#     if g.name == \"Oasis Organics\":\r\n#         for f in g.fields:\r\n#             for l in f.loggers:\r\n# newDate = datetime.datetime(2021, 3, 1).date()\r\n# l.planting_date = newDate\r\n# print(l.planting_date)\r\n# print(type(l.planting_date))\r\n# if f.name == \"Bullseye FarmsST2\":\r\n#     print(\"Old cimis staiton: \" + f.cimisStation)\r\n# f.cimisStation = \"226\"\r\n# print(\"New cimis station: \" + f.cimisStation)\r\n# write_pickle(growers)\r\n\r\n# for l in f.loggers:\r\n#     print(l.planting_date)\r\n#     if hasattr(l, 'gpm'):\r\n#         print('GPM: ' + str(l.gpm))\r\n#     else:\r\n#         print('No GPM')\r\n#     if hasattr(l, 'acres'):\r\n#         print('Acres: ' + str(l.acres))\r\n#     else:\r\n#         print('No Acres')\r\n# print()\r\n# update_information(get_et=True, write_to_db=True)\r\n# for g in growers:\r\n#     if g.name != 'Matteoli':\r\n#         g.region = \"South\"\r\n#     elif g.name  == 'Matteoli':\r\n#         g.region = \"North\"\r\n#     else:\r\n#         g.region = ''\r\n# write_pickle(growers)\r\n# show_pickle()\r\n#     for f in g.fields:\r\n#         for l in f.loggers:\r\n#             print(\"New Logger\")\r\n#             print(l.name)\r\n#             print(l.cropType)\r\n#             if l.fieldCapacity:\r\n#                 print(l.fieldCapacity)\r\n#             else:\r\n#                 print('            No FC')\r\n#             if l.wiltingPoint:\r\n#                 print(l.wiltingPoint)\r\n#             else:\r\n#                 print('           No WP')\r\n#             print()\r\n# if g.name == \"Hughes\":\r\n#         g.fields[0].loggers[0].setCropType(\"Pistachios\")\r\n#         g.fields[0].loggers[1].setCropType(\"Pistachios\")\r\n# write_pickle(growers)\r\n# show_pickle()\r\n# onlyCertainGrowersUpdate('Saul', get_data=True, write_to_db=True)\r\n# update_information(get_data=True, write_to_db=True)\r\n# update_information(all_params=True)\r\n\r\n# setup_grower('Javier Test', 'aaa', 'aaa', 'aaa', True)\r\n# show_pickle()\r\n\r\n# only_certain_growers_field_logger_update('Riley Chaney Farms', 'Riley Chaney Farms16', 'RC-16-E')\r\n\r\n# growers = open_pickle()\r\n# for grower in growers:\r\n#     print(grower.name)\r\n#     for field in grower.fields:\r\n#         if field.name == 'Bone FarmsN42 N43':\r\n#             for logger in field.loggers:\r\n#                 logger.gpm = float(1490)\r\n# write_pickle(growers)\r\n# remove_field('KTN JV', 'KTN JVYA1')\r\n# remove_field('CM Ochoa', 'CM OchoaL37')\r\n# remove_field('CM Ochoa', 'CM OchoaL35')\r\n#\r\n# new_year_pickle_cleanup()\r\n\r\n\r\n# deactivate_grower('S&S Ranch'\r\n# dbw = DBWriter()\r\n# dbw.list_datasets('growers-2024')\r\n# client = dbw.grab_bq_client('growers-2024')\r\n# service_email = client.get_service_account_email()\r\n# print(service_email)\r\n# deactivate_grower('S&S Ranch')\r\n\r\n\r\n# remove_inactive_growers_from_pickle()\r\n# remove_inactive_fields_from_growers_from_pickle()\r\n\r\n# only_certain_growers_update(['Bone Farms'], get_weather=True, get_data=True, write_to_portal=True, write_to_db=True, subtract_from_mrid=216)\r\n# remove_grower('Bone Farms')\r\n# only_certain_growers_field_update('Barrios Farms', 'Barrios Farms25W', True, True, True, True)\r\n\r\n# show_pickle()\r\n\r\n\r\n# growers = open_pickle()\r\n# for grower in growers:\r\n#     for field in grower.fields:\r\n#         for logger in field.loggers:\r\n#             print(f'Field {field.name}')\r\n#             print('BEFORE')\r\n#             logger.show_irrigation_ledger()\r\n#             print('CLEANING...')\r\n#             logger.cwsi_processor.clean_irrigation_ledger(logger.irrigation_ledger)\r\n#             print('AFTER')\r\n#             logger.show_irrigation_ledger()\r\n#             print()\r\n# write_pickle(growers)\r\n\r\n# show_pickle()\r\n\r\n# only_certain_growers_field_update('Barrios Farms', 'Barrios Farms25W', get_data=True, write_to_db=True, write_to_portal=True, subtract_from_mrid=48)\r\n# only_certain_growers_update(['Lucero Kern'], get_weather=True, get_data=True, write_to_db=True, write_to_portal=True)\r\n\r\n# setup_logger()\r\n# growers = open_pickle()\r\n# cimis_stations_pickle = open_pickle(filename=\"cimisStation.pickle\")\r\n# for grower in growers:\r\n#     for field in grower.fields:\r\n#         for logger in field.loggers:\r\n#             if logger.name == 'BF-25W-C':\r\n#                 print(logger)\r\n#                 logger.id = 'z6-11954'\r\n#                 logger.password = '91051-01535'\r\n#                 logger.updated = False\r\n#                 logger.update(cimis_stations_pickle, write_to_db=True)\r\n\r\n# show_pickle()\r\n# only_certain_growers_update(['Seasholtz'], get_weather=True, get_data=True, write_to_portal=True, write_to_db=True, subtract_from_mrid=200)\r\n# show_field('Barrios Farms25W')\r\n# remove_grower('Mike Silva Farms')\r\n# only_certain_growers_field_logger_update('Barrios Farms', 'Barrios Farms36W', 'BF-36W-C', write_to_db=True)\r\n# pull_all_et_values('2024-01-01', '2024-04-01')\r\n# cimis = CIMIS()\r\n# cimis.get_eto(['105', '105','105','105','105','105','105','105','105','105','105','105','105','105','105','105','105','105'],'2024-01-01', '2024-04-01')\r\n# cimis.get_eto(['105'],'2024-01-01', '2024-04-01')\r\n# update_all_eto_values('2024-01-01', '2024-04-01')\r\n# print()\r\n# remove_grower('Seasholtz')\r\n# growers = open_pickle()\r\n# for grower in growers:\r\n#     if grower.technician.name == 'Vanessa':\r\n#         print()\r\n#         # grower.technician.email.append('acaposella@morningstarco.com')\r\n#         # grower.technician.email.append('frankielozano89@gmail.com')\r\n#         break\r\n#\r\n# print()\r\n# write_pickle(growers)\r\n\r\n# show_pickle()\r\n\r\n# remove_grower('Tos Farms')\r\n\r\n# all_techs = get_all_technicians(growers)\r\n# print()\r\n\r\n# only_certain_growers_field_update('Lucero Kern', 'Lucero Kern212', get_weather=True, get_data=True, write_to_db=True, write_to_portal=True)\r\n\r\n# start_date = datetime(2024, 3, 26).date()\r\n# end_date = datetime(2024, 3, 28).date()\r\n# update_et_information(get_et=True, write_to_db=True, start_date=start_date, end_date=end_date)\r\n\r\n# show_pickle()\r\n# only_certain_growers_field_logger_update('Seasholtz', 'SeasholtzOnion', 'SH-ON-NE', write_to_db=True, subtract_from_mrid=48)\r\n\r\n# remove_field('Dwelley Farms', 'Dwelley FarmsLadd 4')\r\n\r\n# remove_grower('Etchegary Farms')\r\n#\r\n\r\n# growers = open_pickle()\r\n# cimis_stations_pickle = open_pickle(filename=\"cimisStation.pickle\")\r\n\r\n# for grower in growers:\r\n#     for field in grower.fields:\r\n#         print(field.name, field.cimis_station)\r\n\r\n# for station in cimis_stations_pickle:\r\n#     print(station.station_number, station.active)\r\n#     if station.station_number == '7':\r\n#         station.active = False\r\n# write_pickle(cimis_stations_pickle, filename='cimisStation.pickle')\r\n\r\n# cimis = CIMIS()\r\n# cimis.get_eto(['105'],'2024-01-01', '2024-04-01')\r\n\r\n# cimis_stations_pickle = open_pickle(filename=\"cimisStation.pickle\")\r\n# print()\r\n# growers = open_pickle()\r\n# for grower in growers:\r\n#     for field in grower.fields:\r\n#         if field.name == 'Lucero BakersfieldGuitar':\r\n#             field.nickname = 'Giumarra'\r\n#             print()\r\n# write_pickle(growers)\r\n\r\n# remove_field('Dwelley Farms', 'Dwelley FarmsLadd 4')\r\n\r\n# growers = open_pickle()\r\n# for grower in growers:\r\n#     for field in grower.fields:\r\n#         if field.crop_type.lower() in ['almonds', 'almond']:\r\n#             for logger in field.loggers:\r\n#                 if not logger.ir_active:\r\n#                     # print(f'Grower: {grower.name}')\r\n#                     # print(f'Field: {field.nickname}')\r\n#                     should_be_on = True\r\n#                     for psi, _ in logger.consecutive_ir_values:\r\n#                         if psi > 0.5:\r\n#                             # print(f'\\tFailed on: {psi}')\r\n#                             should_be_on = False\r\n#                             break\r\n#                         latest_psi = psi\r\n#                     if not should_be_on:\r\n#                         formatted_consecutive_psi = [round(tup[0], 2) for tup in logger.consecutive_ir_values]\r\n#                         print(f'Field: {field.nickname}      Logger: {logger.name}')\r\n#                         print(f'Last 3 PSIs: {formatted_consecutive_psi}')\r\n#                         print(f'https://www.google.com/maps/search/?api=1&query={logger.lat},{logger.long}')\r\n#                         # print(f'{logger.lat}, {logger.long}')\r\n#                         print()\r\n\r\n# show_pickle()\r\n# only_certain_growers_fields_update(['RKB Farms24 27 28 29', 'Riverbend Farms10 11'], get_weather=True, get_data=True, write_to_db=True, write_to_portal=True)\r\n# only_certain_growers_update(['Riverbend Farms'], get_weather=True, get_data=True, write_to_db=True, write_to_portal=True)\r\n\r\n# growers = open_pickle()\r\n# for grower in growers:\r\n#     for field in grower.fields:\r\n#         for logger in field.loggers:\r\n#             if logger.name == 'JD-L4-SE':\r\n#                 print('b4', logger.name, logger.soil.soil_type)\r\n#                 logger.soil.set_soil_type('Sandy Loam')\r\n#                 print('after', logger.name, logger.soil.soil_type)\r\n# write_pickle(growers)\r\n\r\n# cimis_stations_pickle = open_pickle(filename='cimisStation.pickle', specific_file_path=\"H:\\\\Shared drives\\\\Stomato\\\\2023\\\\Pickle\\\\\")\r\n# print()\r\n# cimis_station = CimisStation('139999')\r\n# cimis_stations_pickle.append(cimis_station)\r\n# write_pickle(cimis_stations_pickle, filename='cimisStation.pickle', specific_file_path=\"H:\\\\Shared drives\\\\Stomato\\\\2023\\\\Pickle\\\\\")\r\n# print('decagon station running')\r\n# print('decagon station running outside')\r\n# def main():\r\n#     print('decagon station running inside')\r\n#\r\n# if __name__ == \"__main__\":\r\n#     main()\r\n\r\n# only_certain_growers_update(['Riley Chaney Farms'], get_data=True, write_to_portal=True, subtract_from_mrid=24)\r\n# remove_field('CM Ochoa', 'CM OchoaL36')\r\n# show_pickle()\r\n\r\n\r\n\r\n# growers = open_pickle()\r\n# for grower in growers:\r\n#     for field in grower.fields:\r\n#         if field.field_type == 'R&D':\r\n#             print(field.name)\r\n# show_pickle()\r\n#     if grower.name == 'Greenworks Farms':\r\n#         for field in grower.fields:\r\n#             for logger in field.loggers:\r\n#                 if logger.name == 'GW-WL1-NW':\r\n#                     print(f'Setting nickname for {logger.name}')\r\n#                     logger.nickname = 'Set 2'\r\n#                 if logger.name == 'GW-WL1-SE':\r\n#                     print(f'Setting nickname for {logger.name}')\r\n#                     logger.nickname = 'Set 5'\r\n#                 if logger.name == 'GW-WL2-NW':\r\n#                     print(f'Setting nickname for {logger.name}')\r\n#                     logger.nickname = 'Set 7'\r\n#                 if logger.name == 'GW-WL2-SE':\r\n#                     print(f'Setting nickname for {logger.name}')\r\n#                     logger.nickname = 'Set 6'\r\n#                 # if logger.name == 'GW-WL3-C':\r\n#                 #     logger.nickname = 'Set 3'\r\n# write_pickle(growers)\r\n\r\n# show_pickle()\r\n# only_certain_growers_field_update('Greenworks Farms', 'Greenworks FarmsWhite Legend 1', get_data=True, write_to_portal=True, subtract_from_mrid=48)\r\n\r\n# growers = open_pickle()\r\n# for grower in growers:\r\n#     tech = grower.technician\r\n#     if tech.name == 'Vanessa':\r\n#         # tech.email.append('jvasquez@morningstarco.com')\r\n#\r\n#         print()\r\n#         break\r\n\r\n# write_pickle(growers)\r\n# show_pickle()\r\n# only_certain_growers_field_logger_update('Barrios Farms', 'Barrios Farms22', 'BF-22-NW', write_to_db=True, specific_mrid=22831)\r\n# only_certain_growers_field_logger_update('Lucero Goosepond', 'Lucero Goosepond1', 'GP-1-SE', write_to_db=True, subtract_from_mrid=100)\r\n\r\n# growers = open_pickle()\r\n# for grower in growers:\r\n#     for field in grower.fields:\r\n#         for logger in field.loggers:\r\n#             if logger.name == 'SB-115-NE':\r\n#                 print(logger.name, logger.id)\r\n#                 logger.id = 'z6-07198'\r\n# write_pickle(growers)\r\n# remove_grower('Acemi Farms')\r\n# show_pickle()\r\n# growers = open_pickle()\r\n# for grower in growers:\r\n#     if grower.name == 'Matteoli Bros':\r\n#         for field in grower.fields:\r\n#             if field.name == 'Matteoli BrosJR1':\r\n#                 print(field.name, field.nickname)\r\n#                 field.nickname = 'RJ1'\r\n#                 print(field.name, field.nickname)\r\n# write_pickle(growers)\r\n#     if grower.region ==  '':\r\n#         print(grower)\r\n# print()\r\n# only_certain_growers_field_logger_update('Schreiner Bros', 'Schreiner Bros115', logger_id='z6-23247', write_to_db=True)\r\n\r\n# show_pickle()\r\n\r\n# only_certain_growers_field_update('Lucero Nees', 'Lucero NeesField 9', get_data=True, write_to_portal=True, subtract_from_mrid=48)\r\n\r\n# test_date = datetime(2024, 1, 1).date()\r\n# print(test_date)\r\n# growers = open_pickle()\r\n# for grower in growers:\r\n#     print()\r\n#     for field in grower.fields:\r\n#         for logger in field.loggers:\r\n#             print(logger.field.name, logger.name)\r\n#             # print(logger.consecutive_ir_values)\r\n#             new_tuples = deque()\r\n#             for tup in logger.consecutive_ir_values:\r\n#                 print(f'\\t{len(tup)} VALUES')\r\n#                 if tup and len(tup) < 3:\r\n#                     print('ADDED')\r\n#                     tup = tup + (test_date,)\r\n#                     print(tup)\r\n#                 new_tuples.append(tup)\r\n#             logger.consecutive_ir_values = new_tuples\r\n#\r\n#             # print(logger.consecutive_ir_values)\r\n#         print()\r\n# print()\r\n# write_pickle(growers)\r\n\r\n# show_pickle()\r\n#\r\n# only_certain_growers_field_update('Lucero Nees', 'Lucero NeesField 1', get_data=True, subtract_from_mrid=72)\r\n# only_certain_growers_field_update('Lucero Farms CICC', 'Lucero Farms CICCWT1', get_data=True, subtract_from_mrid=72)\r\n\r\n# show_pickle()\r\n\r\n# growers = open_pickle()\r\n# for grower in growers:\r\n#     if grower.name == 'Knight Farms':\r\n#         for field in grower.fields:\r\n#             if field.name == 'Knight FarmsB4':\r\n#                 for logger in field.loggers:\r\n#                     if logger.id == 'z6-07324':\r\n#                         logger.broken = True\r\n#                         logger.active = False\r\n#                         un_date = datetime(2024, 6, 14).date()\r\n#                         logger.uninstall_date = un_date\r\n#\r\n# write_pickle(growers)\r\n\r\n# show_pickle()\r\n\r\n# growers = open_pickle()\r\n# for grower in growers:\r\n#     for field in grower.fields:\r\n#         if field.cimis_station == '146':\r\n#             print(field.name)\r\n\r\n# cimisst = CimisStation()\r\n# cimisst.deactivate_cimis_station('5')\r\n\r\n# only_certain_growers_update('Kubo & YoungKF1', get_weather=True, get_data=True, write_to_db=True, write_to_portal=True)\r\n# only_certain_growers_field_update('Kubo & Young', 'Kubo & YoungKF1', get_weather=True, get_data=True, write_to_db=True, write_to_portal=True)\r\n\r\n\r\n# show_pickle()\r\n# growers = open_pickle()\r\n# techs = []\r\n# for grower in growers:\r\n#     if grower.technician not in techs:\r\n#         techs.append(grower.technician)\r\n# print('ALL TECHS')\r\n# print(techs)\r\n\r\n#\r\n#\r\n# for grower in growers:\r\n#     print(grower.name)\r\n#     print(grower.technician.name)\r\n#     print(grower.technician)\r\n#     print()\r\n#\r\n# for grower in growers:\r\n#     if grower.technician.name == \"Vanessa\":\r\n#         vane = grower.technician\r\n#         break\r\n# for grower in growers:\r\n#     if grower.technician.name == \"Exsaelth\":\r\n#         ex = grower.technician\r\n#         break\r\n#\r\n# print(vane)\r\n# print(ex)\r\n# print('AFTER SET')\r\n#\r\n# for grower in growers:\r\n#     if grower.technician.name == \"Vanessa\":\r\n#         grower.technician = vane\r\n#     if grower.technician.name == \"Exsaelth\":\r\n#         grower.technician = ex\r\n#\r\n# for grower in growers:\r\n#     print(grower.name)\r\n#     print(grower.technician.name)\r\n#     print(grower.technician)\r\n#     print()\r\n\r\n# techs = []\r\n# for grower in growers:\r\n#     if grower.technician not in techs:\r\n#         techs.append(grower.technician)\r\n# print('ALL TECHS')\r\n# print(techs)\r\n\r\n\r\n# write_pickle(growers)\r\n# show_pickle()\r\n\r\n\r\n\r\n\r\n# techs = get_all_technicians(growers)\r\n# for t in techs:\r\n#     print(t, t.name, t.growers)\r\n# print()\r\n\r\n# remove_last_grower()\r\n# remove_grower(\"TEST\")
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/Decagon.py b/Decagon.py
--- a/Decagon.py	(revision 5cf7d78a1d94023d00661edf5517badacc064130)
+++ b/Decagon.py	(date 1720742442742)
@@ -4,6 +4,7 @@
 """
 import csv
 import datetime
+import json
 import math
 import pickle
 import time
@@ -27,7 +28,7 @@
 from Field import Field
 from Grower import Grower
 from IrrigationRecommendationExpert import IrrigationRecommendationExpert
-from Logger import Logger
+from Logger import Logger, DXD_DIRECTORY
 from Saulisms import Saulisms
 from SwitchTestCase import SwitchTestCase
 from Technician import Technician
@@ -3313,168 +3314,6 @@
     write_pickle(growers)
 
 
-
-# generate_gradient_grower_fields_report()
-# show_pickle()
-
-# growers = open_pickle()
-#
-# for grower in growers:
-#     if grower.name == 'Riley Chaney Farms':
-#         for field in grower.fields:
-#             print(f'Field: {field.name}')
-#             for logger in field.loggers:
-#                 print(f'\tLogger: {logger.name}  |  GPM: {logger.gpm}  |  Irrigation Set Acres: {logger.irrigation_set_acres}')
-#
-# update_grower_field_gpm_and_irrigation_set_acres('Riley Chaney Farms', 'Riley Chaney Farms18', gpm=380)
-# update_grower_field_gpm_and_irrigation_set_acres('Riley Chaney Farms', 'Riley Chaney Farms4W', gpm=520)
-# update_grower_field_gpm_and_irrigation_set_acres('Riley Chaney Farms', 'Riley Chaney Farms16', gpm=650)
-# update_grower_field_gpm_and_irrigation_set_acres('Riley Chaney Farms', 'Riley Chaney Farms4E', gpm=1000)
-# update_grower_field_gpm_and_irrigation_set_acres('Riley Chaney Farms', 'Riley Chaney Farms27', gpm=1270)
-#
-#
-# for grower in growers:
-#     if grower.name == 'Riley Chaney Farms':
-#         for field in grower.fields:
-#             print(f'Field: {field.name}')
-#             for logger in field.loggers:
-#                 print(f'\tLogger: {logger.name}  |  GPM: {logger.gpm}  |  Irrigation Set Acres: {logger.irrigation_set_acres}')
-
-# show_pickle()
-# growers = open_pickle()
-# for grower in growers:
-#     if grower.name == 'Knight Farms':
-#         for field in grower.fields:
-#             if field.name == 'Knight FarmsNC1':
-#                 for logger in field.loggers:
-#                     logger.ir_active = True
-# write_pickle(growers)
-
-# remove_grower('Riley Chaney Farms')
-# remove_field('Riley Chaney Farms', 'Riley Chaney Farms13')
-# remove_field('Riley Chaney Farms', 'Riley Chaney Farms3')
-# remove_field('Riley Chaney Farms', 'Riley Chaney Farms15')
-# remove_field('Riley Chaney Farms', 'Riley Chaney Farms24')
-# remove_field('Riley Chaney Farms', 'Riley Chaney Farms27')
-
-# generate_gradient_grower_fields_report()
-
-# setup_weather_stations()
-
-# pickle_file_name = 'weather_stations_2023.pickle'
-# # old_pickle_file_name = 'weather_stations_2023 OLD BEFORE CLASS CHANGE.pickle'
-# pickle_file_path = 'H:\\Shared drives\\Stomato\\HeatUnits\\Pickles\\'
-# # old_weather_stations = open_pickle(filename=old_pickle_file_name, specific_file_path=pickle_file_path)
-# weather_stations = open_pickle(filename=pickle_file_name, specific_file_path=pickle_file_path)
-# for station in weather_stations:
-#     if station.name == 'Ben Fast':
-#         station.id = 'z6-15905'
-# write_pickle(weather_stations, filename=pickle_file_name, specific_file_path=pickle_file_path)
-# print()
-
-# varieties = Varieties()
-# # varieties.show_all_varieties()
-# print(varieties.get_variety_days_to_harvest('1996'))
-
-# temp_ai_application()
-
-# cimis = CIMIS()
-# cimis_station = CimisStation()
-# inactive_cimis_station_list = cimis_station.return_inactive_cimis_stations_list()
-# closest_cimis_station = cimis.get_closest_cimis_station(37.8806321, -121.1559947, [])
-# print(closest_cimis_station)
-
-# compare_new_psi_algo_vs_()
-# show_pickle()
-# only_certain_growers_update(['Riley Chaney Farms'], get_weather=True, get_data=True, write_to_db=True, write_to_portal=True)
-# only_certain_growers_field_logger_update('Riley Chaney Farms', 'Riley Chaney Farms3', 'RC-3-NE', write_to_db=True)
-
-# show_grower('Riley Chaney Farms')
-
-# growers = open_pickle()
-# for grower in growers:
-#     if grower.name == 'Riley Chaney Farms':
-#         print(grower.name, grower.active)
-#         for field in grower.fields:
-#             # print('\t', field.name, 'Active: ', field.active)
-#             print(field)
-#             for logger in field.loggers:
-#                 print('\t',logger.id, logger.name)
-
-# growers = open_pickle()
-# for grower in growers:
-#     if grower.name == 'Riley Chaney Farms':
-#         for field in grower.fields:
-#             if field.name == 'Riley Chaney Farms3':
-#                 for logger in field.loggers:
-#                     if logger.id == 'z6-11928':
-#                         print(logger.id, logger.password)
-#                         logger.password = '12608-28613'
-#                         print(logger.id, logger.password)
-# write_pickle(growers)
-
-# new_year_pickle_cleanup()
-
-# test_switch_cases()
-# temp_ai_application()
-
-
-
-# setup_ai_game_data(
-#     "2022_pickle.pickle",
-#     "H:\\Shared drives\\Stomato\\2022\\Pickle\\",
-#     "2022_ai_game_data.pickle",
-#     "H:\\Shared drives\\Stomato\\AIIrrigationGame\\"
-# )
-
-# temp_ai_application()
-
-# show_pickle()
-# growers = open_pickle()
-# for grower in growers:
-#     print(grower.name)
-# print()
-# remove_grower('Dwelley Farms')
-
-# show_pickle()
-
-# show_pickle()
-# cimis_stations_pickle = open_pickle(filename="cimisStation.pickle")
-# print()
-
-# growers = open_pickle()
-# for grower in growers:
-#     print(grower.name)
-
-# only_certain_growers_update(['Dwelley Farms'], get_weather=True, get_data=True, write_to_portal=True, write_to_db=True)
-# only_certain_growers_field_update('Dwelley Farms', 'Dwelley FarmsTKs', get_weather=True, get_data=True, write_to_portal=True, write_to_db=True)
-
-
-# show_grower('Riley Chaney Farms')
-
-# growers = open_pickle()
-# for grower in growers:
-#     if grower.name == 'Riley Chaney Farms':
-#         print()
-#         for field in grower.fields:
-#             if field.name == 'Riley Chaney FarmsRN 2 LLC':
-#                 print(field.name, field.nickname)
-#                 field.nickname = 'R&N 2 LLC'
-#                 print(field.name, field.nickname)
-# write_pickle(growers)
-
-# only_certain_growers_field_update('Barrios Farms', 'Barrios Farms22', get_weather=True, get_data=True, write_to_db=True)
-
-# growers = open_pickle()
-# for grower in growers:
-#     if grower.name == 'Barrios Farms':
-#         print()
-        # for field in grower.fields:
-        #     if field.name == 'Riley Chaney FarmsRN 2 LLC':
-        #         print(field.name, field.nickname)
-        #         field.nickname = 'R&N 2 LLC'
-        #         print(field.name, field.nickname)
-
 def update_historical_et_stations():
     """
     Update the Historical ET table for each station with the new years etos from our BQ
@@ -3551,1299 +3390,47 @@
     pass
 
 
-
-
-# show_pickle()
-
-# growers = open_pickle()
-# for grower in growers:
-#     for field in grower.fields:
-#         for logger in field.loggers:
-#             old_opt_low = logger.soil.optimum_lower
-#             old_opt_high = logger.soil.optimum_upper
-#             new_soil = Soil(field_capacity=logger.soil.field_capacity, wilting_point=logger.soil.wilting_point)
-#             logger.soil = new_soil
-#             new_soil_low = new_soil.optimum_lower
-#             if old_opt_low != new_soil_low:
-#                 print(f'Old optimum: {old_opt_low} - {old_opt_high}')
-#                 print(f'New optimum: {new_soil_low} - {new_soil.optimum_upper}')
-#                 print()
-#
-# write_pickle(growers)
-
-
-
-
-# show_pickle()
-
-
-# new_year_pickle_cleanup()
-# remove_inactive_fields_from_growers_from_pickle()
-
-
-# growers = open_pickle()
-# for grower in growers:
-#     for field in grower.fields:
-#         for logger in field.loggers:
-#             if logger.name == 'LM-17J-S':
-#                 logger.show_irrigation_ledger()
-#                 for date, switch_list in logger.irrigation_ledger.items():
-#                     for ind, switch_val in enumerate(switch_list):
-#                         if switch_val == 15:
-#                             logger.irrigation_ledger[date][ind] = 60
-#                 print()
-#                 logger.show_irrigation_ledger()
-# write_pickle(growers)
-# print()
-
-
-# growers = open_pickle()
-# for grower in growers:
-#     print(grower)
-#
-# cimis_stations_pickle = open_pickle(filename="cimisStation.pickle")
-#
-# pickle_file_name = 'weather_stations_2023.pickle'
-# pickle_file_path = 'H:\\Shared drives\\Stomato\\HeatUnits\\'
-# weather_stations = open_pickle(filename=pickle_file_name, specific_file_path=pickle_file_path)
-
-
-
-
-
-# weather_stations = open_pickle(filename=pickle_file_name, specific_file_path=pickle_file_path)
-
-# only_certain_growers_field_logger_update('Barrios Farms', 'Barrios Farms25E', 'BF-25E-C', write_to_db=True, subtract_from_mrid=62)
-
-# dbwriter = DBWriter()
-# db_dates = dbwriter.grab_specific_column_table_data('Barrios_Farms25E', 'BF-25E-C', 'stomato-2023', 'date')
-# db_dates_list = [row[0] for row in db_dates]
-# print()
-
-# reset_updated_all()
-# update_information(get_et=True, write_to_db=True)
-
-
-
-# for grower in growers:
-    # print()
-# for grower in growers:
-#     for field in grower.fields:
-#         if field.name == 'Lucero Rio VistaB 17-22':
-#             for logger in field.loggers:
-#                 print(type(logger.daily_switch))
-#                 logger.gpm = 4488
-#                 logger.irrigation_set_acres = 215
-# #     print(logger.name, logger.soil.soil_type)
-#             #     logger.soil.set_soil_type('Sandy Clay Loam')
-#             #     print(logger.name, logger.soil.soil_type)
-# write_pickle(growers)
-
-# cimis = CIMIS()
-# # # cimisStation = CimisStation()
-# # # stations = get_all_current_cimis_stations()
-# # # stations = cimisStation.return_list_of_stations()
-# cimis_stations_pickle = open_pickle(filename="cimisStation.pickle")
-# # Convert the integers to strings
-# cimis_station_string_list = [str(station.station_number) for station in cimis_stations_pickle]
-#
-# # Join the strings with commas
-# cimis_stations_list_string = ', '.join(cimis_station_string_list)
-# print(cimis_stations_list_string)
-#
-# start_date = str(date.today() - timedelta(3))
-# end_date = str(date.today() - timedelta(1))
-# all_cimis_station_et = cimis.get_eto(cimis_stations_list_string, start_date, end_date)
-# print()
-# pprint.pprint(all_cimis_station_et)
-
-# show_pickle()
-
-# growers = open_pickle()
-# for grower in growers:
-#     print(grower.name)
-
-# Testing new single CIMIS API call
-# dict_of_stations = {}
-# all_current_cimis_stations = open_pickle(filename="cimisStation.pickle")
-# for station in all_current_cimis_stations:
-#     dict_of_stations[station.station_number] = {'station': station, 'dates': [], 'eto': []}
-# print()
-
-# ########################################
-# ### TESTING CIMIS CALL AT 12
-# cimis = CIMIS()
-# cimis_stations_pickle = open_pickle(filename="cimisStation.pickle")
-# print()
-# start_date = str(date.today() - timedelta(1))
-# end_date = str(date.today() - timedelta(1))
-# for stations in cimis_stations_pickle:
-#     stations.updated = False
-# all_cimis_station_et = cimis.get_all_stations_et_data(cimis_stations_pickle, start_date, end_date)
-# print()
-# #########################################
-# write_all_et_values_to_db(all_cimis_station_et)
-
-# print()
-
-# project = 'stomato-info'
-# dataset_id = f"{project}.ET.105"
-# dataset_id = "`" + dataset_id + "`"
-# #
-# dbwriter = DBWriter()
-# dml_statement = f"SELECT * FROM {dataset_id} WHERE date BETWEEN DATE(\'2023-03-01\') and DATE(\'2023-03-25\') ORDER BY date ASC"
-# result = dbwriter.run_dml(dml_statement, project=project)
-#
-# date_data = []
-# eto_data = []
-#
-# for row in result:
-#     date_data.append(row['date'])
-#     eto_data.append(row['eto'])
-# print()
-
-# dml = f"UPDATE `test.test.test`" \
-#       + f" SET daily_switch = 55"\
-#       + f", daily_hours = 0.8"\
-#       + f", daily_inches = 1.5 WHERE date = 'date'"
-#
-# print(dml)
-
-
-# growers = open_pickle()
-# for grower in growers:
-#     for field in grower.fields:
-#         for logger in field.loggers:
-#             logger.irrigation_ledger = {}
-# write_pickle(growers)
-# print()
-
-# for each_dict in dict_of_stations:
-#     print(f"Station: {dict_of_stations[each_dict]['station'].station_number}")
-# print()
-
-# my_list = ['pear']
-# if 'apple' not in my_list:
-#     print("apple not in list")
-
-# all_cimis_station_et = cimis.get_all_stations_et_data(cimis_stations_pickle, start_date, end_date)
-
-
-# show_pickle()
-# temp_ai_application()
-# Testing writing a html doc for notifications instead of a txt
-# lat = 37.0544503152434
-# long = -120.80964223605376
-# saulisms = Saulisms()
-# saying, saying_date = saulisms.get_random_saulism()
-# file_path = "C:\\Users\\javie\\Desktop\\notification_html_test.html"
-# with open(file_path, 'a') as the_file:
-#     the_file.write("<!DOCTYPE html>\n")
-#     the_file.write("<html>\n")
-#     the_file.write("<body>\n")
-#     the_file.write("<h2>SENSOR ERRORS</h2>\n")
-#     the_file.write(f"<h2 style='font-style: italic; font-size:150%;'>\"{saying}\", {saying_date}</h2>")
-#     the_file.write("<h3>=== New Grower ===</h3>\n")
-#     the_file.write("<p>-------------------</p>\n")
-#     the_file.write("<p>Field: Lucero Rio Vista74, 75</p>\n")
-#     the_file.write("<p>Logger: RV-74_75-SE</p>\n")
-#     the_file.write("<p>Logger ID: z6-12306</p>\n")
-#     the_file.write("<p>Date: 06/26/23</p>\n")
-#     the_file.write("<p>Sensor: Canopy Temp</p>\n")
-#     the_file.write("<p>-> Canopy Temp is showing None at some point in the day (Not necessarily at the hottest time). Connection issue?</p>\n")
-#     the_file.write(f"<a href='https://www.google.com/maps/search/?api=1&query={lat},{long}' target='_blank'>Location</a>\n")
-#     the_file.write("<p>-------------------</p>\n")
-#     the_file.write("</body>\n")
-#     the_file.write("</html>\n")
-
-# show_pickle()
-
-# Change Soil Type for a Logger
-# growers = open_pickle()
-# for grower in growers:
-#     for field in grower.fields:
-#         for logger in field.loggers:
-#             if logger.name == 'RC-24-SW':
-#                 print(logger.name, logger.soil.soil_type, logger.soil.field_capacity, logger.soil.wilting_point)
-#                 logger.soil.set_soil_type('Clay Loam')
-#                 print(logger.name, logger.soil.soil_type, logger.soil.field_capacity, logger.soil.wilting_point)
-#                 print('---------------------')
-#             if logger.name == 'RN-2LLC-E':
-#                 print(logger.name, logger.soil.soil_type, logger.soil.field_capacity, logger.soil.wilting_point)
-#                 logger.soil.set_soil_type('Clay Loam')
-#                 print(logger.name, logger.soil.soil_type, logger.soil.field_capacity, logger.soil.wilting_point)
-#                 print('---------------------')
-#             if logger.name == 'RN-2LLC-N':
-#                 print(logger.name, logger.soil.soil_type, logger.soil.field_capacity, logger.soil.wilting_point)
-#                 logger.soil.set_soil_type('Sandy Clay Loam')
-#                 print(logger.name, logger.soil.soil_type, logger.soil.field_capacity, logger.soil.wilting_point)
-#                 print('---------------------')
-#             if logger.name == 'RN-2LLC-W':
-#                 print(logger.name, logger.soil.soil_type, logger.soil.field_capacity, logger.soil.wilting_point)
-#                 logger.soil.set_soil_type('Sandy Clay Loam')
-#                 print(logger.name, logger.soil.soil_type, logger.soil.field_capacity, logger.soil.wilting_point)
-#                 print('---------------------')
-#             if logger.name == 'RC-16-E':
-#                 print(logger.name, logger.soil.soil_type, logger.soil.field_capacity, logger.soil.wilting_point)
-#                 logger.soil.set_soil_type('Clay Loam')
-#                 print(logger.name, logger.soil.soil_type, logger.soil.field_capacity, logger.soil.wilting_point)
-#                 print('---------------------')
-#             if logger.name == 'RC-13-E':
-#                 print(logger.name, logger.soil.soil_type, logger.soil.field_capacity, logger.soil.wilting_point)
-#                 logger.soil.set_soil_type('Clay Loam')
-#                 print(logger.name, logger.soil.soil_type, logger.soil.field_capacity, logger.soil.wilting_point)
-#                 print('---------------------')
-#             if logger.name == 'RC-4E-C':
-#                 print(logger.name, logger.soil.soil_type, logger.soil.field_capacity, logger.soil.wilting_point)
-#                 logger.soil.set_soil_type('Clay Loam')
-#                 print(logger.name, logger.soil.soil_type, logger.soil.field_capacity, logger.soil.wilting_point)
-#                 print('---------------------')
-#             if logger.name == 'RC-3-NE':
-#                 print(logger.name, logger.soil.soil_type, logger.soil.field_capacity, logger.soil.wilting_point)
-#                 logger.soil.set_soil_type('Sandy Loam')
-#                 print(logger.name, logger.soil.soil_type, logger.soil.field_capacity, logger.soil.wilting_point)
-#                 print('---------------------')
-#
-#
-# write_pickle(growers)
-
-# # Testing some notification generation and writing
-# growers = open_pickle()
-# techs = get_all_technicians(growers)
-# notifications_setup(growers, techs, file_type='html')
-# reset_updated_all()
-# update_information(get_data=True, check_for_notifications=True)
-
-# only_certain_growers_field_logger_update('Saul', 'Meza', 'Development-C', check_for_notifications=True, subtract_from_mrid=24)
-
-# growers = open_pickle()
-# for grower in growers:
-#     for field in grower.fields:
-#         for logger in field.loggers:
-#             print()
-#             print(f"https://www.google.com/maps/search/?api=1&query={logger.lat},{logger.long}")
-#             print()
-#         if field.crop_type not in ['Tomatoes', 'tomatoes', 'tomato', 'Tomato']:
-#             print(field.name, field.crop_type)
-#         if field.loggers[0].rnd:
-#             field.field_type = 'R&D'
-#         else:
-#             field.field_type = 'Commercial'
-# print(f'{field.name} - {field.field_type}')
-# write_pickle(growers)
-
-# print(field.name)
-# for logger in field.loggers:
-#     print(f'\t{logger.name} - {logger.rnd}')
-
-# df = pd.DataFrame(columns=['Gradient Grower', 'Gradient Grower-Field', 'Gradient Field', 'Gradient Planting Date', 'Gradient Uninstall Date'])
-#
-# pickle_file_name = "2022_pickle.pickle"
-# pickle_file_path = "H:\\Shared drives\\Stomato\\2022\\Pickle\\"
-# growers = open_pickle(filename=pickle_file_name, specific_file_path=pickle_file_path)
-# for grower in growers:
-#     for field in grower.fields:
-#         if field.crop_type in ['Tomatoes', 'tomatoes', 'tomato', 'Tomato']:
-#             df = df.append({'Gradient Grower': grower.name, 'Gradient Grower-Field': field.name, 'Gradient Field': field.nickname, 'Gradient Planting Date': field.loggers[0].planting_date, 'Gradient Uninstall Date': field.loggers[0].uninstall_date}, ignore_index=True)
-#
-# print()
-# print(df)
-# df.to_excel('growers_fields.xlsx', index=False)
-
-# pickle_file_name = "ai_game_2023_data_errors.pickle"
-# pickle_file_path = "C:\\Users\\javie\\Desktop\\AI Game v3 Distribution\\"
-# ai_data = open_pickle(filename=pickle_file_name, specific_file_path=pickle_file_path)
-# print()
-# ai_data = []
-#
-# write_pickle(ai_data, filename=pickle_file_name, specific_file_path=pickle_file_path)
-
-
-# pickle_file_name = "2022_pickle.pickle"
-# pickle_file_path = "H:\\Shared drives\\Stomato\\2022\\Pickle\\"
-# growers = open_pickle(filename=pickle_file_name, specific_file_path=pickle_file_path)
-# for grower in growers:
-#     for field in grower.fields:
-#         if field.loggers[0].rnd:
-#             field.field_type = 'R&D'
-#         else:
-#             field.field_type = 'Commercial'
-#         print(f'{field.name} - {field.field_type}')
-# write_pickle(growers, filename=pickle_file_name, specific_file_path=pickle_file_path)
-
-# pickle_file_name = "2022_pickle.pickle"
-# pickle_file_path = "H:\\Shared drives\\Stomato\\2022\\Pickle\\"
-# graph_yields(pickle_file_name, pickle_file_path, 'Paid')
-
-# pickle_file_name = "2022_pickle.pickle"
-# pickle_file_path = "H:\\Shared drives\\Stomato\\2022\\Pickle\\"
-# get_tomato_yield_data(pickle_file_name, pickle_file_path, '2022 Tomato Yield.xlsx', 0, 263)
-# show_specific_pickle(pickle_file_name, specific_file_path=pickle_file_path)
-
-# pickle_file_name = "2022_pickle.pickle"
-# pickle_file_path = "H:\\Shared drives\\Stomato\\2022\\Pickle\\"
-# growers = open_specific_pickle(pickle_file_name, specific_file_path=pickle_file_path)
-# for grower in growers:
-#     for field in grower.fields:
-#         print(field.name, field.loggers[0].uninstall_date)
-#         field.crop_type = field.loggers[0].cropType
-#         field.net_yield = None
-#         field.paid_yield = None
-# write_specific_pickle(growers, pickle_file_name, specific_file_path=pickle_file_path)
-# show_specific_pickle(pickle_file_name, specific_file_path=pickle_file_path)
-
-# pickle_file_name = "2022_pickle.pickle"
-# pickle_file_path = "H:\\Shared drives\\Stomato\\2022\\Pickle\\"
-# growers = open_specific_pickle(pickle_file_name, specific_file_path=pickle_file_path)
-# for grower in growers:
-#     for field in grower.fields:
-#         for logger in field.loggers:
-#             if not hasattr(logger, 'rnd'):
-#                 logger.crop_type = logger.cropType
-#                 print(logger.grower.name, logger.field.name, logger.name, logger.crop_type)
-#                 logger.rnd = False
-#                 if logger.name == 'Development-C':
-#                     print(logger.grower.name, logger.field.name, logger.name, logger.crop_type)
-#                     logger.rnd = True
-#                 # logger.rnd = True
-# write_specific_pickle(growers, pickle_file_name, specific_file_path=pickle_file_path)
-# print(datetime.now().date())
-
-# growers = open_pickle()
-# for grower in growers:
-#     for field in grower.fields:
-#         for logger in field.loggers:
-#             if logger.name == 'MB-52-NE':
-#                 logger.consecutive_ir_values = deque()
-#                 logger.consecutive_ir_values.append((8.212098906263984, 26.622))
-#             elif logger.name == 'MB-52-SW':
-#                 logger.consecutive_ir_values = deque()
-#                 logger.consecutive_ir_values.append((4.40852468436712, 10.89))
-# write_pickle(growers)
-
-# show_pickle()
+def change_mrid(logger_id: int, subtract_from_mrid: int = 0):
+    """
 
-# gpm = '1,200'
-# print(float(gpm))
+    :param logger_id:
+    :param subtract_from_mrid:
+    """
+    try:
 
+        file_path = path.join(DXD_DIRECTORY, logger_id + '.dxd')
 
-# def clean_list(list, remove_start, remove_end):
-#     clean = list[remove_start:len(list) - remove_end]
-#     print(clean)
-#
-# list = [1,2,3,4,5,6,7,8,9,10]
-# clean_list(list, 0, 0)
+        print(f'Changing dxd file MRID {file_path}...')
 
-# show_pickle()
+        if not path.isfile(file_path):
+            raise FileNotFoundError(f"The file '{file_path}' does not exist.")
 
-# only_certain_growers_field_logger_update('Matteoli Brothers', 'Matteoli Brothers50', 'MB-50-C', write_to_db=True)
+        with open(file_path, 'r', encoding="utf8") as fd:
+            parsed_json = json.load(fd)
 
-# growers = open_pickle()
-# for grower in growers:
-#     for field in grower.fields:
-#         if field.name == 'Schreiner115':
-#             for logger in field.loggers:
-#                 print(logger.soil.soil_type)
-#                 print(logger.soil.field_capacity)
-#                 print(logger.soil.wilting_point)
-#                 logger.soil.set_soil_type('Silt Loam')
-#                 print(logger.soil.soil_type)
-#                 print(logger.soil.field_capacity)
-#                 print(logger.soil.wilting_point)
-# write_pickle(growers)
-# logger.to_string()
-# logger.ir_active = True
-# logger.to_string()
-# write_pickle(growers)
-# pickle_file_name = "2022_pickle.pickle"
-# pickle_file_path = "H:\\Shared drives\\Stomato\\2022\\Pickle\\"
-# field_capacities = []
-# field_capacities_data = []
-# growers = open_specific_pickle(pickle_file_name, specific_file_path=pickle_file_path)
-# for grower in growers:
-#     for field in grower.fields:
-#         for logger in field.loggers:
-#             logger.field_capacity = logger.fieldCapacity
-#             if logger.field_capacity not in field_capacities:
-#                 percent = 0.15
-#                 fc_upper = round(logger.field_capacity + (logger.field_capacity * percent), 1)
-#                 fc_lower = round(logger.field_capacity - (logger.field_capacity * percent), 1)
-#                 field_capacities.append(logger.field_capacity)
-#                 field_capacities_data.append((logger.field_capacity, fc_upper, fc_lower))
-#
-# for fc_data in field_capacities_data:
-#     fc, fc_u, fc_l = fc_data
-#     print(f'Upper: {fc_u}')
-#     print(f'FC: {fc}')
-#     print(f'Lower: {fc_l}')
-#     print()
-# print(field_capacities)
+        if subtract_from_mrid > -1:
+            print(f'Modifying file MRID with the set back MRID: {subtract_from_mrid}')
+            if "created" in parsed_json:
+                # Modify the MRID to the new value
+                og_mrid = parsed_json['device']['timeseries'][-1]['configuration']['values'][-1][1]
+                new_mrid = og_mrid - subtract_from_mrid
+                if new_mrid < 0:
+                    new_mrid = 0
+                parsed_json['device']['timeseries'][-1]['configuration']['values'][-1][1] = new_mrid
 
-# all_acres = []
-# total_acres = 0
-# number_of_fields = 0
-# price_per_station = []
-# growers = open_pickle()
-# for grower in growers:
-#     for field in grower.fields:
-#         if field.crop_type in ['Tomatoes', 'Tomato', 'tomatoes', 'tomato']:
-#             if not field.loggers[0].rnd:
-#                 # print(type(field.acres))
-#                 total_acres += float(field.acres)
-#                 number_of_fields += 1
-#                 price_per_station.append(50 * float(field.acres) / len(field.loggers))
-#
-# # print(total_acres/number_of_fields)
-# print(price_per_station)
-# print(number_of_fields)
-#
-# print(sum(price_per_station)/len(price_per_station))
+        # Write the modified JSON back to the file
+        with open(file_path, 'w', encoding="utf8") as fd:
+            json.dump(parsed_json, fd, indent=4, sort_keys=True, default=str)
 
+        print('Successfully changed MRID')
 
-# pickle_file_name = "2022_pickle.pickle"
-# pickle_file_path = "H:\\Shared drives\\Stomato\\2022\\Pickle\\"
-# # field_capacities = []
-# # field_capacities_data = []
-# growers = open_specific_pickle(pickle_file_name, specific_file_path=pickle_file_path)
-# all_acres = []
-# total_acres = 0
-# number_of_fields = 0
-# price_per_station = []
-# # growers = open_pickle()
-# for grower in growers:
-#     for field in grower.fields:
-#         if field.crop_type in ['Tomatoes', 'Tomato', 'tomatoes', 'tomato']:
-#             if not field.loggers[0].rnd:
-#                 total_acres += float(field.acres)
-#                 number_of_fields += 1
-#                 price_per_station.append(50 * float(field.acres) / len(field.loggers))
-#
-# # print(total_acres/number_of_fields)
-# print(price_per_station)
-# total_price = 0
-# for station in price_per_station:
-#     total_price += station
-# print(total_price/len(price_per_station))
-# growers = open_pickle()
-# for grower in growers:
-#     for field in grower.fields:
-#         for logger in field.loggers:
-#             st = Soil(field_capacity=logger.soil.field_capacity, wilting_point=logger.soil.wilting_point)
-#             logger.soil = st
-#
-#             print(field.name, logger.name)
-#             print(f'Soil type: {logger.soil.soil_type}')
-#             print(f'Ranges: {logger.soil.bounds}')
-# write_pickle(growers)
-# print(logger.field.name, logger.name, logger.field_capacity, logger.wilting_point)
+    except Exception as error:
+        print(f'ERROR in changing dxd MRID file for json data for {logger_id}')
+        print(error)
 
-# pickle_file_name = "2022_pickle.pickle"
-# pickle_file_path = "H:\\Shared drives\\Stomato\\2022\\Pickle\\"
-# growers = open_pickle(filename=pickle_file_name, specific_file_path=pickle_file_path)
-# for grower in growers:
-#     for field in grower.fields:
-#         for logger in field.loggers:
-#             # logger.field_capacity = logger.fieldCapacity
-#             # logger.wilting_point = logger.wiltingPoint
-#             st = Soil(field_capacity=logger.soil.field_capacity, wilting_point=logger.soil.wilting_point)
-#             logger.soil = st
-# #
-#             print(field.name, logger.name)
-#             print(f'Soil type: {logger.soil.soil_type}')
-#             print(f'Ranges: {logger.soil.bounds}')
-# #             print()
-# write_pickle(growers, filename=pickle_file_name, specific_file_path=pickle_file_path)
-
-# growers = open_pickle()
-# for grower in growers:
-#     for field in grower.fields:
-#         for logger in field.loggers:
-#             soil_type = logger.soil.soil_type
-#             new_soil = Soil(soil_type=soil_type)
-#             logger.soil = new_soil
-# write_pickle(growers)
-
-# show_pickle()
-
-# pickle_file_name = "2022_pickle.pickle"
-# pickle_file_path = "H:\\Shared drives\\Stomato\\2022\\Pickle\\"
-# growers = open_specific_pickle(pickle_file_name, specific_file_path=pickle_file_path)
-# total_2022_fields = 0
-# total_2022_loggers = 0
-# for grower in growers:
-#     for field in grower.fields:
-#         total_2022_fields += 1
-#         for logger in field.loggers:
-#             total_2022_loggers += 1
-# print(f'Field #: {total_2022_fields}')
-# print(f'Logger #: {total_2022_loggers}')
-
-# pickle_file_name = "2023_pickle.pickle"
-# pickle_file_path = "H:\\Shared drives\\Stomato\\2023\\Pickle\\"
-# pickle_file_name = "entry.pickle"
-# pickle_file_path = "H:\\Shared drives\\Stomato\\2021\\Pickle\\"
-# pickle_file_name = "2022_pickle.pickle"
-# pickle_file_path = "H:\\Shared drives\\Stomato\\2022\\Pickle\\"
-# growers = open_specific_pickle(pickle_file_name, specific_file_path=pickle_file_path)
-# for grower in growers:
-#     for field in grower.fields:
-#         for logger in field.loggers:
-#             logger.install_date = None
-#             logger.broken = False
-#         field.crop_type = field.loggers[0].crop_type
-#         field.net_yield = None
-#         field.paid_yield = None
-# print('Done')
-# write_specific_pickle(growers, pickle_file_name, specific_file_path=pickle_file_path)
-# show_specific_pickle(pickle_file_name, specific_file_path=pickle_file_path)
-# show_pickle()
-# pickle_file_name = "entry.pickle"
-# pickle_file_path = "H:\\Shared drives\\Stomato\\2021\\Pickle\\"
-# info = {}
-# growers = open_specific_pickle(pickle_file_name, specific_file_path=pickle_file_path)
-# for grower in growers:
-#     for field in grower.fields:
-#         for logger in field.loggers:
-#             if 'z6' in logger.id:
-#                 if logger.id not in info:
-#                     info[logger.id] = []
-#                 info[logger.id].append(logger.field.name + ' -> ' + logger.name)
-# pprint.pprint(info)
-# write_specific_pickle(growers, pickle_file_name, specific_file_path=pickle_file_path)
-# show_specific_pickle(pickle_file_name, specific_file_path=pickle_file_path)
-
-
-# turn_ai_game_data_into_csv("ai_game_data2.pickle", "G:\\My Drive\\S-TOMAto\\2021\\Pickle\\", 'ai_game_data_2022')
-# grower_pickle_file_name = "2022_data.pickle"
-# grower_pickle_file_path = "H:\\Shared drives\\Stomato\\2022\\Pickle\\"
-# growers = open_specific_pickle(grower_pickle_file_name, specific_file_path=grower_pickle_file_path)
-# print()
-# setup_ai_game_data(
-#     "2022_pickle.pickle",
-#     "H:\\Shared drives\\Stomato\\2022\\Pickle\\",
-#     "2022_data.pickle",
-#     "H:\\Shared drives\\Stomato\\2023\\Pickle\\"
-# )
-
-
-# growers = open_specific_pickle('2022_pickle.pickle', 'H:\\Shared drives\\Stomato\\2022\\Pickle\\')
-# print(growers)
-
-# growers = open_specific_pickle('entry.pickle', 'H:\\Shared drives\\Stomato\\2021\\Pickle\\')
-# num_of_fields = 0
-# for grower in growers:
-#     for field in grower.fields:
-#         num_of_fields += 1
-# print(num_of_fields)
-#     grower.to_string()
-# print()
-
-# show_pickle()
-
-# growers = open_pickle()
-# for grower in growers:
-#     tech = grower.technician
-#     print(tech.name)
-#     if tech.name == 'Vanessa' and len(tech.email) == 1:
-#         tech.email.append('gjazo@morningstarco.com')
-#     elif tech.name == 'Exsaelth' and len(tech.email) == 2:
-#         tech.email.append('jjimenez@morningstarco.com')
-#     print(tech.email)
-# write_pickle(growers)
-#     if isinstance(tech.email, str):
-#         tech.email = tech.email.split('; ')
-#     print(tech.email)
-# write_pickle(growers)
-# if tech.name == 'Development Test Tech':
-#     print(tech.email)
-#     tech.email = tech.email.split('; ')
-#     print(tech.email)
-
-
-#         grower.to_string()
-# recipients = ['jgarrido@morningstarco.com','jsalcedo@morningstarco.com']
-# print(', '.join(recipients))
-
-
-# dbw = DBWriter()
-# dbw.grab_bq_client(my_project='')
-# print()
-# cleanup_cimis_stations_pickle()
-# test_switch_cases()
-
-# update_information(get_data=True, write_to_db=True, write_to_portal=True)
-# show_pickle()
-# only_certain_growers_update(['Bullseye'])
-# only_certain_growers_field_update('Bullseye Farms', 'Bullseye FarmsYO2E', get_data=True, write_to_db=True, write_to_portal=True)
-# cimis_pickle = open_specific_pickle("cimisStation.pickle")
-# print()
-
-# yesterdayRaw = date.today() - timedelta(1)
-# cimis = CIMIS()
-# print(cimis.get_list_of_active_eto_stations())
-# cimis.get_eto('169', str(yesterdayRaw), str(yesterdayRaw))
-
-# stations = {}
-# show_pickle()
-# dbw = DBWriter()
-# cwsi = CwsiProcessor()
-
-# growers = open_pickle()
-# for grower in growers:
-#     for field in grower.fields:
-#         for logger in field.loggers:
-#             if logger.ports:
-#                 for port in logger.ports:
-#                     sensor_type = logger.sensor_name(logger.ports[port])
-#                     logger.ports[port] = sensor_type
-#                 print(logger.ports)
-#             # ports[ind["port"]] = ind["sensor_number"]
-#             # ports[ind["port"]] = sensor_type
-# write_pickle(growers)
-
-# show_pickle()
-# show_grower('Saul')
-
-# only_certain_growers_field_logger_update('Saul', 'Meza', 'Development-C', subtract_from_mrid=24)
-# only_certain_growers_field_update('Saul', 'Meza', get_weather=True)
-
-# weather = WeatherProcessor(37.052433, -120.811616)
-# weather_data = weather.time_machine_forecast(weather.lat, weather.long, datetime(2021, 1, 1), datetime(2021, 1, 10))
-# def print_pretty_weather_data(weather_data):
-#     print("Weather forecast:\n")
-#     for data in weather_data:
-#         date = data['datetime']
-#         temp_min = data['tempmin']
-#         temp_max = data['tempmax']
-#         conditions = data['conditions']
-#
-#         print(f"{date}:")
-#         print(f"  Temperature: {temp_min}°F - {temp_max}°F")
-#         print(f"  Conditions: {conditions}")
-#         print()
-# print_pretty_weather_data(weather_data['days'])
-
-# show_pickle()
-
-# print((logger.ports))
-#
-#             # print(f'GPM: {logger.gpm}')
-#             if not hasattr(logger, 'rnd'):
-#                 print(f'Logger: {logger.name}')
-#                 logger.rnd = False
-#             # print(f'R&D: {logger.rnd}')
-#             # logger.to_string()
-# write_pickle(growers)
-# show_pickle()
-# hi = 'LAKJ:LAKJ:LKJ:LKJA'
-# print(f'hi: {hi:5}')
-# growers = open_pickle()
-# for grower in growers:
-#     print('************************GROWER***********************')
-#     pprint(vars(grower))
-#     print('***********************************************')
-#     for field in grower.fields:
-#         if hasattr(field, 'cimis'):
-#             delattr(field, 'cimis')
-#         print('===================FIELD=========================')
-#         pprint(vars(field))
-#         print('============================================')
-#         for logger in field.loggers:
-#             # logger.irrigation_set_acres = logger.acres
-#             if hasattr(logger, 'filename'):
-#                 delattr(logger, 'filename')
-#             if hasattr(logger, 'filepath'):
-#                 delattr(logger, 'filepath')
-#             print('--------------------LOGGER----------------------')
-#             pprint(vars(logger))
-#             print('------------------------------------------')
-# if hasattr(field, 'failed_cimis_update'):
-#     del field.failed_cimis_update
-# write_pickle(growers)
-# show_pickle()
-#         print(f'')
-#
-# reset_updated_all()
-# update_information(get_weather=True, write_to_db=True)
-# dbw = DBWriter()
-# cwsi = CwsiProcessor()
-# growers = open_pickle()
-# for grower in growers:
-#     for field in grower.fields:
-#         print(type(field.acres))
-#         print()
-#         if field.name == 'Bone Farms LLCF7' or field.name == 'Bone Farms LLCN42 N43':
-#             for logger in field.loggers:
-#                 result = dbw.add_up_whole_column('gdd', logger)
-#                 print(f'Name {logger.name}')
-#                 print(f'Gdd total: {result}')
-#                 logger.gdd_total = result
-#                 crop_stage = cwsi.get_crop_stage(result)
-#                 logger.crop_stage = crop_stage
-#                 print(f'Crop Stage: {crop_stage}')
-# write_pickle(growers)
-# print(logger.to_string())
-#             logger.crop_stage = 'NA'
-# write_pickle(growers)
-#         # field.cimis_station = field.cimisStation
-#         # if field.name == 'Bone Farms LLCF7':
-#         #     print()
-#         print(field.name, len(vars(field)))
-#         print()
-# print(field.name)
-# print()
-#         if field.cimisStation not in stations:
-#             stations[field.cimisStation] = 1
-#         else:
-#             stations[field.cimisStation] += 1
-# print(stations)
-
-# growers = open_pickle()
-# for grower in growers:
-#     for field in grower.fields:
-#         print(f'Field: {field.name} - Station: {field.cimisStation}')
-
-
-# onlyCertainGrowersUpdate(['KTN JV'], get_weather=True, write_to_db=True)
-# show_pickle()
-# write_pickle(growers)
-# show_pickle()
-# only_certain_growers_field_update('Bullseye Farms', 'Bullseye FarmsYO2E', write_to_portal=True, get_data=True)
-# remove_grower('Jesus')
-# growers = open_pickle()
-# for grower in growers:
-#     if hasattr(grower, 'portalGSheetURL'):
-#         del grower.portalGSheetURL
-#     grower.email = ''
-# write_pickle(growers)
-#     print(grower.name, 'Email: ', grower.email)
-# for field in grower.fields:
-#     if hasattr(field, 'gSheetEtName'):
-#         del field.gSheetEtName
-#     if hasattr(field, 'gSheetIrrigationSchedulingName'):
-#         del field.gSheetIrrigationSchedulingName
-#     if hasattr(field, 'gSheetUrl'):
-#         del field.gSheetUrl
-#     if hasattr(field, 'gSheetWeatherIconName'):
-#         del field.gSheetWeatherIconName
-#     if hasattr(field, 'gSheetWeatherName'):
-#         del field.gSheetWeatherName
-#     if hasattr(field, 'portalGSheetName'):
-#         del field.portalGSheetName
-#     print()
-# for logger in field.loggers:
-#     if hasattr(logger, 'almondkc'):
-#         del logger.almondkc
-#     if hasattr(logger, 'pepperkc'):
-#         del logger.pepperkc
-#     if hasattr(logger, 'pistachiokc'):
-#         del logger.pistachiokc
-#     if hasattr(logger, 'tomatokc'):
-#         del logger.tomatokc
-#     if hasattr(logger, 'gsheetid'):
-#         del logger.gsheetid
-# write_pickle(growers)
-#         for logger in field.loggers:
-#             logger.logger_direction = logger.loggerDirection
-#             del logger.loggerDirection
-# logger.crop_type = logger.cropType
-# del logger.cropType
-# logger.field_capacity = logger.fieldCapacity
-# del logger.fieldCapacity
-# logger.wilting_point = logger.wiltingPoint
-# del logger.wiltingPoint
-
-# write_pickle(growers)
-# check_technician_clones()
-#         field.weather_crashed = False
-# remove_field()
-# update_information(get_weather=True, write_to_db=True)
-# growers = open_pickle()
-# for grower in growers:
-#     new_fields = []
-#     print('Old Fields')
-#     for field in grower.fields:
-#         print(field.name, ' - ', field.active)
-#         if field.active:
-#             new_fields.append(field)
-#     print()
-#     print('New Fields')
-#     for f in new_fields:
-#         print(f.name, ' - ', f.active)
-#     print()
-#     grower.fields = new_fields
-# write_pickle(growers)
-
-
-# new_year_pickle_cleanup()
-# deactivate_grower('Javier')
-#     has_active_fields = False
-#     for field in grower.fields:
-#         if field.active:
-#             has_active_fields = True
-#     if has_active_fields:
-#         new_2023_growers.append(grower)
-#         # print(field.name, ' - ', field.active)
-# write_pickle(new_2023_growers)
-# print('Active Growers:')
-# for grower in new_2023_growers:
-#     print(grower.name)
-
-# for data in dict1:
-#     print(f'Length for {data} = {len(dict1[data])}')
-# print()
-# for data in dict2:
-#     print(f'Length for {data} = {len(dict2[data])}')
-
-
-# for data in multiple_days:
-#     print(f'Length for {data} = {len(multiple_days[data])}')
-
-# setup_ai_game_data()
-# cwsi = CwsiProcessor()
-# print('PSI ver1: ', cwsi.get_old_tomatoes_cwsi(-12.6,29.9,4.2,97.3,True))
-# print('PSI ver2: ', cwsi.get_old_tomatoes_cwsi_2(-12.6,29.9,4.2,97.3,True))
-# writeUninstallationProgresstoDB()
-
-# write_pickle(growers)
-# onlyCertainGrowersFieldUpdate('Bullseye Farms', 'Bullseye FarmsYO2W', get_data=True)
-# show_pickle()
-# onlyCertainGrowersFieldUpdate('Andrew', 'Andrew3107', get_weather=True, write_to_db=True)
-# reset_updated_all()
-# update_information(get_weather=True, write_to_db=True)
 # show_pickle()
-# growers = open_pickle()
-# for grower in growers:
-#     if grower.active:
-#         for field in grower.fields:
-#             if field.active:
-#                 print(field.grower.name)
-#                 print(field.name)
-#                 print()
-# update_information(get_data=True, write_to_portal=True)
-
-# today = datetime.today()
-# print((today + timedelta(days=60)).month)
-# growers = open_pickle()
-# for g in growers:
-#     for f in g.fields:
-#         for l in f.loggers:
-#             print(l.name, l.model)
-
-# url = "https://api.openweathermap.org/data/3.0/onecall?lat=37.053841&lon=-120.809556&appid=23e1e01a3d66fbd41154957a11dbe696"
-#
-# headers = {"accept": "application/json"}
-# response = requests.get(url, headers=headers)
-# print(response.text)
-
-
-# ///////////////////////
-# api_key = "91abc03a7e1748b767c16fccd701c7ec"
-# lat = "37.053841"
-# lon = "-120.809556"
-# url = "https://api.openweathermap.org/data/2.5/onecall?lat=%s&lon=%s&appid=%s&units=imperial&exclude=minutely,hourly" % (lat, lon, api_key)
-#
-# response = requests.get(url)
-# data = json.loads(response.text)
-#
-# daily = data["daily"]
-# for entry in daily:
-#     time = datetime.utcfromtimestamp(entry["dt"])
-#     # time = datetime.fromtimestamp(entry["dt"])
-#     date_string_format = time.strftime("%Y-%m-%d")
-#     date_day = time.strftime('%a')
-#     maxTemp = entry["temp"]["max"]
-#     humidity = entry["humidity"]
-#     tempC = (maxTemp - 32) * 5.0 / 9.0
-#     saturation_vapor_pressure = 610.7 * 10 ** ((7.5 * tempC) / (237.7 + tempC))
-#     vpd = (((1 - humidity/100) * saturation_vapor_pressure) * 0.001)
-#     vpd = round(vpd, 1)
-#     icon = entry["weather"][0]["main"]
-#     print(date_string_format, date_day, maxTemp, humidity, vpd, icon)
-# print(data)
-# ///////////////////////////////
-
-# show_pickle()
-# reset_updated_all()
-# update_information(get_weather=True, get_data=True)
-# onlyCertainGrowersUpdate(['Saul'], get_weather=True, get_data=True, write_to_db=True, write_to_portal=True)
-# only_certain_growers_field_logger_update('Saul', 'Meza', 'Development-C', subtract_from_mrid=48)
-
-# show_pickle()
-# growers = open_pickle()
-# for grower in growers:
-#     for field in grower.fields:
-
-#         if field.name == 'Andrew3101A':
-#             for logger in field.loggers:
-#                 print('HI')
-#                 logger.field_capacity = 18
-#                 logger.wilting_point = 8
-# write_pickle(growers)
-
-# show_pickle()
-
-# growers = open_pickle()
-# for grower in growers:
-#     for field in grower.fields:
-#         if field.name == 'RG Farms LLC2':
-#             print()
-#             print(field.acres)
-#             field.acres = 30.35
-#             print(field.acres)
-# write_pickle(growers)
-
-# growers = open_pickle()
-# for grower in growers:
-#     for field in grower.fields:
-#         for logger in field.loggers:
-#             # print(f'Grower: {logger.grower.name} - Field: {logger.field.name}')
-#             # print(f'Logger name: {logger.name}')
-#             print(f'Logger ports: {logger.ports}')
-# print(f'Sensor data: {logger.get_sensor_index_data()}')
-# print(f'Logger keys: {logger.ports.keys()}')
-# print(f'Logger values: {logger.ports.values()}')
-# for key, value in sorted(logger.ports):
-#     print(f'Key: {key} - Value: {value}')
-#             irs = 0
-#             for ind, port in enumerate(logger.ports):
-# #                 port_sensor = logger.sensor_name(logger.ports[port])
-# #                 logger.ports[port] = port_sensor
-#                 print(f'Port: {port} - Sensor: {logger.ports[port]}')
-#                 if logger.ports[port] == 'Infra Red':
-#                     irs += 1
-#             print(f'IRs: {irs}')
-#             if irs > 1:
-#                 break
-#             print(f'Logger ports: {logger.ports}')
-# write_pickle(growers)
-# print()
-
-
-# setup_grower('Javier Test', 'a', 'a', 'North', True)
-
-# show_pickle()
-#         if field.active:
-#             print(field.name)
-# print(field.loggers[0].crop)
-# show_pickle()
-# ///////////////////////////////
-# METER API v1 Testing
-# GET READINGS
-# import requests
-# import json
-# import pprint
-# import pandas as pd
-# device_sn = "z6-16138"
-# device_pw = '65607-66874'
-# token = "Token {TOKEN}".format(TOKEN="6d1f65835ec6fd0c7ec77266d6a74d7f5a7be6b1")
-# url = "https://zentracloud.com/api/v1/readings"
-# headers = {'content-type': 'application/json', 'Authorization': token}
-# output_format = "json"
-#
-# params = {'user': 'jgarrido@morningstarco.com', 'user_password': 'Mexico1012', 'sn': device_sn,
-#           'device_password': device_pw, 'start_mrid': 0}
-# # payload_str = urllib.parse.urlencode(params, safe='@')
-# response = requests.get(url, params=params)
-# # print(response.url)
-# print(f'Response ok: {response.ok}')
-# content = json.loads(response.content)
-# pprint.pprint(content)
-# df = pd.read_json(content['data'], convert_dates=True, orient='split')
-# print(df)
-# -->> Request URL: https://zentracloud.com/api/v1/readings?user=jgarrido@morningstarco.com&user_password=Mexico1012&sn=z6-07220&device_password=82901-36182&start_mrid=22204
-#                  'https://zentracloud.com/api/v1/readings/?user=jgarrido@morningstarco.com&user_password=Mexico1012&sn=z6-07275&device_password=65299-46534&start_mrid=0'
-# ///////////////////////////////
-
-
-
-# ///////////////////////////////
-# METER API v4 Testing
-# GET READINGS
-# import requests
-# import json
-# import pprint
-# import pandas as pd
-# device_sn = "z6-16138"
-# token = "Token {TOKEN}".format(TOKEN="8c5815c78e7dd349d77da4821db5e3ec2087c4a2")
-# url = "https://zentracloud.com/api/v4/get_readings/"
-# headers = {'content-type': 'application/json', 'Authorization': token}
-# output_format = "json"
-# page_num = 1
-#
-# params = {'device_sn': device_sn, 'output_format': output_format, 'per_page': 2000,
-#           'sort_by': 'ascending'} #, 'start_date': '1-1-2023', 'end_date': '7-10-2023'}
-# response1 = requests.get(url, params=params, headers=headers)
-# content1 = json.loads(response1.content)
-# print()
-#
-# next_url = content1['pagination']['next_url']
-# print(next_url)
-# response2 = requests.get(next_url, headers=headers)
-# content2 = json.loads(response2.content)
-# print()
-# pprint.pprint(content)
-# df = pd.read_json(content['data'], convert_dates=True, orient='split')
-# print(df)
-# ///////////////////////////////
-
-
-
-# ///////////////////////////////
-# GET ENVIRONMENTAL
-# device_sn = "z6-12376"
-# token = "Token {TOKEN}".format(TOKEN="6d1f65835ec6fd0c7ec77266d6a74d7f5a7be6b1")
-# url = "https://zentracloud.com/api/v3/get_env_model_data/"
-# headers = {'content-type': 'application/json', 'Authorization': token}
-# output_format = "json"
-#
-# params = {'device_sn': device_sn, 'model_type': 'ETo', 'port_num': 2, 'inputs': {"elevation": 3, "latitude": 38.6120803, "wind_measurement_height": 2}}
-# response = requests.get(url, params=params, headers=headers)
-# content = json.loads(response.content)
-# pprint.pprint(content)
-# df = pd.read_json(content['data'], convert_dates=True, orient='split')
-# print(df)
-# ///////////////////////////////
-
-
-# show_pickle()
-
-# apply_AI_recommendation_to_logger('Nees_AI', 'IRR-1-80-AI')
-# num_of_fields = 0
-# growers = open_pickle()
-# for grower in growers:
-#     for field in grower.fields:
-#         for logger in field.loggers:
-#             crop = logger.crop_type.lower()[0:4]
-#             print(crop)
-#         num_of_fields += 1
-#
-# print(num_of_fields)
-
-# show_pickle()
-# growers = open_pickle()
-# techs = get_all_technicians(growers)
-# for tech in techs:
-#     if tech.name == 'Exsaelth':
-#         tech.email = 'ejimenez@morningstarco.com; cvalcheck@morningstarco.com'
-#     print(tech.name)
-#     print(tech.email)
-# write_pickle(growers)
-# show_pickle()
-# tech = Technician('Vanessa_and_Exsaelth', 'vcastillo@morningstarco.com; ejimenez@morningstarco.com')
-#
-# growers = open_pickle()
-# for grower in growers:
-#     print()
-#     print(grower.name)
-#     for field in grower.fields:
-#         print('\t', field.name)
-#         for logger in field.loggers:
-#             print('\t\t', logger.name, ' R&D: ', logger.rnd)
-#     if grower.name == 'Lucero Watermark' or grower.name == 'Lucero Rio Vista' or grower.name == 'Lucero Morada':
-#         grower.technician = tech
-#         print(grower.technician.name)
-# write_pickle(growers)
-# update_information(get_data=True, check_for_notifications=True)
-# onlyCertainGrowersFieldLoggerUpdate('Saul', 'Meza', 'Development-C', subtract_from_mrid=48)
-# update_information(get_et=True, write_to_db=True)
-# cimisStationsPickle = CimisStation.open_cimis_station_pickle(CimisStation)
-# for stations in cimisStationsPickle:
-#     print(stations.station_number, ' : ', stations.latest_eto_value, ' : ', stations.updated)
-# cimisStations.showCimisStations()
-# show_pickle()
-
-
-# yesterdayRaw = date.today() - timedelta(1)
-# try:
-#     all_et_data_dicts = pull_all_et_values(str(yesterdayRaw), str(yesterdayRaw))
-# except Exception as error:
-#     print('ERROR in get_et')  # Used to just print ERROR
-#     print(error)
-#
-# try:
-#     write_all_et_values_to_db(all_et_data_dicts)
-# except Exception as error:
-#     print('ERROR in write_et_to_db')  # Used to just print ERROR
-#     print(error)
-
-# show_pickle()
-# remove_field('DCB', 'DCBCP:13,15')
-# update_information(get_data=True, write_to_db=True)
-# check_for_new_cimis_stations()
-# CimisStation.showCimisStations(CimisStation)
-# temp_ai_application()
-# print('BEFORE')
-# show_pickle()
-# onlyCertainGrowersFieldUpdate('Saul', 'Meza', get_data=True)
-# onlyCertainGrowersFieldLoggerUpdate('Hughes', 'Hughes233-4', 'HU-233-SW')
-
-
-# temp_ai_application()
-# show_pickle()
-# dbw = DBWriter()
-# dbw.add_gdd_columns_to_all_tables()
-# show_pickle()
-# dbwriter = DBWriter()
-# dbwriter.merge_all_tables_for_gdd()
-# show_pickle()
-# growers = open_pickle()
-# for grower in growers:
-#     for field in grower.fields:
-#         for logger in field.loggers:
-#             print(logger.cropType)
-#             if logger.lat == '':
-#                 logger.lat = None
-#             if logger.long == '':
-#                 logger.long = None
-#             if logger.name == 'HU-228-SW':
-#                 logger.lat = 36.5111213
-#             if logger.lat:
-#                 logger.lat = float(logger.lat)
-#             if logger.long:
-#                 logger.long = float(logger.long)
-#
-#             print(f'{logger.name} ---- Lat {logger.lat}, Long {logger.long}')
-# write_pickle(growers)
-#             print(logger.__dict__)
-# print(dir(logger))
-#     if grower.name == 'Saul':
-#         grower.deactivate()
-#     if 'Lucero' in grower.name:
-#         print(grower.name)
-#         for field in grower.fields:
-#             for logger in field.loggers:
-#                 print('\t* factor for ', field.name, ' -- ', logger.name)
-#                 # print('(449 * ', logger.acres, ') / ')
-#                 # print('(', logger.gpm, ' * 0.85)')
-#                 print('\t', round(((449*logger.acres)/(logger.gpm*0.85)),1))
-#             print()
-#         print()
-# temp_ai_application()
-
-# growers = open_pickle()
-# for grower in growers:
-#     if grower.name == 'Javier':
-#         print(grower.fields[0].loggers[1].gpm)
-#         print(grower.fields[0].loggers[1].acres)
-# for field in grower.fields:
-#     for logger in field.loggers:
-#         logger.gpm = 3600
-#         logger.acres = 128
-#
-
-# show_pickle()
-
-# write_pickle(growers)
-# print(grower.fields[0].loggers[1].gpm)
-# print(grower.fields[0].loggers[1].acres)
-#     print('Grower Name: ', grower.name)
-#     print('Total Fields: ', len(grower.fields))
-#     for field in grower.fields:
-#         print('\tField Name: ', field.name, ' - ')
-#         print('\tTotal Loggers: ', len(field.loggers))
-#         for logger in field.loggers:
-#             print('\t\tLogger Name: ', logger.name)
-#     print()
-# show_pickle()
-
-# onlyCertainGrowersFieldLoggerUpdate('RnD','RnDStoler Rate Trial', 'CONTROL-W', write_to_db=True, subtract_from_mrid=500000)
-# onlyCertainGrowersFieldLoggerUpdate('RnD','RnDStoler Rate Trial', 'RATE-1-E', write_to_db=True, subtract_from_mrid=500000)
-# onlyCertainGrowersFieldLoggerUpdate('RnD','RnDStoler Rate Trial', 'RATE-1-W', write_to_db=True, subtract_from_mrid=500000)
-# onlyCertainGrowersFieldLoggerUpdate('RnD','RnDStoler Rate Trial', 'RATE-2-E', write_to_db=True, subtract_from_mrid=500000)
-# onlyCertainGrowersFieldLoggerUpdate('RnD','RnDStoler Rate Trial', 'RATE-2-W', write_to_db=True, subtract_from_mrid=500000)
-# onlyCertainGrowersFieldLoggerUpdate('Bullseye Farms','Bullseye FarmsYO2E', 'YO2E-WM2-S', write_to_db=True, subtract_from_mrid=500000)
-#
-# apply_AI_recommendation_to_logger('Nees_AI', 'IRR-1-80-AI')
-# apply_AI_recommendation_to_logger('Nees_AI', 'IRR-1-80-Ctrl')
-# apply_AI_recommendation_to_logger('Nees_AI', 'IRR-2-60-AI')
-# apply_AI_recommendation_to_logger('Nees_AI', 'IRR-2-60-Ctrl')
-# apply_AI_recommendation_to_logger('Nees_AI', 'IRR-3-80-AI')
-# apply_AI_recommendation_to_logger('Nees_AI', 'IRR-3-80-Ctrl')
-# apply_AI_recommendation_to_logger('Nees_AI', 'IRR-4-60-AI')
-# apply_AI_recommendation_to_logger('Nees_AI', 'IRR-4-60-Ctrl')
-# apply_AI_recommendation_to_logger('Nees_AI', 'IRR-5-80-AI')
-# apply_AI_recommendation_to_logger('Nees_AI', 'IRR-5-80-Ctrl')
-
-# check_technician_clones()
-#
-# temp_ai_application()
-# growers = open_pickle()
-# for grower in growers:
-#     for field in grower.fields:
-#         field.update_et_tables()
-#         for logger in field.loggers:
-#             print('Grower -> ', grower.name, '  ', logger.name, ' - R&D? ', logger.rnd)
-#     if grower.technician.name == 'Vanessa':
-#         print(grower.technician.email)
-#         grower.technician.email = 'vcastillo@morningstarco.com; jkent@morningstarco.com'
-#         # break
-# #
-# # for grower in growers:
-# #     if grower.technician.name == 'Adriana':
-# #         grower.technician = vane
-# write_pickle(growers)
-#
-# print('AFTER')
-# check_technician_clones()
-
-# ady = get_technician('Adriana')
-# print(round(5.5798723,2))
-
-# growers = open_pickle()
-# del growers[-1]
-# write_pickle(growers)
-# show_pickle()
-#
-# grower_javier = setupGrower('Javier', '', 'Development Test Tech')
-# field_javier = setupField('Nees AI', '', '36.8339015', '-120.7384265', 124)
-# field_javier.nickname = 'Nees AI'
-# logger1 = setup_logger('z6-12374', '39225-28564', 'IRR-1-80-AI', '', 'tomatoes', 42, 36, 1750, 152, 'N', planting_date='05/03/2022')
-# logger2 = setup_logger('z6-11584', '73659-11380', 'IRR-1-80-Ctrl', '', 'tomatoes', 42, 36, 1750, 152, 'N', planting_date='05/03/2022')
-# logger3 = setup_logger('z6-16148', '96777-21941', 'IRR-2-60-AI', '', 'tomatoes', 42, 36, 1750, 152, 'N', planting_date='05/03/2022')
-# logger4 = setup_logger('z6-16143', '41233-23875', 'IRR-2-60-Ctrl', '', 'tomatoes', 42, 36, 1750, 152, 'N', planting_date='05/03/2022')
-# logger5 = setup_logger('z6-13262', '76705-06865', 'IRR-3-80-AI', '', 'tomatoes', 42, 36, 1750, 152, 'N', planting_date='05/03/2022')
-# logger6 = setup_logger('z6-07231', '40779-55230', 'IRR-3-80-Ctrl', '', 'tomatoes', 42, 36, 1750, 152, 'N', planting_date='05/03/2022')
-# logger7 = setup_logger('z6-11553', '80480-15456', 'IRR-4-60-AI', '', 'tomatoes', 42, 36, 1750, 152, 'N', planting_date='05/03/2022')
-# logger8 = setup_logger('z6-01889', '24590-03289', 'IRR-4-60-Ctrl', '', 'tomatoes', 42, 36, 1750, 152, 'N', planting_date='05/03/2022')
-# logger9 = setup_logger('z6-12391', '57568-22189', 'IRR-5-80-AI', '', 'tomatoes', 42, 36, 1750, 152, 'N', planting_date='05/03/2022')
-# logger10 = setup_logger('z6-12368', '32338-34041', 'IRR-5-80-Ctrl', '', 'tomatoes', 42, 36, 1750, 152, 'N', planting_date='05/03/2022')
-
-# field_javier.addLoggers([logger1, logger2, logger3, logger4, logger5, logger6, logger7, logger8, logger9, logger10])
-# addFieldToGrower(grower_javier.name, field_javier)
 
 
-# grower = setupGrower('Saul', '1S8acM-DKwrMaxOZpEdndvHn59xij7OHxtB6ztErIxoY')
-# addGrowerToGrowers(grower)
-#
-# field = setupField('Meza', '1TtZQM1GU6h88P83U6EGEUojIResR5gSyBBC7SR6gQZc', '37.053941099999996', '-120.80917459999999', '56')
-# logger1 = setupLogger('z6-07275', '65299-46534', 'Development', 1665227864, 'Tomato', 36, 22, 1200, 100)
-# field.addLogger(logger1)
-# addFieldToGrower('Saul', field)
-
-# logger_list_javier = []
-# logger_list_jesus = open_specific_pickle("loggerList.pickle")
-# growers = open_pickle()
-# for grower in growers:
-#     print(grower.name)
-
-# del growers[-1]
-# write_pickle(growers)
-# for field in grower.fields:
-# print(field.name)
-#         for logger in field.loggers:
-#             logger_list_javier.append(logger)
-#
-# for logger in logger_list_javier:
-#     if logger.id not in logger_list_jesus:
-#         print(logger.id, ' not in')
-#         print(logger.name)
-#         print(logger.field.name)
-#         print(logger.active)
-#         print()
-
-
-# print(tech_dict)
 #     if grower.name == 'Ryan Jones':
 #         og_vanessa = grower.technician
 #
@@ -6136,96 +4723,31 @@
 # only_certain_growers_field_update('Lucero Farms CICC', 'Lucero Farms CICCWT1', get_data=True, subtract_from_mrid=72)
 
 # show_pickle()
-
-# growers = open_pickle()
-# for grower in growers:
-#     if grower.name == 'Knight Farms':
-#         for field in grower.fields:
-#             if field.name == 'Knight FarmsB4':
-#                 for logger in field.loggers:
-#                     if logger.id == 'z6-07324':
-#                         logger.broken = True
-#                         logger.active = False
-#                         un_date = datetime(2024, 6, 14).date()
-#                         logger.uninstall_date = un_date
-#
-# write_pickle(growers)
+# #
+growers = open_pickle()
+for grower in growers:
+    # if grower.name == 'T&J Farming':
+        for field in grower.fields:
+            # if field.name == 'Lucero SE RobinsonR 1 2':
+                # field.nickname = 'JJB FarmsGI 17'
+                for logger in field.loggers:
+                    # logger.irrigation_set_acres = 63
+                    if logger.id == 'z6-12367':
+                        logger.active = False
+                        logger.uninstall_date = '2024-04'
+                        # logger.gpm = 740.0
+                        # logger.irrigation_set_acres = 37.0
+                        # logger.merge_et_db_with_logger_db_values()
+                        # if logger.name == 'DF-E1-NW':
+                        #     # logger.soil_type = 'Silt Loam'
+                        #     SQLScripts.change_logger_soil_type(logger.name, field.name, grower.name, 'Silt Loam')
+write_pickle(growers)
 
 # show_pickle()
-
-# growers = open_pickle()
-# for grower in growers:
-#     for field in grower.fields:
-#         if field.cimis_station == '146':
-#             print(field.name)
-
+# remove_field('Lucero Rio Vista', 'Lucero Rio VistaT', avoid_user_input=True)
+# remove_grower('Ollie')
 # cimisst = CimisStation()
 # cimisst.deactivate_cimis_station('5')
 
-# only_certain_growers_update('Kubo & YoungKF1', get_weather=True, get_data=True, write_to_db=True, write_to_portal=True)
-# only_certain_growers_field_update('Kubo & Young', 'Kubo & YoungKF1', get_weather=True, get_data=True, write_to_db=True, write_to_portal=True)
-
-
-# show_pickle()
-# growers = open_pickle()
-# techs = []
-# for grower in growers:
-#     if grower.technician not in techs:
-#         techs.append(grower.technician)
-# print('ALL TECHS')
-# print(techs)
-
-#
-#
-# for grower in growers:
-#     print(grower.name)
-#     print(grower.technician.name)
-#     print(grower.technician)
-#     print()
-#
-# for grower in growers:
-#     if grower.technician.name == "Vanessa":
-#         vane = grower.technician
-#         break
-# for grower in growers:
-#     if grower.technician.name == "Exsaelth":
-#         ex = grower.technician
-#         break
-#
-# print(vane)
-# print(ex)
-# print('AFTER SET')
-#
-# for grower in growers:
-#     if grower.technician.name == "Vanessa":
-#         grower.technician = vane
-#     if grower.technician.name == "Exsaelth":
-#         grower.technician = ex
-#
-# for grower in growers:
-#     print(grower.name)
-#     print(grower.technician.name)
-#     print(grower.technician)
-#     print()
-
-# techs = []
-# for grower in growers:
-#     if grower.technician not in techs:
-#         techs.append(grower.technician)
-# print('ALL TECHS')
-# print(techs)
-
-
-# write_pickle(growers)
-# show_pickle()
-
-
-
-
-# techs = get_all_technicians(growers)
-# for t in techs:
-#     print(t, t.name, t.growers)
-# print()
-
-# remove_last_grower()
-# remove_grower("TEST")
\ No newline at end of file
+# only_certain_growers_update('Lucero SE Robinson', get_weather=True, get_data=True, write_to_db=True, write_to_portal=True, subtract_from_mrid=50)
+# only_certain_growers_fields_update(['Riley Chaney Farms14'] , get_weather=True, get_data=True, write_to_db=True, write_to_portal=True, subtract_from_mrid=15)
Index: Field.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import csv\r\nimport uuid\r\nfrom datetime import datetime\r\nfrom datetime import timedelta\r\nfrom itertools import zip_longest\r\n\r\nimport numpy as numpy\r\nfrom google.cloud import bigquery\r\n\r\n# import Decagon\r\nfrom CwsiProcessor import CwsiProcessor\r\nfrom DBWriter import DBWriter\r\nfrom Grower import Grower\r\nfrom Notifications import AllNotifications\r\nfrom Thresholds import Thresholds\r\nfrom WeatherProcessor import WeatherProcessor\r\n\r\nDATABASE_YEAR = '2024'\r\nFIELD_PORTALS_BIGQUERY_PROJECT = 'growers-' + DATABASE_YEAR\r\n\r\n\r\nclass Field(object):\r\n    \"\"\"\r\n    Class to hold information for 1 field where we have a water management trial.\r\n\r\n    Attributes:\r\n        cimis_station: String with the value of the CIMIS station that\r\n            corresponds to the field and from which we will get ET info\r\n        name: String of the field name\r\n        grower: String of the grower name for the field\r\n        weather_processor: WeatherProcessor object used to call\r\n            weather API's to get the forecast and icons\r\n    \"\"\"\r\n\r\n    def __init__(self,\r\n                 name: str,\r\n                 loggers: list,\r\n                 lat: str,\r\n                 long: str,\r\n                 cimis_station: str,\r\n                 acres: float,\r\n                 crop_type: str,\r\n                 grower: Grower = None,\r\n                 active: bool = True,\r\n                 report_url: str = 'https://i.imgur.com/04UdmBH.png',\r\n                 preview_url: str = 'https://i.imgur.com/04UdmBH.png',\r\n                 nickname: str = '',\r\n                 field_type: str = 'Commercial',\r\n                 ):\r\n        \"\"\"\r\n        Inits Field class with the following parameters:\r\n\r\n        :param acres:\r\n        :param preview_url:\r\n        :param report_url:\r\n        :param name:\r\n        :param loggers:\r\n        :param lat:\r\n        :param long:\r\n        :param cimis_station:\r\n        :param grower:\r\n        \"\"\"\r\n\r\n        self.loggers = loggers\r\n        self.id = uuid.uuid4()\r\n        self.lat = lat\r\n        self.long = long\r\n        self.cimis_station = cimis_station\r\n        self.name = name\r\n        self.grower = grower\r\n        self.dbwriter = DBWriter()\r\n        self.cwsi_processor = CwsiProcessor()\r\n        # if grower.name == 'Sugal Chile':\r\n        #     self.weather_processor = WeatherProcessor(self.lat, self.long, use_celsius=True)\r\n        # else:\r\n        self.weather_processor = WeatherProcessor(self.lat, self.long)\r\n        self.all_notifications = AllNotifications()\r\n        self.updated = False\r\n        self.weather_crashed = False\r\n        self.active = active\r\n        self.report_url = report_url\r\n        self.preview_url = preview_url\r\n        self.acres = acres\r\n        self.crop_type = crop_type\r\n        self.net_yield = None\r\n        self.paid_yield = None\r\n        self.field_type = field_type\r\n\r\n        if len(nickname) > 0:\r\n            self.nickname = nickname\r\n        else:\r\n            self.nickname = self.name.split(self.grower.name)[-1]\r\n        if grower is not None:\r\n            self.field_string = self.grower.name + \" - \" + self.name\r\n\r\n    def __repr__(self):\r\n        return f'Field: {self.nickname}, Active: {self.active}, # of Loggers: {len(self.loggers)}'\r\n\r\n    def check_successful_updated_loggers(self):\r\n        successful_loggers = 0\r\n        number_of_active_loggers, number_of_inactive_loggers = self.get_number_of_active_loggers()\r\n        for logger in self.loggers:\r\n            if logger.active and logger.updated:\r\n                successful_loggers += 1\r\n        if successful_loggers == number_of_active_loggers:\r\n            print(f\"\\t\\tAll loggers for Field {self.name} successful! \")\r\n            print(f\"\\t\\tSuccess: {successful_loggers}/ Active: {number_of_active_loggers}\")\r\n            self.updated = True\r\n        else:\r\n            print(\"\\t\\t{0}/{1} loggers updated successfully\".format(successful_loggers, number_of_active_loggers))\r\n            self.updated = False\r\n\r\n    def add_logger(self, logger):\r\n        self.loggers.append(logger)\r\n\r\n    def add_loggers(self, loggers: list):\r\n        for logger in loggers:\r\n            self.loggers.append(logger)\r\n\r\n    def to_string(self, include_loggers: bool = True):\r\n        \"\"\"\r\n        Function used to print out output to screen. Prints out the field name, grower,\r\n            location GSheetURL, gSheetEtName, gSheetWeatherName, gSheetWeatherIconName,\r\n            cimisStation.\r\n            Then it calls on its plots list and has each object in the list call its own toString function\r\n        :return:\r\n        \"\"\"\r\n        if not hasattr(self, 'cimis_station'):\r\n            if hasattr(self, 'cimisStation'):\r\n                self.cimis_station = self.cimisStation\r\n\r\n        if not hasattr(self, 'report_url'):\r\n            self.report_url = 'No URL'\r\n\r\n        if not hasattr(self, 'crop_type'):\r\n            self.crop_type = self.loggers[0].crop_type\r\n\r\n        field_str = f'Field: {str(self.name)}'\r\n        location_str = f'Location: ({str(self.lat)},{str(self.long)})'\r\n        active_str = f'Active: {str(self.active)}'\r\n        crop_type_str = f'Crop Type: {str(self.crop_type)}'\r\n        net_yield_str = f'Yield --> Net T/A: {str(self.net_yield)}'\r\n\r\n\r\n        print('---------------------------------------------------------------------------------------------------')\r\n        print(f'\\t{field_str:40} | Grower: {str(self.grower.name)}')\r\n        print(f'\\t{location_str:40} | CimisStation: {str(self.cimis_station)}')\r\n        print(f'\\t{active_str:40} | Updated: {str(self.updated)}')\r\n        print(f'\\t{crop_type_str:40} | Field Type: {self.field_type}')\r\n        print(f'\\t{net_yield_str:40} | Paid T/A: {str(self.paid_yield)}')\r\n        print(f'\\tReport URL: {str(self.report_url)}')\r\n        print()\r\n        if include_loggers:\r\n            for logger in self.loggers:\r\n                logger.to_string()\r\n        print('---------------------------------------------------------------------------------------------------')\r\n\r\n    def update(\r\n            self,\r\n            cimis_stations_pickle,\r\n            get_weather: bool = False,\r\n            get_data: bool = False,\r\n            write_to_portal: bool = False,\r\n            write_to_db: bool = False,\r\n            check_for_notifications: bool = False,\r\n            check_updated: bool = False,\r\n            subtract_from_mrid: int = 0,\r\n            specific_mrid = None\r\n    ):\r\n        \"\"\"\r\n        Function used to update each fields information. This function will be called every day.\r\n        This function then calls the update function on each of its plots[]\r\n\r\n        :param subtract_from_mrid: Int used to subtract a specific amount from the logger MRIDs for API calls\r\n        :param cimis_stations_pickle:\r\n        :param get_weather: Boolean that dictates if we want to get the fields weather forecast\r\n        :param get_data: Boolean that dictates if we want to get the logger data\r\n        :param write_to_portal:\r\n        :param write_to_db:\r\n        :param check_for_notifications:\r\n        :param check_updated:\r\n        :return:\r\n        \"\"\"\r\n        if self.active:\r\n            # self.field_string = self.grower.name + \" - \" + self.name\r\n            print()\r\n\r\n            if self.updated and not self.weather_crashed:\r\n                print('\\tField: ' + self.name + '  already updated. Skipping...')\r\n            else:\r\n                print(f'FIELD updating: {str(self.grower.name)} - {self.name} ->')\r\n\r\n                # UPDATE WEATHER\r\n                if get_weather:\r\n                    try:\r\n                        forecast = self.get_weather_forecast()\r\n                        self.weather_crashed = False\r\n                        # weatherData, weatherIconData = self.get_weather_forecast()\r\n                    except Exception as error:\r\n                        print(\"\\tError in Field get_weather - \" + self.name)\r\n                        print(\"\\tError type: \" + str(error))\r\n                        forecast = []\r\n                        self.weather_crashed = True\r\n                else:\r\n                    print('\\tNot doing Weather')\r\n\r\n                # Write weather data to DB\r\n                if write_to_db and get_weather:\r\n                    if not forecast:\r\n                        print('\\tNo weather data')\r\n                    else:\r\n                        try:\r\n                            print('\\tPreparing weather data to be written to DB-')\r\n                            # Prep data for writing to DB\r\n                            weather_schema, weather_filename = self.prep_weather_data_for_writing_to_db(forecast)\r\n\r\n                            # Prep DB\r\n                            self.prep_db_for_weather(forecast)\r\n\r\n                            # Write to DB\r\n                            print()\r\n                            print('\\tWriting weather data to DB')\r\n                            field_name = self.dbwriter.remove_unwanted_chars_for_db_dataset(self.name)\r\n                            project = self.dbwriter.get_db_project(self.loggers[0].crop_type)\r\n                            self.dbwriter.write_to_table_from_csv(\r\n                                field_name, 'weather_forecast', weather_filename, weather_schema, project\r\n                            )\r\n                        except Exception as error:\r\n                            print(\"Error in Field Weather DB Write - \" + self.name)\r\n                            print(\"Error type: \" + str(error))\r\n\r\n                if not self.updated:\r\n                    # Get data\r\n                    if get_data:\r\n                        #\r\n                        # For each logger, call its update function\r\n\r\n                        field_loggers_portal_data = {}\r\n                        for logger in self.loggers:\r\n                            logger_portal_data = logger.update(cimis_stations_pickle, write_to_db=write_to_db,\r\n                                                               check_for_notifications=check_for_notifications,\r\n                                                               check_updated=check_updated,\r\n                                                               subtract_from_mrid=subtract_from_mrid)  # do logger updates\r\n                            if logger_portal_data is not None:\r\n                                field_loggers_portal_data[logger.id] = logger_portal_data\r\n                        self.check_successful_updated_loggers()\r\n                    else:\r\n                        print('\\tNot doing Data')\r\n                        print()\r\n\r\n                    # Handling all portal data (Field Portal and Logger Portal) - Processing and writing to portal\r\n                    if write_to_portal and get_data:\r\n                        try:\r\n                            self.cwsi_processor = CwsiProcessor()\r\n                            if field_loggers_portal_data:\r\n                                new_data_to_write_to_portal = False\r\n                                for logger in self.loggers:\r\n                                    if logger.active and logger.id in field_loggers_portal_data:\r\n                                        if field_loggers_portal_data[logger.id]['dates']:\r\n                                            # Intentional way of setting new_data_to_write_to_portal that sets it to True if\r\n                                            # any of the loggers do have data that needs to be written, even if loggers after\r\n                                            # that don't have new data\r\n                                            new_data_to_write_to_portal = new_data_to_write_to_portal or True\r\n                                        else:\r\n                                            new_data_to_write_to_portal = new_data_to_write_to_portal or False\r\n\r\n                                if new_data_to_write_to_portal:\r\n                                    print()\r\n                                    print('\\t>>> Handling ', self.name, ' Portal Data')\r\n\r\n                                    field_averages_table_schema = [bigquery.SchemaField(\"order\", \"FLOAT\"),\r\n                                                                   bigquery.SchemaField(\"field\", \"STRING\"),\r\n                                                                   bigquery.SchemaField(\"crop_type\", \"STRING\"),\r\n                                                                   bigquery.SchemaField(\"crop_image\", \"STRING\"),\r\n                                                                   bigquery.SchemaField(\"soil_moisture_num\", \"FLOAT\"),\r\n                                                                   bigquery.SchemaField(\"soil_moisture_desc\", \"STRING\"),\r\n                                                                   bigquery.SchemaField(\"si_num\", \"FLOAT\"),\r\n                                                                   bigquery.SchemaField(\"si_desc\", \"STRING\"),\r\n                                                                   bigquery.SchemaField(\"report\", \"STRING\"),\r\n                                                                   bigquery.SchemaField(\"preview\", \"STRING\")]\r\n\r\n                                    logger_portal_table_schema = field_averages_table_schema.copy()\r\n                                    logger_portal_table_schema.append(\r\n                                        bigquery.SchemaField(\"logger_name\", \"STRING\")\r\n                                    )\r\n                                    logger_portal_table_schema.append(\r\n                                        bigquery.SchemaField(\"logger_direction\", \"STRING\")\r\n                                    )\r\n                                    logger_portal_table_schema.append(\r\n                                        bigquery.SchemaField(\"location\", \"STRING\")\r\n                                    )\r\n\r\n                                    #  Grab field portal data average\r\n                                    print('\\t\\tGrabbing field average data...')\r\n                                    average_field_portal_data = self.average_field_portal_data(field_loggers_portal_data)\r\n\r\n                                    # Process field portal data average\r\n                                    print('\\t\\tProcessing field portal data...')\r\n                                    processed_average_field_portal_data = self.cwsi_processor.process_data_for_writing_db_portal(\r\n                                        average_field_portal_data, self\r\n                                    )\r\n\r\n                                    # Write field portal data\r\n                                    print('\\t\\tWriting field portal data...')\r\n                                    grower_name = self.dbwriter.remove_unwanted_chars_for_db_dataset(self.grower.name)\r\n                                    field_averages_portal_dataset_id = FIELD_PORTALS_BIGQUERY_PROJECT + '.' + grower_name + '.field_averages'\r\n                                    logger_portal_dataset_id = FIELD_PORTALS_BIGQUERY_PROJECT + '.' + grower_name + '.loggers'\r\n\r\n                                    # Check field portal table for field\r\n                                    value_exists = self.check_if_row_value_exists_in_table(\r\n                                        field_averages_portal_dataset_id, 'field', self.nickname, FIELD_PORTALS_BIGQUERY_PROJECT\r\n                                    )\r\n                                    # If it's there, replace specific column values\r\n                                    if value_exists:\r\n                                        # update_portal_table_value(self, dbw, dataset_id, column_name, value_name, processed_portal_data)\r\n                                        self.update_portal_table_value(\r\n                                            field_averages_portal_dataset_id, 'field', self.nickname,\r\n                                            processed_average_field_portal_data\r\n                                        )\r\n                                    else:\r\n                                        # If not, write new row for field\r\n                                        self.write_portal_row(\r\n                                            field_averages_table_schema, grower_name, processed_average_field_portal_data,\r\n                                            'field_averages'\r\n                                        )\r\n\r\n                                    # Process logger portal data\r\n                                    print()\r\n                                    print('\\t\\tProcessing logger portal data...')\r\n                                    processed_logger_portal_data = []\r\n                                    for logger in self.loggers:\r\n                                        if logger.id in field_loggers_portal_data:\r\n                                            if logger.nickname != '':\r\n                                                logger_name = logger.nickname\r\n                                            else:\r\n                                                logger_name = logger.name\r\n                                            processed_logger_portal_data.append(\r\n                                                self.cwsi_processor.process_data_for_writing_db_portal(\r\n                                                    field_loggers_portal_data[logger.id],\r\n                                                    self,\r\n                                                    logger_name=logger_name,\r\n                                                    logger_direction=logger.logger_direction,\r\n                                                    logger_lat=float(logger.lat),\r\n                                                    logger_long=float(logger.long)\r\n                                                )\r\n                                            )\r\n                                    print('\\t\\tWriting logger portal data...')\r\n                                    for logger_processed_data in processed_logger_portal_data:\r\n                                        value_exists = self.check_if_row_value_exists_in_table(\r\n                                            logger_portal_dataset_id, 'logger_name', logger_processed_data[\"logger_name\"],\r\n                                            FIELD_PORTALS_BIGQUERY_PROJECT\r\n                                        )\r\n                                        # If it's there, replace specific column values\r\n                                        if value_exists:\r\n                                            # update_portal_table_value(self, dbw, dataset_id, column_name, value_name, processed_portal_data)\r\n                                            self.update_portal_table_value(\r\n                                                logger_portal_dataset_id, 'logger_name', logger_processed_data[\"logger_name\"],\r\n                                                logger_processed_data\r\n                                            )\r\n                                        else:\r\n                                            # If not, write new row for field\r\n                                            self.write_portal_row(\r\n                                                logger_portal_table_schema, grower_name, logger_processed_data, 'loggers'\r\n                                            )\r\n                                    print('\\t<<< Done with ', self.name, ' Portal Data')\r\n                                else:\r\n                                    print('\\tNothing new to write to portal')\r\n                        except Exception as error:\r\n                            print(\"Error in field portal data - \" + self.name)\r\n                            print(\"Error type: \" + str(error))\r\n                    print(str(self.grower.name) + ' - ' + str(self.name) + ' Done Updating-')\r\n                    print()\r\n        else:\r\n            print('Field - {} not active'.format(self.name))\r\n\r\n    def write_portal_row(self, table_schema, grower_name: str, processed_portal_data: dict, table_name: str):\r\n        print('\\t\\t\\tdata not already in table')\r\n        print('\\t\\t\\tWriting new row')\r\n        filename = 'portal_data.csv'\r\n        print('\\t\\t\\t- writing data to csv')\r\n        with open(filename, \"w\", newline='') as outfile:\r\n            writer = csv.writer(outfile)\r\n            writer.writerow(processed_portal_data.keys())\r\n            # Using zip_longest because dict rows are uneven length due to daily_switch algo issue\r\n            # This will add full null rows for any additional daily_switch list values\r\n            writer.writerow(processed_portal_data.values())\r\n        print('\\t\\t\\t...Done - file: ' + filename)\r\n        self.dbwriter.write_to_table_from_csv(\r\n            grower_name, table_name, filename, table_schema, project=FIELD_PORTALS_BIGQUERY_PROJECT\r\n        )\r\n\r\n    def update_portal_table_value(self, dataset_id: str, column_name: str, value_name: str,\r\n                                  processed_portal_data: dict):\r\n        print('\\t\\t\\tData already found in table')\r\n        print('\\t\\t\\tUpdating table values')\r\n\r\n        # DML statement doesn't like None for update, so we changed to 'null'\r\n        if processed_portal_data[\"si_num\"] is None:\r\n            processed_portal_data[\"si_num\"] = 'null'\r\n        if processed_portal_data[\"soil_moisture_num\"] is None:\r\n            processed_portal_data[\"soil_moisture_num\"] = 'null'\r\n\r\n        if 'location' in processed_portal_data:\r\n            # This is a logger portal update\r\n            dml = \"UPDATE \" + dataset_id + \" as t SET t.order = \" + str(processed_portal_data[\"order\"]) \\\r\n                  + \", t.soil_moisture_num = \" + str(processed_portal_data[\"soil_moisture_num\"]) \\\r\n                  + \", t.soil_moisture_desc = '\" + str(processed_portal_data[\"soil_moisture_desc\"]) + \"'\" \\\r\n                  + \", t.si_num = \" + str(processed_portal_data[\"si_num\"]) \\\r\n                  + \", t.si_desc = '\" + str(processed_portal_data[\"si_desc\"]) + \"'\" \\\r\n                  + \", t.crop_image = '\" + str(processed_portal_data[\"crop_image\"]) + \"'\" \\\r\n                  + \", t.report = '\" + str(processed_portal_data[\"report\"]) + \"'\" \\\r\n                  + \", t.preview = '\" + str(processed_portal_data[\"preview\"]) + \"'\" \\\r\n                  + \", t.field = '\" + str(processed_portal_data[\"field\"]) + \"'\" \\\r\n                  + \", t.location = '\" + str(processed_portal_data[\"location\"]) + \"'\" \\\r\n                  + \" WHERE t.\" + column_name + \" = '\" + str(value_name) + \"'\"\r\n        else:\r\n            # This is a field portal update\r\n            dml = \"UPDATE \" + dataset_id + \" as t SET t.order = \" + str(processed_portal_data[\"order\"]) \\\r\n              + \", t.soil_moisture_num = \" + str(processed_portal_data[\"soil_moisture_num\"]) \\\r\n              + \", t.soil_moisture_desc = '\" + str(processed_portal_data[\"soil_moisture_desc\"]) + \"'\" \\\r\n              + \", t.si_num = \" + str(processed_portal_data[\"si_num\"]) \\\r\n              + \", t.si_desc = '\" + str(processed_portal_data[\"si_desc\"]) + \"'\" \\\r\n              + \", t.crop_image = '\" + str(processed_portal_data[\"crop_image\"]) + \"'\" \\\r\n              + \", t.report = '\" + str(processed_portal_data[\"report\"]) + \"'\" \\\r\n              + \", t.preview = '\" + str(processed_portal_data[\"preview\"]) + \"'\" \\\r\n              + \", t.field = '\" + str(processed_portal_data[\"field\"]) + \"'\" \\\r\n              + \" WHERE t.\" + column_name + \" = '\" + str(value_name) + \"'\"\r\n        # print('\\t\\t\\t  - ' + str(dml))\r\n        self.dbwriter.run_dml(dml, project=FIELD_PORTALS_BIGQUERY_PROJECT)\r\n\r\n    def check_if_row_value_exists_in_table(self, dataset_id: str, column_name: str, value_name: str,\r\n                                           project: str) -> bool:\r\n        dml = \"SELECT \" + column_name + \" FROM \" + dataset_id + \" WHERE \" + column_name + \" = '\" + value_name + \"'\"\r\n        result = self.dbwriter.run_dml(dml, project=project)\r\n        if len(list(result)) >= 1:\r\n            return True\r\n        return False\r\n\r\n    def average_field_portal_data(self, all_loggers_portal_data: dict) -> dict:\r\n        field_loggers = list(all_loggers_portal_data.keys())\r\n        data_keys = list(all_loggers_portal_data[field_loggers[0]].keys())\r\n        average_field_portal_data = dict.fromkeys(data_keys)\r\n\r\n        for each_logger in all_loggers_portal_data:\r\n            if all_loggers_portal_data[each_logger]['dates'] is not None:\r\n                average_field_portal_data['dates'] = all_loggers_portal_data[each_logger]['dates']\r\n            break\r\n\r\n        canopy_temps = []\r\n        ambient_temps = []\r\n        cwsis = []\r\n        sdds = []\r\n        rhs = []\r\n        vpds = []\r\n        vwc_1s = []\r\n        vwc_2s = []\r\n        vwc_3s = []\r\n        vwc_1_ecs = []\r\n        vwc_2_ecs = []\r\n        vwc_3_ecs = []\r\n        daily_switchs = []\r\n        kcs = []\r\n\r\n        for logger_data in all_loggers_portal_data:\r\n            canopy_temperature = all_loggers_portal_data[logger_data]['canopy temperature']\r\n            ambient_temperature = all_loggers_portal_data[logger_data]['ambient temperature']\r\n            cwsi = all_loggers_portal_data[logger_data]['cwsi']\r\n            sdd = all_loggers_portal_data[logger_data]['sdd']\r\n            rh = all_loggers_portal_data[logger_data]['rh']\r\n            vpd = all_loggers_portal_data[logger_data]['vpd']\r\n            vwc_1 = all_loggers_portal_data[logger_data]['vwc_1']\r\n            vwc_2 = all_loggers_portal_data[logger_data]['vwc_2']\r\n            vwc_3 = all_loggers_portal_data[logger_data]['vwc_3']\r\n            vwc_1_ec = all_loggers_portal_data[logger_data]['vwc_1_ec']\r\n            vwc_2_ec = all_loggers_portal_data[logger_data]['vwc_2_ec']\r\n            vwc_3_ec = all_loggers_portal_data[logger_data]['vwc_3_ec']\r\n            daily_switch = all_loggers_portal_data[logger_data]['daily switch']\r\n            if 'kc' in all_loggers_portal_data[logger_data].keys():\r\n                kc = all_loggers_portal_data[logger_data]['kc']\r\n            else:\r\n                kc = None\r\n\r\n            if canopy_temperature is not None:\r\n                canopy_temps.append(canopy_temperature)\r\n            if ambient_temperature is not None:\r\n                ambient_temps.append(ambient_temperature)\r\n            if cwsi is not None:\r\n                cwsis.append(cwsi)\r\n            if sdd is not None:\r\n                sdds.append(sdd)\r\n            if rh is not None:\r\n                rhs.append(rh)\r\n            if vpd is not None:\r\n                vpds.append(vpd)\r\n            if vwc_1 is not None:\r\n                vwc_1s.append(vwc_1)\r\n            if vwc_2 is not None:\r\n                vwc_2s.append(vwc_2)\r\n            if vwc_3 is not None:\r\n                vwc_3s.append(vwc_3)\r\n            if vwc_1_ec is not None:\r\n                vwc_1_ecs.append(vwc_1_ec)\r\n            if vwc_2_ec is not None:\r\n                vwc_2_ecs.append(vwc_2_ec)\r\n            if vwc_3_ec is not None:\r\n                vwc_3_ecs.append(vwc_3_ec)\r\n            if daily_switch is not None:\r\n                daily_switchs.append(daily_switch)\r\n            if kc is not None:\r\n                kcs.append(kc)\r\n\r\n        if len(canopy_temps) > 0:\r\n            average_field_portal_data['canopy temperature'] = numpy.mean(canopy_temps)\r\n        if len(ambient_temps) > 0:\r\n            average_field_portal_data['ambient temperature'] = numpy.mean(ambient_temps)\r\n        if len(cwsis) > 0:\r\n            average_field_portal_data['cwsi'] = numpy.mean(cwsis)\r\n        if len(sdds) > 0:\r\n            average_field_portal_data['sdd'] = numpy.mean(sdds)\r\n        if len(rhs) > 0:\r\n            average_field_portal_data['rh'] = numpy.mean(rhs)\r\n        if len(vpds) > 0:\r\n            average_field_portal_data['vpd'] = numpy.mean(vpds)\r\n        if len(vwc_1s) > 0:\r\n            average_field_portal_data['vwc_1'] = numpy.mean(vwc_1s)\r\n        if len(vwc_2s) > 0:\r\n            average_field_portal_data['vwc_2'] = numpy.mean(vwc_2s)\r\n        if len(vwc_3s) > 0:\r\n            average_field_portal_data['vwc_3'] = numpy.mean(vwc_3s)\r\n        if len(vwc_1_ecs) > 0:\r\n            average_field_portal_data['vwc_1_ec'] = numpy.mean(vwc_1_ecs)\r\n        if len(vwc_2_ecs) > 0:\r\n            average_field_portal_data['vwc_2_ec'] = numpy.mean(vwc_2_ecs)\r\n        if len(vwc_3_ecs) > 0:\r\n            average_field_portal_data['vwc_3_ec'] = numpy.mean(vwc_3_ecs)\r\n        if len(daily_switchs) > 0:\r\n            average_field_portal_data['daily switch'] = numpy.mean(daily_switchs)\r\n        if len(kcs) > 0:\r\n            average_field_portal_data['kc'] = numpy.mean(kcs)\r\n\r\n        return average_field_portal_data\r\n\r\n    def get_weather_forecast(self) -> list:\r\n        \"\"\"\r\n        Function used to get the weather forecast. Uses the weatherProcessor class to call the yahoo\r\n            weather API and get the forecast for the next 5 days\r\n        :return:\r\n            weatherMatrix: Matrix with weather forecast ready to be written to GSheet\r\n            weatherIconMatrix: Matrix with corresponding weather icons for forecast ready to be written to GSheet\r\n        \"\"\"\r\n        print()\r\n        # ## Uncomment to use Open Weather API\r\n        # print('\\tGetting weather forecast using Open Weather-')\r\n        # forecast = self.weatherProcessor.open_weather_forecast()\r\n        # ##\r\n\r\n        ## Uncomment to use Apple Weather Kit API\r\n        print('\\tGetting weather forecast using Apple Weather Kit-')\r\n        forecast = self.weather_processor.apple_forecast()\r\n        ##\r\n\r\n        print()\r\n\r\n        return forecast\r\n\r\n    def prep_weather_data_for_writing_to_db(self, forecast: list[dict]) -> (list, str):\r\n        weather_schema = [\r\n            bigquery.SchemaField(\"date\", \"DATE\"),\r\n            bigquery.SchemaField(\"day\", \"STRING\"),\r\n            bigquery.SchemaField(\"order\", \"FLOAT\"),\r\n            bigquery.SchemaField(\"temp\", \"FLOAT\"),\r\n            bigquery.SchemaField(\"rh\", \"FLOAT\"),\r\n            bigquery.SchemaField(\"vpd\", \"FLOAT\"),\r\n            bigquery.SchemaField(\"icon\", \"STRING\"),\r\n        ]\r\n        weather_filename = 'weather forecast.csv'\r\n        order = 0\r\n\r\n        weather_data = {\"date\": [], \"day\": [], \"order\": [], \"temp\": [], \"rh\": [], \"vpd\": [], \"icon\": []}\r\n        for ind, data_point in enumerate(forecast):\r\n            date = data_point['time']\r\n            date_string_format = date.strftime(\"%Y-%m-%d\")\r\n            date_day = date.strftime('%a')\r\n            max_temp = data_point['max_temp']\r\n            relative_humidity = data_point['humidity']\r\n            vpd = data_point['vpd']\r\n            forecastText = data_point['icon']\r\n            relative_humidity_percentage = relative_humidity * 100\r\n            order = ind + 1\r\n\r\n            weather_data[\"date\"].append(date_string_format)\r\n            weather_data[\"day\"].append(date_day)\r\n            weather_data[\"order\"].append(order)\r\n            weather_data[\"temp\"].append(round(max_temp, 1))\r\n            weather_data[\"rh\"].append(round(relative_humidity_percentage, 1))\r\n            weather_data[\"vpd\"].append(round(vpd, 1))\r\n            weather_data[\"icon\"].append(forecastText)\r\n\r\n        weather_data = self.add_extra_blank_days(forecast, order, weather_data)\r\n\r\n        print('\\t\\tWriting weather data to csv')\r\n        with open(weather_filename, \"w\", newline='') as outfile:\r\n            writer = csv.writer(outfile)\r\n            writer.writerow(weather_data.keys())\r\n            # Using zip_longest because dict rows are uneven length due to daily_switch algo issue\r\n            # This will add full null rows for any additional daily_switch list values\r\n            writer.writerows(zip_longest(*weather_data.values()))\r\n        print('\\t\\tDone - file: ' + weather_filename)\r\n        print()\r\n\r\n        return weather_schema, weather_filename\r\n\r\n    def add_extra_blank_days(self, forecast: list, order: int, weather_data: dict):\r\n\r\n        current_forecast_days = len(weather_data[\"date\"])\r\n        days_to_add = 10 - current_forecast_days\r\n\r\n        if days_to_add > 0:\r\n            for x in range(days_to_add):\r\n                extra_day = (forecast[-1]['time']) + timedelta(days=x + 1)\r\n                extra_day_string = extra_day.strftime(\"%Y-%m-%d\")\r\n                weather_data[\"date\"].extend([extra_day_string])\r\n                weather_data[\"day\"].extend([None])\r\n                weather_data[\"order\"].extend([order + x + 1])\r\n                weather_data[\"temp\"].extend([None])\r\n                weather_data[\"rh\"].extend([None])\r\n                weather_data[\"vpd\"].extend([None])\r\n                weather_data[\"icon\"].extend([None])\r\n\r\n        return weather_data\r\n\r\n    def prep_db_for_weather(self, forecast: list):\r\n        dbw = DBWriter()\r\n        field_name = dbw.remove_unwanted_chars_for_db_dataset(self.name)\r\n        project = dbw.get_db_project(self.loggers[0].crop_type)\r\n        table_exsists = dbw.check_if_table_exists(field_name, 'weather_forecast', project=project)\r\n        if table_exsists:\r\n            # Change the order of the previous day to 99\r\n            print('\\t\\tChanging weather order from prev day to 99')\r\n            self.change_order_from_previous_day(forecast)\r\n            # Delete all data that is between the new ranges for forecast\r\n            print('\\t\\tRemoving redundant data')\r\n            self.remove_data_that_is_about_to_be_updated(forecast)\r\n\r\n    def change_order_from_previous_day(self, forecast: list):\r\n        dbw = DBWriter()\r\n        field = dbw.remove_unwanted_chars_for_db_dataset(self.name)\r\n        first_date = (forecast[0]['time'])\r\n        # first_date_string = first_date.strftime(\"%Y-%m-%d\")\r\n        previous_date = first_date - timedelta(days=1)\r\n        previous_date_string = previous_date.strftime(\"%Y-%m-%d\")\r\n        project = dbw.get_db_project(self.loggers[0].crop_type)\r\n        dml = \"UPDATE `\" + project + \".\" + field + \".weather_forecast` as t SET t.order = 99.0 WHERE date <= '\" + previous_date_string + \"'\"\r\n        dbw.run_dml(dml, project=project)\r\n\r\n    def remove_data_that_is_about_to_be_updated(self, forecast: list):\r\n        dbw = DBWriter()\r\n        field = dbw.remove_unwanted_chars_for_db_dataset(self.name)\r\n        first_date = (forecast[0]['time'])\r\n        last_date = (forecast[-1]['time']) + timedelta(days=2)\r\n        first_date_string = first_date.strftime(\"%Y-%m-%d\")\r\n        last_date_string = last_date.strftime(\"%Y-%m-%d\")\r\n        project = dbw.get_db_project(self.loggers[0].crop_type)\r\n        dml = \"DELETE `\" + project + \".\" + field + \".weather_forecast` \" \\\r\n                                                   \"WHERE date BETWEEN DATE('\" + first_date_string + \"') AND DATE('\" + last_date_string + \"') \"\r\n        # print(dml)\r\n        dbw.run_dml(dml, project=project)\r\n\r\n    def update_et_tables(self):\r\n        # latest_et = self.get_latest_et()\r\n        for logger in self.loggers:\r\n            print('\\tUpdating et values in Logger table...')\r\n            try:\r\n                logger.merge_et_db_with_logger_db_values()\r\n            except Exception as err:\r\n                print(\"ET Did not update for this logger\")\r\n                print(err)\r\n\r\n    def get_number_of_active_loggers(self) -> (int, int):\r\n        active_loggers = 0\r\n        inactive_loggers = 0\r\n        for logger in self.loggers:\r\n            if logger.active:\r\n                active_loggers += 1\r\n            else:\r\n                inactive_loggers += 1\r\n        return active_loggers, inactive_loggers\r\n\r\n    def check_for_notifications(self, weather_data):\r\n        \"\"\"\r\n        Function to check for notifications related to the weather forecast data.\r\n\r\n        :param weather_data:\r\n            2-dimensional list of the weather forecast data before it is written to GSheets. Format of the list is:\r\n            [\r\n                [day],\r\n                [temperature highs],\r\n                [temperature lows]\r\n            ]\r\n\r\n        :return:\r\n        \"\"\"\r\n        thresholds = Thresholds()\r\n        weather_threshold = thresholds.weather_threshold\r\n        consecutive_temps = thresholds.consecutive_temps\r\n        all_weather_results = {\"days\": [],\r\n                               \"temps\": []}\r\n        consecutive_weather_results = {\"days\": [],\r\n                                       \"temps\": []}\r\n\r\n        i = 0\r\n        j = 0\r\n        length = len(weather_data[1])\r\n        while i < length:\r\n            if int(weather_data[1][i]) >= weather_threshold:\r\n                all_weather_results[\"temps\"].insert(j, [int(weather_data[1][i])])\r\n                all_weather_results[\"days\"].insert(j, [weather_data[0][i]])\r\n                i += 1\r\n                while i < len(weather_data[1]):\r\n                    if int(weather_data[1][i]) >= weather_threshold:\r\n                        all_weather_results[\"temps\"][j].append(int(weather_data[1][i]))\r\n                        all_weather_results[\"days\"][j].append(weather_data[0][i])\r\n                        i += 1\r\n                    else:\r\n                        break\r\n                j += 1\r\n            i += 1\r\n        for ind, i in enumerate(all_weather_results[\"temps\"]):\r\n            if len(i) >= consecutive_temps:\r\n                consecutive_weather_results[\"temps\"].append(all_weather_results[\"temps\"][ind])\r\n                consecutive_weather_results[\"days\"].append(all_weather_results[\"days\"][ind])\r\n\r\n        if consecutive_weather_results[\"temps\"]:\r\n            self.all_notifications.add_notification(\r\n                datetime.now(), str(self.grower.name) + \" - \" + self.name,\r\n                self.field_string, \"Weather\", consecutive_weather_results,\r\n                weather_threshold, \"were greater than\"\r\n            )\r\n\r\n    def deactivate(self):\r\n        print('Deactivating Field {}...'.format(self.name))\r\n        self.active = False\r\n        for logger in self.loggers:\r\n            logger.deactivate()\r\n        print('Done')\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/Field.py b/Field.py
--- a/Field.py	(revision 5cf7d78a1d94023d00661edf5517badacc064130)
+++ b/Field.py	(date 1720049462095)
@@ -240,7 +240,8 @@
                             logger_portal_data = logger.update(cimis_stations_pickle, write_to_db=write_to_db,
                                                                check_for_notifications=check_for_notifications,
                                                                check_updated=check_updated,
-                                                               subtract_from_mrid=subtract_from_mrid)  # do logger updates
+                                                               subtract_from_mrid=subtract_from_mrid,
+                                                               specific_mrid=specific_mrid)  # do logger updates
                             if logger_portal_data is not None:
                                 field_loggers_portal_data[logger.id] = logger_portal_data
                         self.check_successful_updated_loggers()
Index: .idea/vcs.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+><?xml version=\"1.0\" encoding=\"UTF-8\"?>\r\n<project version=\"4\">\r\n  <component name=\"VcsDirectoryMappings\">\r\n    <mapping directory=\"$PROJECT_DIR$\" vcs=\"Git\" />\r\n  </component>\r\n</project>
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/.idea/vcs.xml b/.idea/vcs.xml
--- a/.idea/vcs.xml	(revision 5cf7d78a1d94023d00661edf5517badacc064130)
+++ b/.idea/vcs.xml	(date 1720207096959)
@@ -2,5 +2,6 @@
 <project version="4">
   <component name="VcsDirectoryMappings">
     <mapping directory="$PROJECT_DIR$" vcs="Git" />
+    <mapping directory="$PROJECT_DIR$/pysda" vcs="Git" />
   </component>
 </project>
\ No newline at end of file
Index: SlackBot.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/SlackBot.py b/SlackBot.py
new file mode 100644
--- /dev/null	(date 1720064110058)
+++ b/SlackBot.py	(date 1720064110058)
@@ -0,0 +1,313 @@
+import os
+from slack_bolt import App
+from dotenv import load_dotenv
+import PickleHandler
+from flask import Flask, request
+from slack_bolt.adapter.flask import SlackRequestHandler
+import LoggerSetups
+import SQLScripts
+from collections import defaultdict
+
+# Load environment variables from .env file
+load_dotenv()
+
+# Initialize your app with your bot token and signing secret
+app = App(
+    token=os.getenv("SLACK_BOT_TOKEN"),
+    signing_secret=os.getenv("SLACK_SIGNING_SECRET")
+)
+flask_app = Flask(__name__)
+handler = SlackRequestHandler(app)
+
+user_selections = defaultdict(dict)
+
+
+# This will match any message that contains 👋
+@app.message(":wave:")
+def say_hello(message, say):
+    user = message['user']
+    say(f"Hi there, <@{user}>!")
+
+
+@app.event("app_home_opened")
+def say_hw(message, say):
+    say("hello world")
+
+
+# Example lists for dropdown options
+growers = PickleHandler.open_pickle()
+grower_names = [grower.name for grower in growers]
+fields = [field.name for grower in growers for field in grower.fields]
+
+
+
+
+
+
+
+def generate_options(list_of_items):
+    return [
+        {
+            "text": {
+                "text": item,
+                "type": "plain_text"
+            },
+            "value": item
+        } for item in list_of_items
+    ]
+
+@app.command("/menu")
+def menu_command(ack, body, respond):
+    # Acknowledge command request
+    ack()
+
+    # Get the user who invoked the command
+    user_id = body["user_id"]
+    menu_options = ['Get Soil Type', 'Change Soil Type']
+    main_menu(ack, respond, menu_options)
+
+
+@app.action("get_soil_type")
+def handle_get_soil(ack, body, respond):
+    # Acknowledge command request
+    ack()
+
+    # Get the user who invoked the command
+    user_id = body["user_id"]
+    coords = body['text'].split(" ")
+    lat = coords[0]
+    long = coords[1]
+    soil = LoggerSetups.get_soil_type_from_coords(lat, long)
+    respond(f"Soil type at {coords}{soil}")
+
+@app.action("soil_select")
+@app.action("logger_select")
+def handle_selections(ack, body, respond):
+    ack()
+    user_id = body['user']['id']
+    action_id = body['actions'][0]['action_id']
+
+    if action_id == 'soil_select':
+        selected_option = body['actions'][0]['selected_option']['value']
+        user_selections[user_id][action_id] = selected_option
+    elif action_id == 'logger_select':  # logger_select
+        selected_options = body['actions'][0]['selected_options']
+        user_selections[user_id][action_id] = [option['value'] for option in selected_options]
+
+    # Check if both selections are complete
+    if 'soil_select' in user_selections[user_id] and 'logger_select' in user_selections[user_id]:
+        loggers = user_selections[user_id]['logger_select']
+        soil_type = user_selections[user_id]['soil_select']
+        field = user_selections[user_id]['field']
+        grower = user_selections[user_id]['grower']
+
+        for logger in loggers:
+            SQLScripts.change_logger_soil_type(logger, field, grower, soil_type)
+        response_text = f"Changing the following:\nLoggers: {', '.join(loggers)} to Soil Type: {soil_type}"
+        respond(text=response_text)
+
+        # Clear the selections for this user
+        del user_selections[user_id]
+    else:
+        # If both selections aren't complete, don't respond yet
+        pass
+
+
+@app.action("field_select")
+def handle_field_select(ack, body, respond):
+    ack()
+    # Extract values from the actions
+    field = body['actions'][0]['selected_options'][0]['value']
+    respond(f"You selected field: {field}")
+    field_obj = PickleHandler.get_field(field)
+
+    # Add to selections
+    user_id = body['user']['id']
+    user_selections[user_id]['field'] = field
+
+    logger_list = [logger.name for logger in field_obj.loggers]
+    soil_types = ['Sand', 'Loamy Sand', 'Sandy Loam', 'Sandy Clay Loam', 'Loam', 'Sandy Clay', 'Silt Loam', 'Silt',
+                  'Clay Loam', 'Silty Clay Loam', 'Silty Clay', 'Clay']
+    logger_and_soil_list_menu(ack, respond, logger_list, soil_types)
+
+
+@app.action("grower_select")
+def handle_grower_menu(ack, body, respond):
+    ack()
+    # Extract values from the actions
+    selected_value = body['actions'][0]['selected_options'][0]['value']
+    grower = PickleHandler.get_grower(selected_value)
+    field_list = [field.name for field in grower.fields]
+
+    # Add to selections
+    user_id = body['user']['id']
+    user_selections[user_id]['grower'] = grower
+
+    field_list_menu(ack, respond, field_list)
+
+@app.action("menu_select")
+def handle_main_menu(ack, body, respond):
+    ack()
+    # Extract values from the actions
+    menu_option = body['actions'][0]['selected_options'][0]['value']
+    if menu_option == 'Change Soil Type':
+        change_soil_menu(ack, body, respond)
+    elif menu_option == 'Get Soil Type':
+        handle_get_soil(ack, body, respond)
+
+
+
+@app.command("/change_soil")
+def change_soil_menu(ack, body, respond):
+    ack()
+
+    response = {
+        "response_type": "in_channel",
+        "text": "Change Soil Type",
+        "attachments": [
+            {
+                "text": "Choose Grower",
+                "fallback": "You are unable to choose an option",
+                "color": "#3AA3E3",
+                "attachment_type": "default",
+                "callback_id": "grower_select",
+                "actions": [
+                    {
+                        "name": "grower_list",
+                        "text": "Pick a grower...",
+                        "type": "select",
+                        "options": generate_options(grower_names)
+                    }
+                ]
+            }
+        ]
+    }
+
+    respond(response)
+
+
+def field_list_menu(ack, respond, field_list):
+    ack()
+
+    response = {
+        "response_type": "in_channel",
+        "text": "Choose Grower Field",
+        "attachments": [
+            {
+                "text": "Choose Grower Field",
+                "fallback": "You are unable to choose an option",
+                "color": "#3AA3E3",
+                "attachment_type": "default",
+                "callback_id": "field_select",
+                "actions": [
+                    {
+                        "name": "field_list",
+                        "text": "Pick a field...",
+                        "type": "select",
+                        "options": generate_options(field_list)
+                    }
+                ]
+            },
+        ]
+    }
+
+    respond(response)
+
+
+def logger_and_soil_list_menu(ack, respond, logger_list, soil_types):
+    ack()
+
+    blocks = [
+        {
+            "type": "section",
+            "text": {
+                "type": "mrkdwn",
+                "text": "Choose logger(s) to change"
+            },
+            "accessory": {
+                "type": "multi_static_select",
+                "placeholder": {
+                    "type": "plain_text",
+                    "text": "Select logger(s)",
+                    "emoji": True
+                },
+                "options": generate_options(logger_list),
+                "action_id": "logger_select"
+            }
+        },
+        {
+            "type": "section",
+            "text": {
+                "type": "mrkdwn",
+                "text": "Choose Soil Type"
+            },
+            "accessory": {
+                "type": "static_select",
+                "placeholder": {
+                    "type": "plain_text",
+                    "text": "Select Soil Type",
+                    "emoji": True
+                },
+                "options": generate_options(soil_types),
+                "action_id": "soil_select"
+            }
+        }
+    ]
+
+    respond(blocks=blocks)
+
+def main_menu(ack, respond, menu_options):
+    ack()
+
+    blocks = [
+        {
+            "type": "section",
+            "text": {
+                "type": "mrkdwn",
+                "text": "Menu"
+            },
+            "accessory": {
+                "type": "static_select",
+                "placeholder": {
+                    "type": "plain_text",
+                    "text": "Choose an option",
+                    "emoji": True
+                },
+                "options": generate_options(menu_options),
+                "action_id": "menu_select"
+            }
+        }
+    ]
+
+    respond(blocks=blocks)
+
+
+def get_soil_menu(ack, respond, menu_options):
+    ack()
+
+    blocks = [
+        {
+            "type": "input",
+            "text": {
+                "type": "plain_text_input",
+                "text": "Get Soil Type"
+            },
+            "accessory": {
+                "type": "plain_text_input",
+                "placeholder": {
+                    "type": "plain_text_input",
+                    "text": "Enter coordinates with a space separating",
+                    "emoji": True
+                },
+                "options": generate_options(menu_options),
+                "action_id": "get_soil_type"
+            }
+        }
+    ]
+
+    respond(blocks=blocks)
+
+
+# Ready? Start your app!
+if __name__ == "__main__":
+    app.start(port=int(os.getenv("PORT", 3000)))
Index: .env
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/.env b/.env
new file mode 100644
--- /dev/null	(date 1719605229780)
+++ b/.env	(date 1719605229780)
@@ -0,0 +1,3 @@
+
+SLACK_BOT_TOKEN = 'xoxb-155423053363-7345200304579-ZgDBFDxKxJ5sB2u1Adc3r0f3'
+SLACK_SIGNING_SECRET = '680501ed3370ebdb9e029411b853aa8e'
\ No newline at end of file
Index: .idea/workspace.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+><?xml version=\"1.0\" encoding=\"UTF-8\"?>\r\n<project version=\"4\">\r\n  <component name=\"AnalysisUIOptions\">\r\n    <option name=\"SCOPE_TYPE\" value=\"3\" />\r\n  </component>\r\n  <component name=\"AutoImportSettings\">\r\n    <option name=\"autoReloadType\" value=\"SELECTIVE\" />\r\n  </component>\r\n  <component name=\"ChangeListManager\">\r\n    <list default=\"true\" id=\"3aa7c90b-1f87-403f-bac4-0b49ed107be9\" name=\"Default\" comment=\"VP4 Notification Changes&#10;&#10;Simplified vp4 notifications into 1 single method to handle air temp, rh, and vpd notifications. This allows us to just create a single notification if any of those are None instead of one for each type since it is almost never the case that just 1 fails.&#10;New method is called vp4_notifications.&#10;Removed old individual methods for air temp notifications, rh notifications, and vpd notifications.&#10;Formatting changes.\">\r\n      <change beforePath=\"$PROJECT_DIR$/.idea/misc.xml\" beforeDir=\"false\" afterPath=\"$PROJECT_DIR$/.idea/misc.xml\" afterDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/.idea/workspace.xml\" beforeDir=\"false\" afterPath=\"$PROJECT_DIR$/.idea/workspace.xml\" afterDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/CimisUpdate.py\" beforeDir=\"false\" afterPath=\"$PROJECT_DIR$/CimisUpdate.py\" afterDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/Decagon.py\" beforeDir=\"false\" afterPath=\"$PROJECT_DIR$/Decagon.py\" afterDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/ai_data.csv\" beforeDir=\"false\" afterPath=\"$PROJECT_DIR$/ai_data.csv\" afterDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/all et.csv\" beforeDir=\"false\" afterPath=\"$PROJECT_DIR$/all et.csv\" afterDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/data.csv\" beforeDir=\"false\" afterPath=\"$PROJECT_DIR$/data.csv\" afterDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/irrScheduling.csv\" beforeDir=\"false\" afterPath=\"$PROJECT_DIR$/irrScheduling.csv\" afterDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/portal_data.csv\" beforeDir=\"false\" afterPath=\"$PROJECT_DIR$/portal_data.csv\" afterDir=\"false\" />\r\n      <change beforePath=\"$PROJECT_DIR$/weather forecast.csv\" beforeDir=\"false\" afterPath=\"$PROJECT_DIR$/weather forecast.csv\" afterDir=\"false\" />\r\n    </list>\r\n    <option name=\"SHOW_DIALOG\" value=\"false\" />\r\n    <option name=\"HIGHLIGHT_CONFLICTS\" value=\"true\" />\r\n    <option name=\"HIGHLIGHT_NON_ACTIVE_CHANGELIST\" value=\"false\" />\r\n    <option name=\"LAST_RESOLUTION\" value=\"IGNORE\" />\r\n  </component>\r\n  <component name=\"CreatePatchCommitExecutor\">\r\n    <option name=\"PATCH_PATH\" value=\"\" />\r\n  </component>\r\n  <component name=\"FileTemplateManagerImpl\">\r\n    <option name=\"RECENT_TEMPLATES\">\r\n      <list>\r\n        <option value=\"Python Script\" />\r\n      </list>\r\n    </option>\r\n  </component>\r\n  <component name=\"FindInProjectRecents\">\r\n    <findStrings>\r\n      <find>IN LOGGER</find>\r\n      <find>delete</find>\r\n      <find>vwc 36</find>\r\n      <find>failed_cimis.pickle</find>\r\n      <find>SB-140</find>\r\n      <find>'Final results</find>\r\n      <find>Final</find>\r\n      <find>Results</find>\r\n      <find>vp4_connected</find>\r\n      <find>MODEL</find>\r\n      <find>Grower</find>\r\n      <find>Nothing new</find>\r\n      <find>Converting</find>\r\n      <find>JK</find>\r\n      <find>Switch</find>\r\n      <find>prev_day_minutes</find>\r\n      <find>switch</find>\r\n      <find>OPC112</find>\r\n      <find>get_swi</find>\r\n      <find>OPCN2</find>\r\n      <find>get_sw</find>\r\n      <find>TP</find>\r\n      <find>F&amp;S</find>\r\n      <find>get_switch</find>\r\n      <find>TeixeiraR34</find>\r\n      <find>Ochoa</find>\r\n      <find>Carvalho342</find>\r\n      <find>DohertyD3</find>\r\n      <find>QuadH2</find>\r\n      <find>CM</find>\r\n    </findStrings>\r\n  </component>\r\n  <component name=\"Git.Merge.Settings\">\r\n    <option name=\"BRANCH\" value=\"origin/master\" />\r\n  </component>\r\n  <component name=\"Git.Rebase.Settings\">\r\n    <option name=\"NEW_BASE\" value=\"master\" />\r\n  </component>\r\n  <component name=\"Git.Settings\">\r\n    <option name=\"RECENT_BRANCH_BY_REPOSITORY\">\r\n      <map>\r\n        <entry key=\"$PROJECT_DIR$\" value=\"master\" />\r\n      </map>\r\n    </option>\r\n    <option name=\"RECENT_GIT_ROOT_PATH\" value=\"$PROJECT_DIR$\" />\r\n  </component>\r\n  <component name=\"GitSEFilterConfiguration\">\r\n    <file-type-list>\r\n      <filtered-out-file-type name=\"LOCAL_BRANCH\" />\r\n      <filtered-out-file-type name=\"REMOTE_BRANCH\" />\r\n      <filtered-out-file-type name=\"TAG\" />\r\n      <filtered-out-file-type name=\"COMMIT_BY_MESSAGE\" />\r\n    </file-type-list>\r\n  </component>\r\n  <component name=\"HighlightingSettingsPerFile\">\r\n    <setting file=\"mock:///\" root0=\"FORCE_HIGHLIGHTING\" />\r\n  </component>\r\n  <component name=\"JupyterTrust\" id=\"a5593add-7099-434b-9a5f-e365579fda73\" />\r\n  <component name=\"MarkdownSettingsMigration\">\r\n    <option name=\"stateVersion\" value=\"1\" />\r\n  </component>\r\n  <component name=\"ProblemsViewState\">\r\n    <option name=\"selectedIndex\" value=\"2\" />\r\n  </component>\r\n  <component name=\"ProjectFrameBounds\">\r\n    <option name=\"x\" value=\"-8\" />\r\n    <option name=\"y\" value=\"-8\" />\r\n    <option name=\"width\" value=\"1936\" />\r\n    <option name=\"height\" value=\"1100\" />\r\n  </component>\r\n  <component name=\"ProjectId\" id=\"1aMD9QCNddDn0kzcajQ5kQOkEBT\" />\r\n  <component name=\"ProjectLevelVcsManager\">\r\n    <ConfirmationsSetting value=\"2\" id=\"Add\" />\r\n  </component>\r\n  <component name=\"ProjectViewState\">\r\n    <option name=\"hideEmptyMiddlePackages\" value=\"true\" />\r\n    <option name=\"showLibraryContents\" value=\"true\" />\r\n  </component>\r\n  <component name=\"PropertiesComponent\">{\r\n  &quot;keyToString&quot;: {\r\n    &quot;WebServerToolWindowFactoryState&quot;: &quot;false&quot;,\r\n    &quot;git-widget-placeholder&quot;: &quot;master&quot;,\r\n    &quot;ignore.virus.scanning.warn.message&quot;: &quot;true&quot;,\r\n    &quot;last_opened_file_path&quot;: &quot;C:/Users/javie/PycharmProjects/Stomato&quot;,\r\n    &quot;node.js.detected.package.eslint&quot;: &quot;true&quot;,\r\n    &quot;node.js.detected.package.tslint&quot;: &quot;true&quot;,\r\n    &quot;node.js.selected.package.eslint&quot;: &quot;(autodetect)&quot;,\r\n    &quot;node.js.selected.package.tslint&quot;: &quot;(autodetect)&quot;,\r\n    &quot;nodejs_package_manager_path&quot;: &quot;npm&quot;,\r\n    &quot;settings.editor.selected.configurable&quot;: &quot;configurable.group.language&quot;,\r\n    &quot;vue.rearranger.settings.migration&quot;: &quot;true&quot;\r\n  }\r\n}</component>\r\n  <component name=\"PyConsoleOptionsProvider\">\r\n    <option name=\"myPythonConsoleState\">\r\n      <console-settings module-name=\"Stomato\" is-module-sdk=\"true\">\r\n        <option name=\"myUseModuleSdk\" value=\"true\" />\r\n        <option name=\"myModuleName\" value=\"Stomato\" />\r\n      </console-settings>\r\n    </option>\r\n  </component>\r\n  <component name=\"PyDebuggerOptionsProvider\">\r\n    <option name=\"mySupportGeventDebugging\" value=\"true\" />\r\n  </component>\r\n  <component name=\"RecentsManager\">\r\n    <key name=\"CopyFile.RECENT_KEYS\">\r\n      <recent name=\"C:\\Users\\javie\\PycharmProjects\\Stomato\" />\r\n      <recent name=\"C:\\Users\\javie\\Projects\\S-TOMAto\" />\r\n      <recent name=\"C:\\Users\\Javier\\Documents\\Stomato\" />\r\n    </key>\r\n    <key name=\"MoveFile.RECENT_KEYS\">\r\n      <recent name=\"C:\\Users\\javie\\PycharmProjects\\Stomato\\Logos\" />\r\n      <recent name=\"C:\\Users\\javie\\PycharmProjects\\Stomato\\AIGame\" />\r\n      <recent name=\"C:\\Users\\javie\\PycharmProjects\\Stomato\\AI Game\" />\r\n      <recent name=\"C:\\Users\\javie\\Projects\\S-TOMAto\" />\r\n      <recent name=\"C:\\Users\\javie\\Projects\\S-TOMAto\\ssl\" />\r\n    </key>\r\n  </component>\r\n  <component name=\"RunManager\" selected=\"Python.Decagon\">\r\n    <configuration default=\"true\" type=\"tests\" factoryName=\"Attests\">\r\n      <option name=\"INTERPRETER_OPTIONS\" value=\"\" />\r\n      <option name=\"PARENT_ENVS\" value=\"true\" />\r\n      <envs />\r\n      <option name=\"SDK_HOME\" value=\"\" />\r\n      <option name=\"WORKING_DIRECTORY\" value=\"\" />\r\n      <option name=\"IS_MODULE_SDK\" value=\"false\" />\r\n      <option name=\"ADD_CONTENT_ROOTS\" value=\"true\" />\r\n      <option name=\"ADD_SOURCE_ROOTS\" value=\"true\" />\r\n      <module name=\"Stomato\" />\r\n      <option name=\"SCRIPT_NAME\" value=\"\" />\r\n      <option name=\"CLASS_NAME\" value=\"\" />\r\n      <option name=\"METHOD_NAME\" value=\"\" />\r\n      <option name=\"FOLDER_NAME\" value=\"\" />\r\n      <option name=\"TEST_TYPE\" value=\"TEST_SCRIPT\" />\r\n      <option name=\"PATTERN\" value=\"\" />\r\n      <option name=\"USE_PATTERN\" value=\"false\" />\r\n      <method />\r\n    </configuration>\r\n    <configuration name=\"AIIrrigationGameV3 (2)\" type=\"PythonConfigurationType\" factoryName=\"Python\" temporary=\"true\" nameIsGenerated=\"true\">\r\n      <module name=\"Stomato\" />\r\n      <option name=\"INTERPRETER_OPTIONS\" value=\"\" />\r\n      <option name=\"PARENT_ENVS\" value=\"true\" />\r\n      <envs>\r\n        <env name=\"PYTHONUNBUFFERED\" value=\"1\" />\r\n      </envs>\r\n      <option name=\"SDK_HOME\" value=\"\" />\r\n      <option name=\"WORKING_DIRECTORY\" value=\"$PROJECT_DIR$/AI Game\" />\r\n      <option name=\"IS_MODULE_SDK\" value=\"true\" />\r\n      <option name=\"ADD_CONTENT_ROOTS\" value=\"true\" />\r\n      <option name=\"ADD_SOURCE_ROOTS\" value=\"true\" />\r\n      <EXTENSION ID=\"PythonCoverageRunConfigurationExtension\" runner=\"coverage.py\" />\r\n      <option name=\"SCRIPT_NAME\" value=\"$PROJECT_DIR$/AI Game/AIIrrigationGameV3.py\" />\r\n      <option name=\"PARAMETERS\" value=\"\" />\r\n      <option name=\"SHOW_COMMAND_LINE\" value=\"false\" />\r\n      <option name=\"EMULATE_TERMINAL\" value=\"false\" />\r\n      <option name=\"MODULE_MODE\" value=\"false\" />\r\n      <option name=\"REDIRECT_INPUT\" value=\"false\" />\r\n      <option name=\"INPUT_FILE\" value=\"\" />\r\n      <method v=\"2\" />\r\n    </configuration>\r\n    <configuration name=\"AIIrrigationGameV3 (3)\" type=\"PythonConfigurationType\" factoryName=\"Python\" temporary=\"true\" nameIsGenerated=\"true\">\r\n      <module name=\"Stomato\" />\r\n      <option name=\"INTERPRETER_OPTIONS\" value=\"\" />\r\n      <option name=\"PARENT_ENVS\" value=\"true\" />\r\n      <envs>\r\n        <env name=\"PYTHONUNBUFFERED\" value=\"1\" />\r\n      </envs>\r\n      <option name=\"SDK_HOME\" value=\"\" />\r\n      <option name=\"WORKING_DIRECTORY\" value=\"$PROJECT_DIR$/AIGame\" />\r\n      <option name=\"IS_MODULE_SDK\" value=\"true\" />\r\n      <option name=\"ADD_CONTENT_ROOTS\" value=\"true\" />\r\n      <option name=\"ADD_SOURCE_ROOTS\" value=\"true\" />\r\n      <EXTENSION ID=\"PythonCoverageRunConfigurationExtension\" runner=\"coverage.py\" />\r\n      <option name=\"SCRIPT_NAME\" value=\"$PROJECT_DIR$/AIGame/AIIrrigationGameV3.py\" />\r\n      <option name=\"PARAMETERS\" value=\"\" />\r\n      <option name=\"SHOW_COMMAND_LINE\" value=\"false\" />\r\n      <option name=\"EMULATE_TERMINAL\" value=\"false\" />\r\n      <option name=\"MODULE_MODE\" value=\"false\" />\r\n      <option name=\"REDIRECT_INPUT\" value=\"false\" />\r\n      <option name=\"INPUT_FILE\" value=\"\" />\r\n      <method v=\"2\" />\r\n    </configuration>\r\n    <configuration name=\"AIIrrigationGameV3 (4)\" type=\"PythonConfigurationType\" factoryName=\"Python\" temporary=\"true\" nameIsGenerated=\"true\">\r\n      <module name=\"Stomato\" />\r\n      <option name=\"INTERPRETER_OPTIONS\" value=\"\" />\r\n      <option name=\"PARENT_ENVS\" value=\"true\" />\r\n      <envs>\r\n        <env name=\"PYTHONUNBUFFERED\" value=\"1\" />\r\n      </envs>\r\n      <option name=\"SDK_HOME\" value=\"\" />\r\n      <option name=\"WORKING_DIRECTORY\" value=\"$PROJECT_DIR$\" />\r\n      <option name=\"IS_MODULE_SDK\" value=\"true\" />\r\n      <option name=\"ADD_CONTENT_ROOTS\" value=\"true\" />\r\n      <option name=\"ADD_SOURCE_ROOTS\" value=\"true\" />\r\n      <EXTENSION ID=\"PythonCoverageRunConfigurationExtension\" runner=\"coverage.py\" />\r\n      <option name=\"SCRIPT_NAME\" value=\"$PROJECT_DIR$/AIIrrigationGameV3.py\" />\r\n      <option name=\"PARAMETERS\" value=\"\" />\r\n      <option name=\"SHOW_COMMAND_LINE\" value=\"false\" />\r\n      <option name=\"EMULATE_TERMINAL\" value=\"false\" />\r\n      <option name=\"MODULE_MODE\" value=\"false\" />\r\n      <option name=\"REDIRECT_INPUT\" value=\"false\" />\r\n      <option name=\"INPUT_FILE\" value=\"\" />\r\n      <method v=\"2\" />\r\n    </configuration>\r\n    <configuration name=\"Decagon\" type=\"PythonConfigurationType\" factoryName=\"Python\" temporary=\"true\" nameIsGenerated=\"true\">\r\n      <module name=\"Stomato\" />\r\n      <option name=\"INTERPRETER_OPTIONS\" value=\"\" />\r\n      <option name=\"PARENT_ENVS\" value=\"true\" />\r\n      <envs>\r\n        <env name=\"PYTHONUNBUFFERED\" value=\"1\" />\r\n      </envs>\r\n      <option name=\"SDK_HOME\" value=\"\" />\r\n      <option name=\"WORKING_DIRECTORY\" value=\"$PROJECT_DIR$\" />\r\n      <option name=\"IS_MODULE_SDK\" value=\"true\" />\r\n      <option name=\"ADD_CONTENT_ROOTS\" value=\"true\" />\r\n      <option name=\"ADD_SOURCE_ROOTS\" value=\"true\" />\r\n      <EXTENSION ID=\"PythonCoverageRunConfigurationExtension\" runner=\"coverage.py\" />\r\n      <option name=\"SCRIPT_NAME\" value=\"$PROJECT_DIR$/Decagon.py\" />\r\n      <option name=\"PARAMETERS\" value=\"\" />\r\n      <option name=\"SHOW_COMMAND_LINE\" value=\"false\" />\r\n      <option name=\"EMULATE_TERMINAL\" value=\"false\" />\r\n      <option name=\"MODULE_MODE\" value=\"false\" />\r\n      <option name=\"REDIRECT_INPUT\" value=\"false\" />\r\n      <option name=\"INPUT_FILE\" value=\"\" />\r\n      <method v=\"2\" />\r\n    </configuration>\r\n    <configuration name=\"HeatUnits\" type=\"PythonConfigurationType\" factoryName=\"Python\" temporary=\"true\" nameIsGenerated=\"true\">\r\n      <module name=\"Stomato\" />\r\n      <option name=\"INTERPRETER_OPTIONS\" value=\"\" />\r\n      <option name=\"PARENT_ENVS\" value=\"true\" />\r\n      <envs>\r\n        <env name=\"PYTHONUNBUFFERED\" value=\"1\" />\r\n      </envs>\r\n      <option name=\"SDK_HOME\" value=\"\" />\r\n      <option name=\"WORKING_DIRECTORY\" value=\"$PROJECT_DIR$\" />\r\n      <option name=\"IS_MODULE_SDK\" value=\"true\" />\r\n      <option name=\"ADD_CONTENT_ROOTS\" value=\"true\" />\r\n      <option name=\"ADD_SOURCE_ROOTS\" value=\"true\" />\r\n      <EXTENSION ID=\"PythonCoverageRunConfigurationExtension\" runner=\"coverage.py\" />\r\n      <option name=\"SCRIPT_NAME\" value=\"$PROJECT_DIR$/HeatUnits.py\" />\r\n      <option name=\"PARAMETERS\" value=\"\" />\r\n      <option name=\"SHOW_COMMAND_LINE\" value=\"false\" />\r\n      <option name=\"EMULATE_TERMINAL\" value=\"false\" />\r\n      <option name=\"MODULE_MODE\" value=\"false\" />\r\n      <option name=\"REDIRECT_INPUT\" value=\"false\" />\r\n      <option name=\"INPUT_FILE\" value=\"\" />\r\n      <method v=\"2\" />\r\n    </configuration>\r\n    <configuration default=\"true\" type=\"tests\" factoryName=\"Nosetests\">\r\n      <module name=\"Stomato\" />\r\n      <option name=\"INTERPRETER_OPTIONS\" value=\"\" />\r\n      <option name=\"PARENT_ENVS\" value=\"true\" />\r\n      <option name=\"SDK_HOME\" value=\"\" />\r\n      <option name=\"WORKING_DIRECTORY\" value=\"\" />\r\n      <option name=\"IS_MODULE_SDK\" value=\"false\" />\r\n      <option name=\"ADD_CONTENT_ROOTS\" value=\"true\" />\r\n      <option name=\"ADD_SOURCE_ROOTS\" value=\"true\" />\r\n      <EXTENSION ID=\"PythonCoverageRunConfigurationExtension\" runner=\"coverage.py\" />\r\n      <option name=\"_new_regexPattern\" value=\"&quot;&quot;\" />\r\n      <option name=\"_new_additionalArguments\" value=\"&quot;&quot;\" />\r\n      <option name=\"_new_target\" value=\"&quot;.&quot;\" />\r\n      <option name=\"_new_targetType\" value=\"&quot;PATH&quot;\" />\r\n      <method v=\"2\" />\r\n    </configuration>\r\n    <configuration default=\"true\" type=\"tests\" factoryName=\"Unittests\">\r\n      <module name=\"Stomato\" />\r\n      <option name=\"INTERPRETER_OPTIONS\" value=\"\" />\r\n      <option name=\"PARENT_ENVS\" value=\"true\" />\r\n      <option name=\"SDK_HOME\" value=\"\" />\r\n      <option name=\"WORKING_DIRECTORY\" value=\"\" />\r\n      <option name=\"IS_MODULE_SDK\" value=\"false\" />\r\n      <option name=\"ADD_CONTENT_ROOTS\" value=\"true\" />\r\n      <option name=\"ADD_SOURCE_ROOTS\" value=\"true\" />\r\n      <EXTENSION ID=\"PythonCoverageRunConfigurationExtension\" runner=\"coverage.py\" />\r\n      <option name=\"_new_additionalArguments\" value=\"&quot;&quot;\" />\r\n      <option name=\"_new_target\" value=\"&quot;.&quot;\" />\r\n      <option name=\"_new_targetType\" value=\"&quot;PATH&quot;\" />\r\n      <method v=\"2\" />\r\n    </configuration>\r\n    <configuration default=\"true\" type=\"tests\" factoryName=\"py.test\">\r\n      <module name=\"Stomato\" />\r\n      <option name=\"INTERPRETER_OPTIONS\" value=\"\" />\r\n      <option name=\"PARENT_ENVS\" value=\"true\" />\r\n      <option name=\"SDK_HOME\" value=\"\" />\r\n      <option name=\"WORKING_DIRECTORY\" value=\"\" />\r\n      <option name=\"IS_MODULE_SDK\" value=\"false\" />\r\n      <option name=\"ADD_CONTENT_ROOTS\" value=\"true\" />\r\n      <option name=\"ADD_SOURCE_ROOTS\" value=\"true\" />\r\n      <EXTENSION ID=\"PythonCoverageRunConfigurationExtension\" runner=\"coverage.py\" />\r\n      <option name=\"_new_keywords\" value=\"&quot;&quot;\" />\r\n      <option name=\"_new_parameters\" value=\"&quot;&quot;\" />\r\n      <option name=\"_new_additionalArguments\" value=\"&quot;&quot;\" />\r\n      <option name=\"_new_target\" value=\"&quot;.&quot;\" />\r\n      <option name=\"_new_targetType\" value=\"&quot;PATH&quot;\" />\r\n      <method v=\"2\" />\r\n    </configuration>\r\n    <list>\r\n      <item itemvalue=\"Python.HeatUnits\" />\r\n      <item itemvalue=\"Python.AIIrrigationGameV3 (2)\" />\r\n      <item itemvalue=\"Python.AIIrrigationGameV3 (3)\" />\r\n      <item itemvalue=\"Python.AIIrrigationGameV3 (4)\" />\r\n      <item itemvalue=\"Python.Decagon\" />\r\n    </list>\r\n    <recent_temporary>\r\n      <list>\r\n        <item itemvalue=\"Python.Decagon\" />\r\n        <item itemvalue=\"Python.HeatUnits\" />\r\n        <item itemvalue=\"Python.AIIrrigationGameV3 (4)\" />\r\n        <item itemvalue=\"Python.AIIrrigationGameV3 (3)\" />\r\n        <item itemvalue=\"Python.AIIrrigationGameV3 (2)\" />\r\n      </list>\r\n    </recent_temporary>\r\n  </component>\r\n  <component name=\"SpellCheckerSettings\" RuntimeDictionaries=\"0\" Folders=\"0\" CustomDictionaries=\"0\" DefaultDictionary=\"project-level\" UseSingleDictionary=\"true\" transferred=\"true\" />\r\n  <component name=\"SvnConfiguration\">\r\n    <configuration />\r\n  </component>\r\n  <component name=\"TaskManager\">\r\n    <task active=\"true\" id=\"Default\" summary=\"Default task\">\r\n      <changelist id=\"3aa7c90b-1f87-403f-bac4-0b49ed107be9\" name=\"Default\" comment=\"\" />\r\n      <created>1487028217187</created>\r\n      <option name=\"number\" value=\"Default\" />\r\n      <option name=\"presentableId\" value=\"Default\" />\r\n      <updated>1487028217187</updated>\r\n      <workItem from=\"1605129884620\" duration=\"145000\" />\r\n      <workItem from=\"1605130060530\" duration=\"35278000\" />\r\n      <workItem from=\"1607291245080\" duration=\"1190000\" />\r\n      <workItem from=\"1607383244511\" duration=\"7553000\" />\r\n      <workItem from=\"1607548979078\" duration=\"3754000\" />\r\n      <workItem from=\"1607553651418\" duration=\"17118000\" />\r\n      <workItem from=\"1607689891059\" duration=\"5504000\" />\r\n      <workItem from=\"1608060424541\" duration=\"58636000\" />\r\n      <workItem from=\"1610149124892\" duration=\"6211000\" />\r\n      <workItem from=\"1610537327787\" duration=\"40054000\" />\r\n      <workItem from=\"1611488172733\" duration=\"31888000\" />\r\n      <workItem from=\"1612440711302\" duration=\"17240000\" />\r\n      <workItem from=\"1612561663001\" duration=\"4655000\" />\r\n      <workItem from=\"1612830453536\" duration=\"11311000\" />\r\n      <workItem from=\"1613080557188\" duration=\"4065000\" />\r\n      <workItem from=\"1613618833495\" duration=\"2149000\" />\r\n      <workItem from=\"1613666950983\" duration=\"7717000\" />\r\n      <workItem from=\"1614009600471\" duration=\"2019000\" />\r\n      <workItem from=\"1614291091396\" duration=\"1511000\" />\r\n      <workItem from=\"1614364207160\" duration=\"2001000\" />\r\n      <workItem from=\"1614617953355\" duration=\"2152000\" />\r\n      <workItem from=\"1614703156595\" duration=\"3174000\" />\r\n      <workItem from=\"1614957784121\" duration=\"2410000\" />\r\n      <workItem from=\"1615206260553\" duration=\"50000\" />\r\n      <workItem from=\"1615218696486\" duration=\"1074000\" />\r\n      <workItem from=\"1615220753729\" duration=\"1867000\" />\r\n      <workItem from=\"1615310497153\" duration=\"3925000\" />\r\n      <workItem from=\"1615388232435\" duration=\"182000\" />\r\n      <workItem from=\"1615389899479\" duration=\"785000\" />\r\n      <workItem from=\"1615399895663\" duration=\"2323000\" />\r\n      <workItem from=\"1615419143099\" duration=\"1002000\" />\r\n      <workItem from=\"1617013211354\" duration=\"48204000\" />\r\n      <workItem from=\"1617748132407\" duration=\"26135000\" />\r\n      <workItem from=\"1617905241291\" duration=\"110000\" />\r\n      <workItem from=\"1617905461958\" duration=\"36062000\" />\r\n      <workItem from=\"1618392723430\" duration=\"183894000\" />\r\n      <workItem from=\"1620643821048\" duration=\"7864000\" />\r\n      <workItem from=\"1620819109173\" duration=\"59413000\" />\r\n      <workItem from=\"1621674236561\" duration=\"35814000\" />\r\n      <workItem from=\"1622641636696\" duration=\"31255000\" />\r\n      <workItem from=\"1623104726201\" duration=\"8910000\" />\r\n      <workItem from=\"1623305121076\" duration=\"49475000\" />\r\n      <workItem from=\"1624525983019\" duration=\"11946000\" />\r\n      <workItem from=\"1624926747360\" duration=\"8702000\" />\r\n      <workItem from=\"1625267760339\" duration=\"9175000\" />\r\n      <workItem from=\"1625667908177\" duration=\"3489000\" />\r\n      <workItem from=\"1626141654181\" duration=\"760000\" />\r\n      <workItem from=\"1626304184251\" duration=\"17573000\" />\r\n      <workItem from=\"1627179918020\" duration=\"6000000\" />\r\n      <workItem from=\"1627522193589\" duration=\"44463000\" />\r\n      <workItem from=\"1628685253434\" duration=\"7562000\" />\r\n      <workItem from=\"1628895996797\" duration=\"5405000\" />\r\n      <workItem from=\"1629519094001\" duration=\"64107000\" />\r\n      <workItem from=\"1630820604906\" duration=\"81000\" />\r\n      <workItem from=\"1630972145933\" duration=\"13068000\" />\r\n      <workItem from=\"1631043208174\" duration=\"40657000\" />\r\n      <workItem from=\"1631675344538\" duration=\"39881000\" />\r\n      <workItem from=\"1632483008669\" duration=\"62634000\" />\r\n      <workItem from=\"1633561848284\" duration=\"20514000\" />\r\n      <workItem from=\"1634071114758\" duration=\"29042000\" />\r\n      <workItem from=\"1634205505386\" duration=\"760000\" />\r\n      <workItem from=\"1634508447986\" duration=\"46186000\" />\r\n      <workItem from=\"1635563392486\" duration=\"7946000\" />\r\n      <workItem from=\"1635813530050\" duration=\"11275000\" />\r\n      <workItem from=\"1636402837909\" duration=\"7978000\" />\r\n      <workItem from=\"1636686576193\" duration=\"72544000\" />\r\n      <workItem from=\"1639623090539\" duration=\"597000\" />\r\n      <workItem from=\"1641862430443\" duration=\"10031000\" />\r\n      <workItem from=\"1642109259317\" duration=\"14670000\" />\r\n      <workItem from=\"1642468081345\" duration=\"123000\" />\r\n      <workItem from=\"1642468312662\" duration=\"4400000\" />\r\n      <workItem from=\"1642479890801\" duration=\"3012000\" />\r\n      <workItem from=\"1642607098853\" duration=\"3509000\" />\r\n      <workItem from=\"1643418289163\" duration=\"2451000\" />\r\n      <workItem from=\"1643431775486\" duration=\"2669000\" />\r\n      <workItem from=\"1643530688030\" duration=\"16607000\" />\r\n      <workItem from=\"1644884974706\" duration=\"6976000\" />\r\n      <workItem from=\"1645051866051\" duration=\"4965000\" />\r\n      <workItem from=\"1646170191631\" duration=\"1143000\" />\r\n      <workItem from=\"1647030210662\" duration=\"20674000\" />\r\n      <workItem from=\"1647899848340\" duration=\"8878000\" />\r\n      <workItem from=\"1648605161943\" duration=\"602000\" />\r\n      <workItem from=\"1648677498393\" duration=\"17335000\" />\r\n      <workItem from=\"1649028728369\" duration=\"62000\" />\r\n      <workItem from=\"1649028799373\" duration=\"41714000\" />\r\n      <workItem from=\"1649296707874\" duration=\"1532000\" />\r\n      <workItem from=\"1649715253762\" duration=\"16105000\" />\r\n      <workItem from=\"1650672932583\" duration=\"3495000\" />\r\n      <workItem from=\"1650676711602\" duration=\"124000\" />\r\n      <workItem from=\"1650676849444\" duration=\"23979000\" />\r\n      <workItem from=\"1652287991235\" duration=\"4330000\" />\r\n      <workItem from=\"1652475327231\" duration=\"59000\" />\r\n      <workItem from=\"1652475415424\" duration=\"26422000\" />\r\n      <workItem from=\"1654114664338\" duration=\"37311000\" />\r\n      <workItem from=\"1655238921783\" duration=\"2997000\" />\r\n      <workItem from=\"1655836574428\" duration=\"18511000\" />\r\n      <workItem from=\"1656846376891\" duration=\"4630000\" />\r\n      <workItem from=\"1657690237491\" duration=\"11380000\" />\r\n      <workItem from=\"1660709759094\" duration=\"1304000\" />\r\n      <workItem from=\"1663715527866\" duration=\"58524000\" />\r\n      <workItem from=\"1673917023959\" duration=\"24211000\" />\r\n      <workItem from=\"1677103677259\" duration=\"7166000\" />\r\n      <workItem from=\"1678912269971\" duration=\"22450000\" />\r\n      <workItem from=\"1679515676906\" duration=\"53089000\" />\r\n      <workItem from=\"1680132841035\" duration=\"65000\" />\r\n      <workItem from=\"1680132918916\" duration=\"137000\" />\r\n      <workItem from=\"1680133083506\" duration=\"16881000\" />\r\n      <workItem from=\"1680238363154\" duration=\"166000\" />\r\n      <workItem from=\"1680238880050\" duration=\"5824000\" />\r\n      <workItem from=\"1680553460768\" duration=\"645000\" />\r\n      <workItem from=\"1680556381530\" duration=\"857000\" />\r\n      <workItem from=\"1680557253511\" duration=\"980000\" />\r\n      <workItem from=\"1680558257466\" duration=\"32869000\" />\r\n      <workItem from=\"1680818886482\" duration=\"161344000\" />\r\n      <workItem from=\"1683923672560\" duration=\"108547000\" />\r\n      <workItem from=\"1684964327889\" duration=\"11711000\" />\r\n      <workItem from=\"1685127366248\" duration=\"81133000\" />\r\n      <workItem from=\"1685653445940\" duration=\"10983000\" />\r\n      <workItem from=\"1685740244204\" duration=\"56272000\" />\r\n      <workItem from=\"1686272407325\" duration=\"16994000\" />\r\n      <workItem from=\"1686693323310\" duration=\"23345000\" />\r\n      <workItem from=\"1686855364679\" duration=\"18047000\" />\r\n      <workItem from=\"1686875965074\" duration=\"6029000\" />\r\n    </task>\r\n    <task id=\"LOCAL-00297\" summary=\"Fixed Emails&#10;&#10;Emailing wasn't working past the first recipient. Fixed it now\">\r\n      <created>1681780581866</created>\r\n      <option name=\"number\" value=\"00297\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00297\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1681780581866</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00298\" summary=\"BigQuery Credentials Changes&#10;&#10;Changed how we get credentials for bigquery clients by setting a credentials parameter from the path of the json credentials\">\r\n      <created>1681946889382</created>\r\n      <option name=\"number\" value=\"00298\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00298\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1681946889382</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00299\" summary=\"IrrigationRecommendationExpert.py Crop Stage Changes&#10;&#10;Added an else clause in case the crop_stage cannot be determined by fitting the date between each stage range\">\r\n      <created>1681946997337</created>\r\n      <option name=\"number\" value=\"00299\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00299\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1681946997337</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00300\" summary=\"AI Function Changes&#10;&#10;Changed setup_ai_game_data so it grabs lat, long, and planting date and adds those to the tuple so we don't need to access the pickle&#10;Added funtion turn_ai_game_data_into_csv to grab ai_game_data and create a csv for it to be used by the Neural Net imports\">\r\n      <created>1681947144677</created>\r\n      <option name=\"number\" value=\"00300\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00300\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1681947144677</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00301\" summary=\"AIExpertSystemTesting.py Changes&#10;&#10;Reworked a bunch of the AI game to have a more modern look and be cleaner. Also made some changes required for the .exe packager\">\r\n      <created>1681947238632</created>\r\n      <option name=\"number\" value=\"00301\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00301\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1681947238632</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00302\" summary=\"Testing lines removed\">\r\n      <created>1681947311832</created>\r\n      <option name=\"number\" value=\"00302\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00302\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1681947311832</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00303\" summary=\"Soils&#10;&#10;Added Soils.py which contains 2 classes, AllSoils and Soil.&#10;Soil will now be the class that Logger instantiates to have a soil associated with it. &#10;Soil has soil_type, field_capacity, and wilting_point among other useful variables and functions.&#10;AllSoils is just a static class to hold all of the different soil types we use to be able to play with them and generate graphs and other things.&#10;Modified all use cases of logger.field_capacity and logger.wilting_point to now utilize logger.soil.field_capacity and logger.soil.wilting_point.\">\r\n      <created>1683067787052</created>\r\n      <option name=\"number\" value=\"00303\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00303\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1683067787052</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00304\" summary=\"Bugfix&#10;&#10;Fixed bug with calculate_portal_soil_moisture_desc() that was causing Silty Clay Loam to return low for the  wrong range of values\">\r\n      <created>1683785761195</created>\r\n      <option name=\"number\" value=\"00304\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00304\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1683785761195</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00305\" summary=\"Pickle Function Changes&#10;&#10;Removed redundant &quot;specific&quot; pickle functions like write_specific_pickle and instead modified the regular pickle functions to use optional parameters.&#10;Modified all uses of &quot;specific&quot; pickle functions to use regular functions with optional parameters\">\r\n      <created>1683786623261</created>\r\n      <option name=\"number\" value=\"00305\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00305\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1683786623261</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00306\" summary=\"Reworked grabbing data from dxd's&#10;&#10;We now loop through all applicable timeseries in a dxd to grab the data we care about instead of just assuming the data we want is in the last timeseries.&#10;Moved getting ports information out of read_dxd(). We now call it separately and don't assign the ports to as a Logger variable but instead a temporary variable.&#10;Added specific checks to select a timeseries that we care about. We now check if it has data from the year we are interested, if it has ports 1-6 configured, if port 1 is IR, port 2 VP4, and so on.&#10;get_ports() now takes a timeseries instead of a dxd. Don't need to read the dxd twice. We now read once and pass along to whatever function needs it&#10;Put all the timestamp converting functions into one function called convert_timestamp_to_local_datetime() which now handles calling the appropriate functions in order.&#10;Added a new function to Logger called set_broken() to be used for when a logger is replaced. This will set the appropriate parameters so the Logger stays in the Field but is inactive. This way we know the logger was installed in that field at some point.&#10;Added self.uninstall_date = None to Loggers __init__\">\r\n      <created>1683787119552</created>\r\n      <option name=\"number\" value=\"00306\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00306\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1683787119552</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00307\" summary=\"YearAnalysis.py changes.&#10;&#10;Added optional parameters to analyze_year() so you can specify what analysis you want to run as well as if you want to graph_data, save_new_pickle, or grab_data_from_pickle instead of dxds.&#10;Passed on appropriate optional parameters to analyze_logger(). &#10;Added a couple of checks for 2 logger ids that we know are garbage from 2022.&#10;Other changes\">\r\n      <created>1683787329352</created>\r\n      <option name=\"number\" value=\"00307\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00307\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1683787329352</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00308\" summary=\"AI Expert Game Changes&#10;&#10;Added scroll bar to the main game by creating a canvas and putthing the frames inside.&#10;Attempting to add keybinding for Enter key to buttons in each Phase so the game can be played without mouse. Work in progress\">\r\n      <created>1683787413796</created>\r\n      <option name=\"number\" value=\"00308\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00308\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1683787413796</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00309\" summary=\"Portal Soil Description Calculation Changes&#10;&#10;Modified CwsiProcessor.py function calculate_portal_soil_moisture() to use an instance of the Soil class to get the vwc range description. This moves the logic to the Soil class where it should be.&#10;Removed redundant functions soil_type_lookup() and find_closest_soil_type()\">\r\n      <created>1683934778406</created>\r\n      <option name=\"number\" value=\"00309\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00309\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1683934778407</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00310\" summary=\"Decagon test code&#10;&#10;Modified some test code for going through a pickle and assigning a Soil to each logger\">\r\n      <created>1683934842420</created>\r\n      <option name=\"number\" value=\"00310\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00310\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1683934842420</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00311\" summary=\"Formatting Cleanup\">\r\n      <created>1683934866605</created>\r\n      <option name=\"number\" value=\"00311\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00311\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1683934866605</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00312\" summary=\"Soils.py Changes&#10;&#10;Cleaned up the graph function of AllSoils to simplify color allocation and account for new ranges we defined.&#10;Modified constants in Soil.py to now have VERY_LOW, BELOW_OPT and VERY_HIGH.&#10;Modified type hinting for field_capacity and wilting_point throughout functions in Soil.py to account for cases where field capacity and wilting point aren't clean ints (like in the case of a soil type that falls in between 2 types).&#10;Modified Soil.py's vwc ranges and their functions to incorporate new below_optimum and modified very_low, high, and very_high.&#10;Made some functions static.&#10;Modified find_vwc_range_description() to incorporate the new ranges.\">\r\n      <created>1683935136026</created>\r\n      <option name=\"number\" value=\"00312\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00312\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1683935136026</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00313\" summary=\"Logos&#10;&#10;Added the Gradient and Morning Star Logos for use in AI Irrgation Game\">\r\n      <created>1684372405467</created>\r\n      <option name=\"number\" value=\"00313\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00313\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1684372405467</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00314\" summary=\"Print Statement Rearrangement&#10;&#10;Moved some print statements from CwsiProcessor.py to the Logger update function. That way we can call the CwsiProcessor.py functions without needing to print certain information.&#10;Added an optional boolean parameter to get_highest_and_lowest_temperature_indexes() called mute_prints that allows the use of the function without printing anything by passing in True.\">\r\n      <created>1684372577908</created>\r\n      <option name=\"number\" value=\"00314\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00314\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1684372577908</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00315\" summary=\"IR Active Logic&#10;&#10;Moved the ir_active check to after values have been assigned in final_results(). That way we still get the values and can then just decide if we append them to the return dictionary or not. This allows us to use those values to do some checks.&#10;Added a call to logger.update_ir_consecutive_data() that handles the 3 consecutive days of PSI data required to turn on the PSI values for a logger.\">\r\n      <created>1684372746988</created>\r\n      <option name=\"number\" value=\"00315\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00315\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1684372746988</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00316\" summary=\"New function to compare IR data&#10;&#10;Added compare_new_psi_algo_vs_old() that goes through a pickle with 2022 data and a pickle with re-processed 2022 data (using new PSI algorithm) and compares how close the new algorithm guessed the activation date for PSI.\">\r\n      <created>1684372836684</created>\r\n      <option name=\"number\" value=\"00316\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00316\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1684372836684</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00317\" summary=\"New IR Active Logic &#10;&#10;New import for deque from collections.&#10;Added self.consecutive_ir_values = deque() to a Logger's _init_ function.&#10;Went back and added the same deque to older Loggers already instantiated.&#10;Moved the check if IR should be active out of Logger's update and into CWSI final_results().&#10;Moved some print statements from CwsiProcessor.py to Logger update function.&#10;Refactored should_ir_be_active() to take in an optional parameter for the date you are checking. This way we can pass in the actual date we care about instead of assuming we should always use today's date.&#10;Added new logic to should_ir_be_active() which checks the 3 consecutive PSI values for the logger and only activates if they pass some checks.&#10;Added function update_ir_consecutive_data() that is in charge of keeping 3 consecutive days worth of PSI data in Logger for use in determining IR activation.\">\r\n      <created>1684373149201</created>\r\n      <option name=\"number\" value=\"00317\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00317\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1684373149201</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00318\" summary=\"YearAnalysis.py Updates&#10;&#10;Updated YearAnalysis.py to be able to handle psi_analysis partially. &#10;More work on psi_analysis is still pending.\">\r\n      <created>1684373219394</created>\r\n      <option name=\"number\" value=\"00318\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00318\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1684373219394</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00319\" summary=\"Add Gradient Logos&#10;&#10;Added a couple of Gradient Logos\">\r\n      <created>1685134852031</created>\r\n      <option name=\"number\" value=\"00319\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00319\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1685134852031</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00320\" summary=\"Field Type&#10;&#10;Added field_type to Field object to track if the field is a demo. &#10;field_type not holds a string that can be 'Commercial', 'Demo', or 'R&amp;D' depending on what the field is.&#10;Modified setup_field() in Decagon to incorporate new optional parameter for field_type.&#10;Modified Field objet init to have the new optional parameter field_type&#10;Added consec_psis to Logger to_string().&#10;Changed logger usage of rnd boolean to check Field.field_type instead.\">\r\n      <created>1685137571916</created>\r\n      <option name=\"number\" value=\"00320\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00320\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1685137571916</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00321\" summary=\"Notification updates&#10;&#10;Updated notifications for vwc to use the new soil class and the thresholds defined in there for the specific soil type.&#10;Updated canopy temp notifications.&#10;Added new PSI notifications for high thresholds and for when an IR should be turned on but hasn't been activated yet by our system.\">\r\n      <created>1685145496314</created>\r\n      <option name=\"number\" value=\"00321\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00321\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1685145496314</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00322\" summary=\"Formatting\">\r\n      <created>1685145590215</created>\r\n      <option name=\"number\" value=\"00322\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00322\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1685145590215</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00323\" summary=\"Changes to Decagon setup_ai_game_data()&#10;&#10;Simplified setting up new data for AI game by just passing along the logger instead of individual data points\">\r\n      <created>1685497624511</created>\r\n      <option name=\"number\" value=\"00323\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00323\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1685497624511</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00324\" summary=\"Added Notifications for All data from API, not just final results&#10;&#10;Added new method check_for_notifications_all_data() that takes in the converted data from the API call and uses that to check for notifications.&#10;Renamed original check_for_notifications() to check_for_notifications_final_results() to indicate we are using final results for our checks.&#10;Added several new notification possibilities including a sensor reporting None at all and a z6 not reporting more than 2 data points for a day.\">\r\n      <created>1685577941301</created>\r\n      <option name=\"number\" value=\"00324\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00324\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1685577941301</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00325\" summary=\"GDD Error range&#10;&#10;Modified get_crop_stage in CwsiProcessor.py to have an extra level (10) for 90-100% Red.&#10;Modified get_crop_stage_level to handle accumulated_gdds higher than 1214 since we were hitting some of those numbers already causing errors.\">\r\n      <created>1686171370132</created>\r\n      <option name=\"number\" value=\"00325\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00325\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1686171370132</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00326\" summary=\"Stopping Warning Notifications&#10;&#10;Added optional boolean parameters (warnings, errors) defaulting to True to check_for_notifications. Notifications will only be generated if the boolean parameter is true. &#10;Passed in warnings=False to check_for_notifications to stop Warning Notifications for the time being&#10;Commented out writing and emailing Warning Notifications for the time being\">\r\n      <created>1686189591157</created>\r\n      <option name=\"number\" value=\"00326\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00326\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1686189591157</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00327\" summary=\"VWC Portal Range Bug&#10;&#10;We were using 40 days to determine if we use vwc1 and vwc2 or vwc2 and vwc3 for the portal vwc description when we should be using 30 days.\">\r\n      <created>1686702191044</created>\r\n      <option name=\"number\" value=\"00327\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00327\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1686702191046</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00328\" summary=\"Splitting Check for notifications and Email notifications&#10;&#10;Added optional parameter email_notifications to Decagon update_information() method to decide when we email notifications&#10;Additional formatting\">\r\n      <created>1686702340932</created>\r\n      <option name=\"number\" value=\"00328\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00328\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1686702340932</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00329\" summary=\"Moved updated checks for Logger, Field and Grower&#10;&#10;Each class is now in charge of handling its self.updated conditionals for updating instead of it being done by its parent class\">\r\n      <created>1686702466945</created>\r\n      <option name=\"number\" value=\"00329\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00329\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1686702466945</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00330\" summary=\"Modified Update Retry to handle Notification emailing via the optional parameter email_notifications=True\">\r\n      <created>1686702528358</created>\r\n      <option name=\"number\" value=\"00330\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00330\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1686702528358</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00331\" summary=\"Logos location\">\r\n      <created>1686702564782</created>\r\n      <option name=\"number\" value=\"00331\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00331\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1686702564782</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00332\" summary=\"IrrigationRecommendationExpert.py changes&#10;&#10;Modified get_crop_stage() so it uses the new season break points of 30 days after planting, 30 before harvest, and 14 before harvest.\">\r\n      <created>1686702695344</created>\r\n      <option name=\"number\" value=\"00332\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00332\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1686702695344</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00333\" summary=\"AI Game Init files&#10;&#10;Data and Error pickles to initialize a new AI Game. Both have no game data points in them, just the logger options to play with.\">\r\n      <created>1686702834367</created>\r\n      <option name=\"number\" value=\"00333\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00333\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1686702834367</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00334\" summary=\"AI Game v2&#10;&#10;The start of the new TKinter AI Game. Stopped mid way to move to Kivy instead.\">\r\n      <created>1686702947573</created>\r\n      <option name=\"number\" value=\"00334\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00334\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1686702947573</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00335\" summary=\"AI Game v1 updates&#10;&#10;Attempts at adding scroll to the AI Game v1 (not very successfully).&#10;Started adding error handling.\">\r\n      <created>1686703027902</created>\r\n      <option name=\"number\" value=\"00335\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00335\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1686703027902</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00336\" summary=\"Changes to AIGameData.py&#10;&#10;Added function get_random_logger_from_pool() which takes a parameter indicating if you want the logger from the South, North, or All pool.&#10;Removed individual functions to get a logger from South, North and All pools since its not in 1 function.\">\r\n      <created>1686703293416</created>\r\n      <option name=\"number\" value=\"00336\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00336\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1686703293416</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00337\" summary=\"KivyTesting.py&#10;&#10;Testing file to load up and understand different Kivy examples.\">\r\n      <created>1686703416193</created>\r\n      <option name=\"number\" value=\"00337\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00337\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1686703416193</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00338\" summary=\"AIIrrigationGameV3.py&#10;&#10;Brand new version 3 of the AI Game done in Kivy.&#10;The .py file specifies most of the functionality while the .kv file specifies most of the UI.&#10;Spec file was used to package the app into an executable for windows.\">\r\n      <created>1686703492418</created>\r\n      <option name=\"number\" value=\"00338\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00338\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1686703492418</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00339\" summary=\"YearAnalysis.py&#10;&#10;A lot more work on psi analysis.\">\r\n      <created>1686703533946</created>\r\n      <option name=\"number\" value=\"00339\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00339\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1686703533946</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00340\" summary=\"BS\">\r\n      <created>1686703545173</created>\r\n      <option name=\"number\" value=\"00340\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00340\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1686703545173</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00341\" summary=\"get_logger_data optional parameter&#10;&#10;Added file_name optional parameter to get_logger_data in case you want to save the dxd file with a specific name. Useful for analysis later on\">\r\n      <created>1686710232174</created>\r\n      <option name=\"number\" value=\"00341\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00341\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1686710232174</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00342\" summary=\"Added get_logger_id_and_password_dict()&#10;&#10;This method has everything it needs to go look at the Gsheet where we store the ID's and PW's and return a dict with all of them.\">\r\n      <created>1686710282261</created>\r\n      <option name=\"number\" value=\"00342\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00342\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1686710282261</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00343\" summary=\"BS\">\r\n      <created>1686710296415</created>\r\n      <option name=\"number\" value=\"00343\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00343\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1686710296415</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00344\" summary=\"Started HeatUnits.py&#10;&#10;This file will be used to do the processing work for the Heat Units trials this year 2023.&#10;Started by setting up Logger instances for each of the current Weather Stations we have installed.&#10;Setup download of data for each into its on directory.\">\r\n      <created>1686710371419</created>\r\n      <option name=\"number\" value=\"00344\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00344\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1686710371419</updated>\r\n    </task>\r\n    <task id=\"LOCAL-00345\" summary=\"VP4 Notification Changes&#10;&#10;Simplified vp4 notifications into 1 single method to handle air temp, rh, and vpd notifications. This allows us to just create a single notification if any of those are None instead of one for each type since it is almost never the case that just 1 fails.&#10;New method is called vp4_notifications.&#10;Removed old individual methods for air temp notifications, rh notifications, and vpd notifications.&#10;Formatting changes.\">\r\n      <created>1686858169672</created>\r\n      <option name=\"number\" value=\"00345\" />\r\n      <option name=\"presentableId\" value=\"LOCAL-00345\" />\r\n      <option name=\"project\" value=\"LOCAL\" />\r\n      <updated>1686858169672</updated>\r\n    </task>\r\n    <option name=\"localTasksCounter\" value=\"346\" />\r\n    <servers />\r\n  </component>\r\n  <component name=\"TodoView\">\r\n    <todo-panel id=\"selected-file\">\r\n      <is-autoscroll-to-source value=\"true\" />\r\n    </todo-panel>\r\n    <todo-panel id=\"all\">\r\n      <are-packages-shown value=\"true\" />\r\n      <is-autoscroll-to-source value=\"true\" />\r\n    </todo-panel>\r\n  </component>\r\n  <component name=\"ToolWindowManager\">\r\n    <frame x=\"-8\" y=\"-8\" width=\"1936\" height=\"1100\" extended-state=\"0\" />\r\n    <layout>\r\n      <window_info id=\"Project\" active=\"false\" anchor=\"left\" auto_hide=\"false\" internal_type=\"DOCKED\" type=\"DOCKED\" visible=\"false\" show_stripe_button=\"true\" weight=\"0.25\" sideWeight=\"0.5\" order=\"0\" side_tool=\"false\" content_ui=\"combo\" />\r\n      <window_info id=\"TODO\" active=\"false\" anchor=\"bottom\" auto_hide=\"false\" internal_type=\"DOCKED\" type=\"DOCKED\" visible=\"false\" show_stripe_button=\"true\" weight=\"0.33\" sideWeight=\"0.5\" order=\"6\" side_tool=\"false\" content_ui=\"tabs\" />\r\n      <window_info id=\"Statistic\" active=\"false\" anchor=\"bottom\" auto_hide=\"false\" internal_type=\"DOCKED\" type=\"DOCKED\" visible=\"false\" show_stripe_button=\"true\" weight=\"0.33\" sideWeight=\"0.5\" order=\"-1\" side_tool=\"false\" content_ui=\"tabs\" />\r\n      <window_info id=\"Event Log\" active=\"false\" anchor=\"bottom\" auto_hide=\"false\" internal_type=\"DOCKED\" type=\"DOCKED\" visible=\"false\" show_stripe_button=\"true\" weight=\"0.33\" sideWeight=\"0.5\" order=\"-1\" side_tool=\"true\" content_ui=\"tabs\" />\r\n      <window_info id=\"Version Control\" active=\"false\" anchor=\"bottom\" auto_hide=\"false\" internal_type=\"DOCKED\" type=\"DOCKED\" visible=\"false\" show_stripe_button=\"true\" weight=\"0.33\" sideWeight=\"0.5\" order=\"-1\" side_tool=\"false\" content_ui=\"tabs\" />\r\n      <window_info id=\"Python Console\" active=\"false\" anchor=\"bottom\" auto_hide=\"false\" internal_type=\"DOCKED\" type=\"DOCKED\" visible=\"false\" show_stripe_button=\"true\" weight=\"0.33\" sideWeight=\"0.5\" order=\"-1\" side_tool=\"false\" content_ui=\"tabs\" />\r\n      <window_info id=\"Run\" active=\"false\" anchor=\"bottom\" auto_hide=\"false\" internal_type=\"DOCKED\" type=\"DOCKED\" visible=\"false\" show_stripe_button=\"true\" weight=\"0.33\" sideWeight=\"0.5\" order=\"2\" side_tool=\"false\" content_ui=\"tabs\" />\r\n      <window_info id=\"Structure\" active=\"false\" anchor=\"left\" auto_hide=\"false\" internal_type=\"DOCKED\" type=\"DOCKED\" visible=\"false\" show_stripe_button=\"true\" weight=\"0.25\" sideWeight=\"0.5\" order=\"1\" side_tool=\"false\" content_ui=\"tabs\" />\r\n      <window_info id=\"Terminal\" active=\"false\" anchor=\"bottom\" auto_hide=\"false\" internal_type=\"DOCKED\" type=\"DOCKED\" visible=\"false\" show_stripe_button=\"true\" weight=\"0.33\" sideWeight=\"0.5\" order=\"-1\" side_tool=\"false\" content_ui=\"tabs\" />\r\n      <window_info id=\"Favorites\" active=\"false\" anchor=\"left\" auto_hide=\"false\" internal_type=\"DOCKED\" type=\"DOCKED\" visible=\"false\" show_stripe_button=\"true\" weight=\"0.33\" sideWeight=\"0.5\" order=\"-1\" side_tool=\"true\" content_ui=\"tabs\" />\r\n      <window_info id=\"Debug\" active=\"false\" anchor=\"bottom\" auto_hide=\"false\" internal_type=\"DOCKED\" type=\"DOCKED\" visible=\"false\" show_stripe_button=\"true\" weight=\"0.4\" sideWeight=\"0.5\" order=\"3\" side_tool=\"false\" content_ui=\"tabs\" />\r\n      <window_info id=\"Cvs\" active=\"false\" anchor=\"bottom\" auto_hide=\"false\" internal_type=\"DOCKED\" type=\"DOCKED\" visible=\"false\" show_stripe_button=\"true\" weight=\"0.25\" sideWeight=\"0.5\" order=\"4\" side_tool=\"false\" content_ui=\"tabs\" />\r\n      <window_info id=\"Hierarchy\" active=\"false\" anchor=\"right\" auto_hide=\"false\" internal_type=\"DOCKED\" type=\"DOCKED\" visible=\"false\" show_stripe_button=\"true\" weight=\"0.25\" sideWeight=\"0.5\" order=\"2\" side_tool=\"false\" content_ui=\"combo\" />\r\n      <window_info id=\"Message\" active=\"false\" anchor=\"bottom\" auto_hide=\"false\" internal_type=\"DOCKED\" type=\"DOCKED\" visible=\"false\" show_stripe_button=\"true\" weight=\"0.33\" sideWeight=\"0.5\" order=\"0\" side_tool=\"false\" content_ui=\"tabs\" />\r\n      <window_info id=\"Commander\" active=\"false\" anchor=\"right\" auto_hide=\"false\" internal_type=\"DOCKED\" type=\"DOCKED\" visible=\"false\" show_stripe_button=\"true\" weight=\"0.4\" sideWeight=\"0.5\" order=\"0\" side_tool=\"false\" content_ui=\"tabs\" />\r\n      <window_info id=\"Find\" active=\"false\" anchor=\"bottom\" auto_hide=\"false\" internal_type=\"DOCKED\" type=\"DOCKED\" visible=\"false\" show_stripe_button=\"true\" weight=\"0.33\" sideWeight=\"0.5\" order=\"1\" side_tool=\"false\" content_ui=\"tabs\" />\r\n      <window_info id=\"Inspection\" active=\"false\" anchor=\"bottom\" auto_hide=\"false\" internal_type=\"DOCKED\" type=\"DOCKED\" visible=\"false\" show_stripe_button=\"true\" weight=\"0.4\" sideWeight=\"0.5\" order=\"5\" side_tool=\"false\" content_ui=\"tabs\" />\r\n      <window_info id=\"Ant Build\" active=\"false\" anchor=\"right\" auto_hide=\"false\" internal_type=\"DOCKED\" type=\"DOCKED\" visible=\"false\" show_stripe_button=\"true\" weight=\"0.25\" sideWeight=\"0.5\" order=\"1\" side_tool=\"false\" content_ui=\"tabs\" />\r\n    </layout>\r\n  </component>\r\n  <component name=\"TypeScriptGeneratedFilesManager\">\r\n    <option name=\"version\" value=\"3\" />\r\n  </component>\r\n  <component name=\"Vcs.Log.Tabs.Properties\">\r\n    <option name=\"RECENT_FILTERS\">\r\n      <map>\r\n        <entry key=\"Branch\">\r\n          <value>\r\n            <list>\r\n              <RecentGroup>\r\n                <option name=\"FILTER_VALUES\">\r\n                  <option value=\"master\" />\r\n                </option>\r\n              </RecentGroup>\r\n              <RecentGroup>\r\n                <option name=\"FILTER_VALUES\">\r\n                  <option value=\"loggerSetups\" />\r\n                </option>\r\n              </RecentGroup>\r\n            </list>\r\n          </value>\r\n        </entry>\r\n      </map>\r\n    </option>\r\n    <option name=\"TAB_STATES\">\r\n      <map>\r\n        <entry key=\"MAIN\">\r\n          <value>\r\n            <State>\r\n              <option name=\"FILTERS\">\r\n                <map>\r\n                  <entry key=\"branch\">\r\n                    <value>\r\n                      <list>\r\n                        <option value=\"origin/master\" />\r\n                      </list>\r\n                    </value>\r\n                  </entry>\r\n                </map>\r\n              </option>\r\n            </State>\r\n          </value>\r\n        </entry>\r\n      </map>\r\n    </option>\r\n  </component>\r\n  <component name=\"VcsManagerConfiguration\">\r\n    <option name=\"ADD_EXTERNAL_FILES_SILENTLY\" value=\"true\" />\r\n    <MESSAGE value=\"Field Type&#10;&#10;Added field_type to Field object to track if the field is a demo. &#10;field_type not holds a string that can be 'Commercial', 'Demo', or 'R&amp;D' depending on what the field is.&#10;Modified setup_field() in Decagon to incorporate new optional parameter for field_type.&#10;Modified Field objet init to have the new optional parameter field_type&#10;Added consec_psis to Logger to_string().&#10;Changed logger usage of rnd boolean to check Field.field_type instead.\" />\r\n    <MESSAGE value=\"Notification updates&#10;&#10;Updated notifications for vwc to use the new soil class and the thresholds defined in there for the specific soil type.&#10;Updated canopy temp notifications.&#10;Added new PSI notifications for high thresholds and for when an IR should be turned on but hasn't been activated yet by our system.\" />\r\n    <MESSAGE value=\"Formatting\" />\r\n    <MESSAGE value=\"Changes to Decagon setup_ai_game_data()&#10;&#10;Simplified setting up new data for AI game by just passing along the logger instead of individual data points\" />\r\n    <MESSAGE value=\"Added Notifications for All data from API, not just final results&#10;&#10;Added new method check_for_notifications_all_data() that takes in the converted data from the API call and uses that to check for notifications.&#10;Renamed original check_for_notifications() to check_for_notifications_final_results() to indicate we are using final results for our checks.&#10;Added several new notification possibilities including a sensor reporting None at all and a z6 not reporting more than 2 data points for a day.\" />\r\n    <MESSAGE value=\"GDD Error range&#10;&#10;Modified get_crop_stage in CwsiProcessor.py to have an extra level (10) for 90-100% Red.&#10;Modified get_crop_stage_level to handle accumulated_gdds higher than 1214 since we were hitting some of those numbers already causing errors.\" />\r\n    <MESSAGE value=\"Stopping Warning Notifications&#10;&#10;Added optional boolean parameters (warnings, errors) defaulting to True to check_for_notifications. Notifications will only be generated if the boolean parameter is true. &#10;Passed in warnings=False to check_for_notifications to stop Warning Notifications for the time being&#10;Commented out writing and emailing Warning Notifications for the time being\" />\r\n    <MESSAGE value=\"VWC Portal Range Bug&#10;&#10;We were using 40 days to determine if we use vwc1 and vwc2 or vwc2 and vwc3 for the portal vwc description when we should be using 30 days.\" />\r\n    <MESSAGE value=\"Splitting Check for notifications and Email notifications&#10;&#10;Added optional parameter email_notifications to Decagon update_information() method to decide when we email notifications&#10;Additional formatting\" />\r\n    <MESSAGE value=\"Moved updated checks for Logger, Field and Grower&#10;&#10;Each class is now in charge of handling its self.updated conditionals for updating instead of it being done by its parent class\" />\r\n    <MESSAGE value=\"Modified Update Retry to handle Notification emailing via the optional parameter email_notifications=True\" />\r\n    <MESSAGE value=\"Logos location\" />\r\n    <MESSAGE value=\"IrrigationRecommendationExpert.py changes&#10;&#10;Modified get_crop_stage() so it uses the new season break points of 30 days after planting, 30 before harvest, and 14 before harvest.\" />\r\n    <MESSAGE value=\"AI Game Init files&#10;&#10;Data and Error pickles to initialize a new AI Game. Both have no game data points in them, just the logger options to play with.\" />\r\n    <MESSAGE value=\"AI Game v2&#10;&#10;The start of the new TKinter AI Game. Stopped mid way to move to Kivy instead.\" />\r\n    <MESSAGE value=\"AI Game v1 updates&#10;&#10;Attempts at adding scroll to the AI Game v1 (not very successfully).&#10;Started adding error handling.\" />\r\n    <MESSAGE value=\"Changes to AIGameData.py&#10;&#10;Added function get_random_logger_from_pool() which takes a parameter indicating if you want the logger from the South, North, or All pool.&#10;Removed individual functions to get a logger from South, North and All pools since its not in 1 function.\" />\r\n    <MESSAGE value=\"KivyTesting.py&#10;&#10;Testing file to load up and understand different Kivy examples.\" />\r\n    <MESSAGE value=\"AIIrrigationGameV3.py&#10;&#10;Brand new version 3 of the AI Game done in Kivy.&#10;The .py file specifies most of the functionality while the .kv file specifies most of the UI.&#10;Spec file was used to package the app into an executable for windows.\" />\r\n    <MESSAGE value=\"YearAnalysis.py&#10;&#10;A lot more work on psi analysis.\" />\r\n    <MESSAGE value=\"get_logger_data optional parameter&#10;&#10;Added file_name optional parameter to get_logger_data in case you want to save the dxd file with a specific name. Useful for analysis later on\" />\r\n    <MESSAGE value=\"Added get_logger_id_and_password_dict()&#10;&#10;This method has everything it needs to go look at the Gsheet where we store the ID's and PW's and return a dict with all of them.\" />\r\n    <MESSAGE value=\"BS\" />\r\n    <MESSAGE value=\"Started HeatUnits.py&#10;&#10;This file will be used to do the processing work for the Heat Units trials this year 2023.&#10;Started by setting up Logger instances for each of the current Weather Stations we have installed.&#10;Setup download of data for each into its on directory.\" />\r\n    <MESSAGE value=\"VP4 Notification Changes&#10;&#10;Simplified vp4 notifications into 1 single method to handle air temp, rh, and vpd notifications. This allows us to just create a single notification if any of those are None instead of one for each type since it is almost never the case that just 1 fails.&#10;New method is called vp4_notifications.&#10;Removed old individual methods for air temp notifications, rh notifications, and vpd notifications.&#10;Formatting changes.\" />\r\n    <option name=\"LAST_COMMIT_MESSAGE\" value=\"VP4 Notification Changes&#10;&#10;Simplified vp4 notifications into 1 single method to handle air temp, rh, and vpd notifications. This allows us to just create a single notification if any of those are None instead of one for each type since it is almost never the case that just 1 fails.&#10;New method is called vp4_notifications.&#10;Removed old individual methods for air temp notifications, rh notifications, and vpd notifications.&#10;Formatting changes.\" />\r\n    <option name=\"OPTIMIZE_IMPORTS_BEFORE_PROJECT_COMMIT\" value=\"true\" />\r\n  </component>\r\n  <component name=\"XDebuggerManager\">\r\n    <breakpoint-manager>\r\n      <breakpoints>\r\n        <line-breakpoint enabled=\"true\" suspend=\"THREAD\" type=\"python-line\">\r\n          <url>file://$PROJECT_DIR$/WeatherTest.py</url>\r\n          <line>233</line>\r\n          <option name=\"timeStamp\" value=\"66\" />\r\n        </line-breakpoint>\r\n        <line-breakpoint enabled=\"true\" suspend=\"THREAD\" type=\"python-line\">\r\n          <url>file://$PROJECT_DIR$/Decagon.py</url>\r\n          <line>3367</line>\r\n          <option name=\"timeStamp\" value=\"109\" />\r\n        </line-breakpoint>\r\n        <line-breakpoint enabled=\"true\" suspend=\"THREAD\" type=\"python-line\">\r\n          <url>file://$PROJECT_DIR$/CIMIS.py</url>\r\n          <line>180</line>\r\n          <option name=\"timeStamp\" value=\"127\" />\r\n        </line-breakpoint>\r\n        <line-breakpoint enabled=\"true\" suspend=\"THREAD\" type=\"python-line\">\r\n          <url>file://$PROJECT_DIR$/Decagon.py</url>\r\n          <line>3324</line>\r\n          <option name=\"timeStamp\" value=\"144\" />\r\n        </line-breakpoint>\r\n        <line-breakpoint enabled=\"true\" suspend=\"THREAD\" type=\"python-line\">\r\n          <url>file://$PROJECT_DIR$/Decagon.py</url>\r\n          <line>3287</line>\r\n          <option name=\"timeStamp\" value=\"147\" />\r\n        </line-breakpoint>\r\n        <line-breakpoint enabled=\"true\" suspend=\"THREAD\" type=\"python-line\">\r\n          <url>file://$PROJECT_DIR$/AIGameData.py</url>\r\n          <line>425</line>\r\n          <option name=\"timeStamp\" value=\"157\" />\r\n        </line-breakpoint>\r\n        <line-breakpoint enabled=\"true\" suspend=\"THREAD\" type=\"python-line\">\r\n          <url>file://$PROJECT_DIR$/IrrigationRecommendationExpert.py</url>\r\n          <line>80</line>\r\n          <option name=\"timeStamp\" value=\"172\" />\r\n        </line-breakpoint>\r\n        <line-breakpoint enabled=\"true\" suspend=\"THREAD\" type=\"python-line\">\r\n          <url>file://$PROJECT_DIR$/Decagon.py</url>\r\n          <line>2927</line>\r\n          <option name=\"timeStamp\" value=\"205\" />\r\n        </line-breakpoint>\r\n        <line-breakpoint enabled=\"true\" suspend=\"THREAD\" type=\"python-line\">\r\n          <url>file://$PROJECT_DIR$/YearAnalysis.py</url>\r\n          <line>1660</line>\r\n          <option name=\"timeStamp\" value=\"215\" />\r\n        </line-breakpoint>\r\n        <line-breakpoint enabled=\"true\" suspend=\"THREAD\" type=\"python-line\">\r\n          <url>file://$PROJECT_DIR$/YearAnalysis.py</url>\r\n          <line>1586</line>\r\n          <option name=\"timeStamp\" value=\"218\" />\r\n        </line-breakpoint>\r\n        <line-breakpoint enabled=\"true\" suspend=\"THREAD\" type=\"python-line\">\r\n          <url>file://$PROJECT_DIR$/YearAnalysis.py</url>\r\n          <line>533</line>\r\n          <option name=\"timeStamp\" value=\"232\" />\r\n        </line-breakpoint>\r\n        <line-breakpoint enabled=\"true\" suspend=\"THREAD\" type=\"python-line\">\r\n          <url>file://$PROJECT_DIR$/Decagon.py</url>\r\n          <line>549</line>\r\n          <option name=\"timeStamp\" value=\"234\" />\r\n        </line-breakpoint>\r\n        <line-breakpoint enabled=\"true\" suspend=\"THREAD\" type=\"python-line\">\r\n          <url>file://$PROJECT_DIR$/YearAnalysis.py</url>\r\n          <line>1518</line>\r\n          <option name=\"timeStamp\" value=\"243\" />\r\n        </line-breakpoint>\r\n        <line-breakpoint enabled=\"true\" suspend=\"THREAD\" type=\"python-line\">\r\n          <url>file://$PROJECT_DIR$/YearAnalysis.py</url>\r\n          <line>1517</line>\r\n          <option name=\"timeStamp\" value=\"244\" />\r\n        </line-breakpoint>\r\n        <line-breakpoint enabled=\"true\" suspend=\"THREAD\" type=\"python-line\">\r\n          <url>file://$PROJECT_DIR$/CwsiProcessor.py</url>\r\n          <line>382</line>\r\n          <option name=\"timeStamp\" value=\"247\" />\r\n        </line-breakpoint>\r\n        <line-breakpoint enabled=\"true\" suspend=\"THREAD\" type=\"python-line\">\r\n          <url>file://$PROJECT_DIR$/YearAnalysis.py</url>\r\n          <line>1508</line>\r\n          <option name=\"timeStamp\" value=\"248\" />\r\n        </line-breakpoint>\r\n        <line-breakpoint enabled=\"true\" suspend=\"THREAD\" type=\"python-line\">\r\n          <url>file://$PROJECT_DIR$/YearAnalysis.py</url>\r\n          <line>1507</line>\r\n          <option name=\"timeStamp\" value=\"249\" />\r\n        </line-breakpoint>\r\n        <line-breakpoint enabled=\"true\" suspend=\"THREAD\" type=\"python-line\">\r\n          <url>file://$PROJECT_DIR$/Decagon.py</url>\r\n          <line>3328</line>\r\n          <option name=\"timeStamp\" value=\"251\" />\r\n        </line-breakpoint>\r\n        <line-breakpoint enabled=\"true\" suspend=\"THREAD\" type=\"python-line\">\r\n          <url>file://$PROJECT_DIR$/Logger.py</url>\r\n          <line>790</line>\r\n          <option name=\"timeStamp\" value=\"259\" />\r\n        </line-breakpoint>\r\n        <line-breakpoint enabled=\"true\" suspend=\"THREAD\" type=\"python-line\">\r\n          <url>file://$PROJECT_DIR$/HeatUnits.py</url>\r\n          <line>47</line>\r\n          <option name=\"timeStamp\" value=\"265\" />\r\n        </line-breakpoint>\r\n        <line-breakpoint enabled=\"true\" suspend=\"THREAD\" type=\"python-line\">\r\n          <url>file://$PROJECT_DIR$/HeatUnits.py</url>\r\n          <line>49</line>\r\n          <option name=\"timeStamp\" value=\"266\" />\r\n        </line-breakpoint>\r\n        <line-breakpoint enabled=\"true\" suspend=\"THREAD\" type=\"python-line\">\r\n          <url>file://$PROJECT_DIR$/HeatUnits.py</url>\r\n          <line>13</line>\r\n          <option name=\"timeStamp\" value=\"267\" />\r\n        </line-breakpoint>\r\n      </breakpoints>\r\n      <default-breakpoints>\r\n        <breakpoint type=\"python-exception\">\r\n          <properties notifyOnTerminate=\"true\" exception=\"BaseException\">\r\n            <option name=\"notifyOnTerminate\" value=\"true\" />\r\n          </properties>\r\n        </breakpoint>\r\n      </default-breakpoints>\r\n    </breakpoint-manager>\r\n  </component>\r\n  <component name=\"com.intellij.coverage.CoverageDataManagerImpl\">\r\n    <SUITE FILE_PATH=\"coverage/S_TOMAto$TKInter.coverage\" NAME=\"TKInter Coverage Results\" MODIFIED=\"1631131790797\" SOURCE_PROVIDER=\"com.intellij.coverage.DefaultCoverageFileProvider\" RUNNER=\"coverage.py\" COVERAGE_BY_TEST_ENABLED=\"true\" COVERAGE_TRACING_ENABLED=\"false\" WORKING_DIRECTORY=\"$PROJECT_DIR$\" />\r\n    <SUITE FILE_PATH=\"coverage/Stomato$SwitchTestCase.coverage\" NAME=\"SwitchTestCase Coverage Results\" MODIFIED=\"1678730749480\" SOURCE_PROVIDER=\"com.intellij.coverage.DefaultCoverageFileProvider\" RUNNER=\"coverage.py\" COVERAGE_BY_TEST_ENABLED=\"true\" COVERAGE_TRACING_ENABLED=\"false\" WORKING_DIRECTORY=\"$PROJECT_DIR$\" />\r\n    <SUITE FILE_PATH=\"coverage/Stomato$DBWriter.coverage\" NAME=\"DBWriter Coverage Results\" MODIFIED=\"1662592976330\" SOURCE_PROVIDER=\"com.intellij.coverage.DefaultCoverageFileProvider\" RUNNER=\"coverage.py\" COVERAGE_BY_TEST_ENABLED=\"true\" COVERAGE_TRACING_ENABLED=\"false\" WORKING_DIRECTORY=\"$PROJECT_DIR$\" />\r\n    <SUITE FILE_PATH=\"coverage/S_TOMAto$CwsiFormulaTester.coverage\" NAME=\"CwsiFormulaTester Coverage Results\" MODIFIED=\"1630459322067\" SOURCE_PROVIDER=\"com.intellij.coverage.DefaultCoverageFileProvider\" RUNNER=\"coverage.py\" COVERAGE_BY_TEST_ENABLED=\"true\" COVERAGE_TRACING_ENABLED=\"false\" WORKING_DIRECTORY=\"$PROJECT_DIR$\" />\r\n    <SUITE FILE_PATH=\"coverage/S_TOMAto$STOMAtoUpdate.coverage\" NAME=\"STOMAtoUpdate Coverage Results\" MODIFIED=\"1642038156945\" SOURCE_PROVIDER=\"com.intellij.coverage.DefaultCoverageFileProvider\" RUNNER=\"coverage.py\" COVERAGE_BY_TEST_ENABLED=\"true\" COVERAGE_TRACING_ENABLED=\"false\" WORKING_DIRECTORY=\"$PROJECT_DIR$\" />\r\n    <SUITE FILE_PATH=\"coverage/Stomato$SQLScripts.coverage\" NAME=\"SQLScripts Coverage Results\" MODIFIED=\"1660142569019\" SOURCE_PROVIDER=\"com.intellij.coverage.DefaultCoverageFileProvider\" RUNNER=\"coverage.py\" COVERAGE_BY_TEST_ENABLED=\"true\" COVERAGE_TRACING_ENABLED=\"false\" WORKING_DIRECTORY=\"$PROJECT_DIR$\" />\r\n    <SUITE FILE_PATH=\"coverage/S_TOMAto$formatHistoricalET.coverage\" NAME=\"formatHistoricalET Coverage Results\" MODIFIED=\"1620265637963\" SOURCE_PROVIDER=\"com.intellij.coverage.DefaultCoverageFileProvider\" RUNNER=\"coverage.py\" COVERAGE_BY_TEST_ENABLED=\"true\" COVERAGE_TRACING_ENABLED=\"false\" WORKING_DIRECTORY=\"$PROJECT_DIR$\" />\r\n    <SUITE FILE_PATH=\"coverage/Stomato$AIIrrigationGameV3__3_.coverage\" NAME=\"AIIrrigationGameV3 (3) Coverage Results\" MODIFIED=\"1686323145119\" SOURCE_PROVIDER=\"com.intellij.coverage.DefaultCoverageFileProvider\" RUNNER=\"coverage.py\" COVERAGE_BY_TEST_ENABLED=\"true\" COVERAGE_TRACING_ENABLED=\"false\" WORKING_DIRECTORY=\"$PROJECT_DIR$/AIGame\" />\r\n    <SUITE FILE_PATH=\"coverage/Stomato$Soils.coverage\" NAME=\"Soils Coverage Results\" MODIFIED=\"1683932387848\" SOURCE_PROVIDER=\"com.intellij.coverage.DefaultCoverageFileProvider\" RUNNER=\"coverage.py\" COVERAGE_BY_TEST_ENABLED=\"true\" COVERAGE_TRACING_ENABLED=\"false\" WORKING_DIRECTORY=\"$PROJECT_DIR$\" />\r\n    <SUITE FILE_PATH=\"coverage/Stomato$formatHistoricalET.coverage\" NAME=\"formatHistoricalET Coverage Results\" MODIFIED=\"1615220646151\" SOURCE_PROVIDER=\"com.intellij.coverage.DefaultCoverageFileProvider\" RUNNER=\"coverage.py\" COVERAGE_BY_TEST_ENABLED=\"true\" COVERAGE_TRACING_ENABLED=\"false\" WORKING_DIRECTORY=\"$PROJECT_DIR$\" />\r\n    <SUITE FILE_PATH=\"coverage/Stomato$PrerunMaintenance.coverage\" NAME=\"PrerunMaintenance Coverage Results\" MODIFIED=\"1654037151077\" SOURCE_PROVIDER=\"com.intellij.coverage.DefaultCoverageFileProvider\" RUNNER=\"coverage.py\" COVERAGE_BY_TEST_ENABLED=\"true\" COVERAGE_TRACING_ENABLED=\"false\" WORKING_DIRECTORY=\"$PROJECT_DIR$\" />\r\n    <SUITE FILE_PATH=\"coverage/Stomato$gmailTesting.coverage\" NAME=\"gmailTesting Coverage Results\" MODIFIED=\"1655757137703\" SOURCE_PROVIDER=\"com.intellij.coverage.DefaultCoverageFileProvider\" RUNNER=\"coverage.py\" COVERAGE_BY_TEST_ENABLED=\"true\" COVERAGE_TRACING_ENABLED=\"false\" WORKING_DIRECTORY=\"$PROJECT_DIR$\" />\r\n    <SUITE FILE_PATH=\"coverage/Stomato$uninstallFields.coverage\" NAME=\"uninstallFields Coverage Results\" MODIFIED=\"1666393598857\" SOURCE_PROVIDER=\"com.intellij.coverage.DefaultCoverageFileProvider\" RUNNER=\"coverage.py\" COVERAGE_BY_TEST_ENABLED=\"true\" COVERAGE_TRACING_ENABLED=\"false\" WORKING_DIRECTORY=\"$PROJECT_DIR$\" />\r\n    <SUITE FILE_PATH=\"coverage/Stomato$sqlTesting.coverage\" NAME=\"sqlTesting Coverage Results\" MODIFIED=\"1612901497912\" SOURCE_PROVIDER=\"com.intellij.coverage.DefaultCoverageFileProvider\" RUNNER=\"coverage.py\" COVERAGE_BY_TEST_ENABLED=\"true\" COVERAGE_TRACING_ENABLED=\"false\" WORKING_DIRECTORY=\"$PROJECT_DIR$\" />\r\n    <SUITE FILE_PATH=\"coverage/Stomato$TKInter.coverage\" NAME=\"TKInter Coverage Results\" MODIFIED=\"1678243631453\" SOURCE_PROVIDER=\"com.intellij.coverage.DefaultCoverageFileProvider\" RUNNER=\"coverage.py\" COVERAGE_BY_TEST_ENABLED=\"true\" COVERAGE_TRACING_ENABLED=\"false\" WORKING_DIRECTORY=\"$PROJECT_DIR$\" />\r\n    <SUITE FILE_PATH=\"coverage/S_TOMAto$STOMAtoUpdateRetry.coverage\" NAME=\"STOMAtoUpdateRetry Coverage Results\" MODIFIED=\"1632331197894\" SOURCE_PROVIDER=\"com.intellij.coverage.DefaultCoverageFileProvider\" RUNNER=\"coverage.py\" COVERAGE_BY_TEST_ENABLED=\"true\" COVERAGE_TRACING_ENABLED=\"false\" WORKING_DIRECTORY=\"$PROJECT_DIR$\" />\r\n    <SUITE FILE_PATH=\"coverage/S_TOMAto$DBWriter.coverage\" NAME=\"DBWriter Coverage Results\" MODIFIED=\"1621249399322\" SOURCE_PROVIDER=\"com.intellij.coverage.DefaultCoverageFileProvider\" RUNNER=\"coverage.py\" COVERAGE_BY_TEST_ENABLED=\"true\" COVERAGE_TRACING_ENABLED=\"false\" WORKING_DIRECTORY=\"$PROJECT_DIR$\" />\r\n    <SUITE FILE_PATH=\"coverage/Stomato$HeatUnits.coverage\" NAME=\"HeatUnits Coverage Results\" MODIFIED=\"1686881280835\" SOURCE_PROVIDER=\"com.intellij.coverage.DefaultCoverageFileProvider\" RUNNER=\"coverage.py\" COVERAGE_BY_TEST_ENABLED=\"true\" COVERAGE_TRACING_ENABLED=\"false\" WORKING_DIRECTORY=\"$PROJECT_DIR$\" />\r\n    <SUITE FILE_PATH=\"coverage/S_TOMAto$.coverage\" NAME=\" Coverage Results\" MODIFIED=\"1634172899963\" SOURCE_PROVIDER=\"com.intellij.coverage.DefaultCoverageFileProvider\" RUNNER=\"coverage.py\" COVERAGE_BY_TEST_ENABLED=\"true\" COVERAGE_TRACING_ENABLED=\"false\" WORKING_DIRECTORY=\"$PROJECT_DIR$\" />\r\n    <SUITE FILE_PATH=\"coverage/S_TOMAto$CwsiProcessor.coverage\" NAME=\"CwsiProcessor Coverage Results\" MODIFIED=\"1619518007322\" SOURCE_PROVIDER=\"com.intellij.coverage.DefaultCoverageFileProvider\" RUNNER=\"coverage.py\" COVERAGE_BY_TEST_ENABLED=\"true\" COVERAGE_TRACING_ENABLED=\"false\" WORKING_DIRECTORY=\"$PROJECT_DIR$\" />\r\n    <SUITE FILE_PATH=\"coverage/Stomato$AIIrrigationGameV3__2_.coverage\" NAME=\"AIIrrigationGameV3 (2) Coverage Results\" MODIFIED=\"1686273006519\" SOURCE_PROVIDER=\"com.intellij.coverage.DefaultCoverageFileProvider\" RUNNER=\"coverage.py\" COVERAGE_BY_TEST_ENABLED=\"true\" COVERAGE_TRACING_ENABLED=\"false\" WORKING_DIRECTORY=\"$PROJECT_DIR$/AI Game\" />\r\n    <SUITE FILE_PATH=\"coverage/Stomato$EmailTester.coverage\" NAME=\"EmailTester Coverage Results\" MODIFIED=\"1681779541242\" SOURCE_PROVIDER=\"com.intellij.coverage.DefaultCoverageFileProvider\" RUNNER=\"coverage.py\" COVERAGE_BY_TEST_ENABLED=\"true\" COVERAGE_TRACING_ENABLED=\"false\" WORKING_DIRECTORY=\"$PROJECT_DIR$\" />\r\n    <SUITE FILE_PATH=\"coverage/Stomato$loggerSetups.coverage\" NAME=\"loggerSetups Coverage Results\" MODIFIED=\"1679606405842\" SOURCE_PROVIDER=\"com.intellij.coverage.DefaultCoverageFileProvider\" RUNNER=\"coverage.py\" COVERAGE_BY_TEST_ENABLED=\"true\" COVERAGE_TRACING_ENABLED=\"false\" WORKING_DIRECTORY=\"$PROJECT_DIR$\" />\r\n    <SUITE FILE_PATH=\"coverage/S_TOMAto$loggerSetups.coverage\" NAME=\"loggerSetups Coverage Results\" MODIFIED=\"1639095975188\" SOURCE_PROVIDER=\"com.intellij.coverage.DefaultCoverageFileProvider\" RUNNER=\"coverage.py\" COVERAGE_BY_TEST_ENABLED=\"true\" COVERAGE_TRACING_ENABLED=\"false\" WORKING_DIRECTORY=\"$PROJECT_DIR$\" />\r\n    <SUITE FILE_PATH=\"coverage/Stomato$AIExpertSystemTesting.coverage\" NAME=\"AIExpertSystemTesting Coverage Results\" MODIFIED=\"1683677065044\" SOURCE_PROVIDER=\"com.intellij.coverage.DefaultCoverageFileProvider\" RUNNER=\"coverage.py\" COVERAGE_BY_TEST_ENABLED=\"true\" COVERAGE_TRACING_ENABLED=\"false\" WORKING_DIRECTORY=\"$PROJECT_DIR$\" />\r\n    <SUITE FILE_PATH=\"coverage/S_TOMAto$CimisUpdate.coverage\" NAME=\"CimisUpdate Coverage Results\" MODIFIED=\"1621340772933\" SOURCE_PROVIDER=\"com.intellij.coverage.DefaultCoverageFileProvider\" RUNNER=\"coverage.py\" COVERAGE_BY_TEST_ENABLED=\"true\" COVERAGE_TRACING_ENABLED=\"false\" WORKING_DIRECTORY=\"$PROJECT_DIR$\" />\r\n    <SUITE FILE_PATH=\"coverage/S_TOMAto$Decagon.coverage\" NAME=\"Decagon Coverage Results\" MODIFIED=\"1687765303466\" SOURCE_PROVIDER=\"com.intellij.coverage.DefaultCoverageFileProvider\" RUNNER=\"coverage.py\" COVERAGE_BY_TEST_ENABLED=\"true\" COVERAGE_TRACING_ENABLED=\"false\" WORKING_DIRECTORY=\"$PROJECT_DIR$\" />\r\n    <SUITE FILE_PATH=\"coverage/Stomato$gSheetTest.coverage\" NAME=\"gSheetTest Coverage Results\" MODIFIED=\"1609745391990\" SOURCE_PROVIDER=\"com.intellij.coverage.DefaultCoverageFileProvider\" RUNNER=\"coverage.py\" COVERAGE_BY_TEST_ENABLED=\"true\" COVERAGE_TRACING_ENABLED=\"false\" WORKING_DIRECTORY=\"$PROJECT_DIR$\" />\r\n    <SUITE FILE_PATH=\"coverage/Stomato$WeatherTest.coverage\" NAME=\"WeatherTest Coverage Results\" MODIFIED=\"1678321995221\" SOURCE_PROVIDER=\"com.intellij.coverage.DefaultCoverageFileProvider\" RUNNER=\"coverage.py\" COVERAGE_BY_TEST_ENABLED=\"true\" COVERAGE_TRACING_ENABLED=\"false\" WORKING_DIRECTORY=\"$PROJECT_DIR$\" />\r\n    <SUITE FILE_PATH=\"coverage/Stomato$AIGameData.coverage\" NAME=\"AIGameData Coverage Results\" MODIFIED=\"1681321697188\" SOURCE_PROVIDER=\"com.intellij.coverage.DefaultCoverageFileProvider\" RUNNER=\"coverage.py\" COVERAGE_BY_TEST_ENABLED=\"true\" COVERAGE_TRACING_ENABLED=\"false\" WORKING_DIRECTORY=\"$PROJECT_DIR$\" />\r\n    <SUITE FILE_PATH=\"coverage/S_TOMAto$AIGameData.coverage\" NAME=\"AIGameData Coverage Results\" MODIFIED=\"1632877501080\" SOURCE_PROVIDER=\"com.intellij.coverage.DefaultCoverageFileProvider\" RUNNER=\"coverage.py\" COVERAGE_BY_TEST_ENABLED=\"true\" COVERAGE_TRACING_ENABLED=\"false\" WORKING_DIRECTORY=\"$PROJECT_DIR$\" />\r\n    <SUITE FILE_PATH=\"coverage/Stomato$AIIrrigationGameV3.coverage\" NAME=\"AIIrrigationGameV3 Coverage Results\" MODIFIED=\"1686272739977\" SOURCE_PROVIDER=\"com.intellij.coverage.DefaultCoverageFileProvider\" RUNNER=\"coverage.py\" COVERAGE_BY_TEST_ENABLED=\"true\" COVERAGE_TRACING_ENABLED=\"false\" WORKING_DIRECTORY=\"$PROJECT_DIR$\" />\r\n    <SUITE FILE_PATH=\"coverage/S_TOMAto$AIExpertSystemTesting.coverage\" NAME=\"AIExpertSystemTesting Coverage Results\" MODIFIED=\"1636765308468\" SOURCE_PROVIDER=\"com.intellij.coverage.DefaultCoverageFileProvider\" RUNNER=\"coverage.py\" COVERAGE_BY_TEST_ENABLED=\"true\" COVERAGE_TRACING_ENABLED=\"false\" WORKING_DIRECTORY=\"$PROJECT_DIR$\" />\r\n    <SUITE FILE_PATH=\"coverage/Stomato$AIIrrigationGameV3__1_.coverage\" NAME=\"AIIrrigationGameV3 (1) Coverage Results\" MODIFIED=\"1686272827720\" SOURCE_PROVIDER=\"com.intellij.coverage.DefaultCoverageFileProvider\" RUNNER=\"coverage.py\" COVERAGE_BY_TEST_ENABLED=\"true\" COVERAGE_TRACING_ENABLED=\"false\" WORKING_DIRECTORY=\"$PROJECT_DIR$/AI\" />\r\n    <SUITE FILE_PATH=\"coverage/Stomato$Decagon.coverage\" NAME=\"Decagon Coverage Results\" MODIFIED=\"1683932813529\" SOURCE_PROVIDER=\"com.intellij.coverage.DefaultCoverageFileProvider\" RUNNER=\"coverage.py\" COVERAGE_BY_TEST_ENABLED=\"true\" COVERAGE_TRACING_ENABLED=\"false\" WORKING_DIRECTORY=\"$PROJECT_DIR$\" />\r\n    <SUITE FILE_PATH=\"coverage/S_TOMAto$Field.coverage\" NAME=\"Field Coverage Results\" MODIFIED=\"1618891943834\" SOURCE_PROVIDER=\"com.intellij.coverage.DefaultCoverageFileProvider\" RUNNER=\"coverage.py\" COVERAGE_BY_TEST_ENABLED=\"true\" COVERAGE_TRACING_ENABLED=\"false\" WORKING_DIRECTORY=\"$PROJECT_DIR$\" />\r\n    <SUITE FILE_PATH=\"coverage/Stomato$CimisUpdate.coverage\" NAME=\"CimisUpdate Coverage Results\" MODIFIED=\"1660869984017\" SOURCE_PROVIDER=\"com.intellij.coverage.DefaultCoverageFileProvider\" RUNNER=\"coverage.py\" COVERAGE_BY_TEST_ENABLED=\"true\" COVERAGE_TRACING_ENABLED=\"false\" WORKING_DIRECTORY=\"$PROJECT_DIR$\" />\r\n    <SUITE FILE_PATH=\"coverage/Stomato$STOMAtoUpdateRetry.coverage\" NAME=\"STOMAtoUpdateRetry Coverage Results\" MODIFIED=\"1659446223871\" SOURCE_PROVIDER=\"com.intellij.coverage.DefaultCoverageFileProvider\" RUNNER=\"coverage.py\" COVERAGE_BY_TEST_ENABLED=\"true\" COVERAGE_TRACING_ENABLED=\"false\" WORKING_DIRECTORY=\"$PROJECT_DIR$\" />\r\n    <SUITE FILE_PATH=\"coverage/Stomato$CimisStation.coverage\" NAME=\"CimisStation Coverage Results\" MODIFIED=\"1657387377095\" SOURCE_PROVIDER=\"com.intellij.coverage.DefaultCoverageFileProvider\" RUNNER=\"coverage.py\" COVERAGE_BY_TEST_ENABLED=\"true\" COVERAGE_TRACING_ENABLED=\"false\" WORKING_DIRECTORY=\"$PROJECT_DIR$\" />\r\n    <SUITE FILE_PATH=\"coverage/S_TOMAto$SQLScripts.coverage\" NAME=\"SQLScripts Coverage Results\" MODIFIED=\"1642230710365\" SOURCE_PROVIDER=\"com.intellij.coverage.DefaultCoverageFileProvider\" RUNNER=\"coverage.py\" COVERAGE_BY_TEST_ENABLED=\"true\" COVERAGE_TRACING_ENABLED=\"false\" WORKING_DIRECTORY=\"$PROJECT_DIR$\" />\r\n    <SUITE FILE_PATH=\"coverage/Stomato$EmailProcessor.coverage\" NAME=\"EmailProcessor Coverage Results\" MODIFIED=\"1655838431141\" SOURCE_PROVIDER=\"com.intellij.coverage.DefaultCoverageFileProvider\" RUNNER=\"coverage.py\" COVERAGE_BY_TEST_ENABLED=\"true\" COVERAGE_TRACING_ENABLED=\"false\" WORKING_DIRECTORY=\"$PROJECT_DIR$\" />\r\n    <SUITE FILE_PATH=\"coverage/Stomato$KivyTesting.coverage\" NAME=\"KivyTesting Coverage Results\" MODIFIED=\"1685580311134\" SOURCE_PROVIDER=\"com.intellij.coverage.DefaultCoverageFileProvider\" RUNNER=\"coverage.py\" COVERAGE_BY_TEST_ENABLED=\"true\" COVERAGE_TRACING_ENABLED=\"false\" WORKING_DIRECTORY=\"$PROJECT_DIR$\" />\r\n    <SUITE FILE_PATH=\"coverage/Stomato_dev$Decagon.coverage\" NAME=\"Decagon Coverage Results\" MODIFIED=\"1611789454686\" SOURCE_PROVIDER=\"com.intellij.coverage.DefaultCoverageFileProvider\" RUNNER=\"coverage.py\" COVERAGE_BY_TEST_ENABLED=\"true\" COVERAGE_TRACING_ENABLED=\"false\" WORKING_DIRECTORY=\"$PROJECT_DIR$\" />\r\n    <SUITE FILE_PATH=\"coverage/Stomato$bigQueryTest.coverage\" NAME=\"bigQueryTest Coverage Results\" MODIFIED=\"1608572673786\" SOURCE_PROVIDER=\"com.intellij.coverage.DefaultCoverageFileProvider\" RUNNER=\"coverage.py\" COVERAGE_BY_TEST_ENABLED=\"true\" COVERAGE_TRACING_ENABLED=\"false\" WORKING_DIRECTORY=\"$PROJECT_DIR$\" />\r\n    <SUITE FILE_PATH=\"coverage/Stomato$DataTests.coverage\" NAME=\"DataTests Coverage Results\" MODIFIED=\"1679072769429\" SOURCE_PROVIDER=\"com.intellij.coverage.DefaultCoverageFileProvider\" RUNNER=\"coverage.py\" COVERAGE_BY_TEST_ENABLED=\"true\" COVERAGE_TRACING_ENABLED=\"false\" WORKING_DIRECTORY=\"$PROJECT_DIR$\" />\r\n    <SUITE FILE_PATH=\"coverage/Stomato$YearAnalysis.coverage\" NAME=\"YearAnalysis Coverage Results\" MODIFIED=\"1683672160830\" SOURCE_PROVIDER=\"com.intellij.coverage.DefaultCoverageFileProvider\" RUNNER=\"coverage.py\" COVERAGE_BY_TEST_ENABLED=\"true\" COVERAGE_TRACING_ENABLED=\"false\" WORKING_DIRECTORY=\"$PROJECT_DIR$\" />\r\n    <SUITE FILE_PATH=\"coverage/S_TOMAto$Testing.coverage\" NAME=\"Testing Coverage Results\" MODIFIED=\"1628198021719\" SOURCE_PROVIDER=\"com.intellij.coverage.DefaultCoverageFileProvider\" RUNNER=\"coverage.py\" COVERAGE_BY_TEST_ENABLED=\"true\" COVERAGE_TRACING_ENABLED=\"false\" WORKING_DIRECTORY=\"$PROJECT_DIR$\" />\r\n    <SUITE FILE_PATH=\"coverage/Stomato$AIIIrrigationGameV2.coverage\" NAME=\"AIIIrrigationGameV2 Coverage Results\" MODIFIED=\"1684454590969\" SOURCE_PROVIDER=\"com.intellij.coverage.DefaultCoverageFileProvider\" RUNNER=\"coverage.py\" COVERAGE_BY_TEST_ENABLED=\"true\" COVERAGE_TRACING_ENABLED=\"false\" WORKING_DIRECTORY=\"$PROJECT_DIR$\" />\r\n    <SUITE FILE_PATH=\"coverage/Stomato$AIIrrigationGameV3__4_.coverage\" NAME=\"AIIrrigationGameV3 (4) Coverage Results\" MODIFIED=\"1686334569719\" SOURCE_PROVIDER=\"com.intellij.coverage.DefaultCoverageFileProvider\" RUNNER=\"coverage.py\" COVERAGE_BY_TEST_ENABLED=\"true\" COVERAGE_TRACING_ENABLED=\"false\" WORKING_DIRECTORY=\"$PROJECT_DIR$\" />\r\n    <SUITE FILE_PATH=\"coverage/Stomato$technicianPortal.coverage\" NAME=\"technicianPortal Coverage Results\" MODIFIED=\"1670450349216\" SOURCE_PROVIDER=\"com.intellij.coverage.DefaultCoverageFileProvider\" RUNNER=\"coverage.py\" COVERAGE_BY_TEST_ENABLED=\"true\" COVERAGE_TRACING_ENABLED=\"false\" WORKING_DIRECTORY=\"$PROJECT_DIR$\" />\r\n    <SUITE FILE_PATH=\"coverage/Stomato$schedulerTest.coverage\" NAME=\"schedulerTest Coverage Results\" MODIFIED=\"1659650548778\" SOURCE_PROVIDER=\"com.intellij.coverage.DefaultCoverageFileProvider\" RUNNER=\"coverage.py\" COVERAGE_BY_TEST_ENABLED=\"true\" COVERAGE_TRACING_ENABLED=\"false\" WORKING_DIRECTORY=\"$PROJECT_DIR$\" />\r\n    <SUITE FILE_PATH=\"coverage/S_TOMAto$CIMIS.coverage\" NAME=\"CIMIS Coverage Results\" MODIFIED=\"1618829670273\" SOURCE_PROVIDER=\"com.intellij.coverage.DefaultCoverageFileProvider\" RUNNER=\"coverage.py\" COVERAGE_BY_TEST_ENABLED=\"true\" COVERAGE_TRACING_ENABLED=\"false\" WORKING_DIRECTORY=\"$PROJECT_DIR$\" />\r\n    <SUITE FILE_PATH=\"coverage/Stomato$STOMAtoUpdate.coverage\" NAME=\"STOMAtoUpdate Coverage Results\" MODIFIED=\"1643531784052\" SOURCE_PROVIDER=\"com.intellij.coverage.DefaultCoverageFileProvider\" RUNNER=\"coverage.py\" COVERAGE_BY_TEST_ENABLED=\"true\" COVERAGE_TRACING_ENABLED=\"false\" WORKING_DIRECTORY=\"$PROJECT_DIR$\" />\r\n  </component>\r\n  <component name=\"masterDetails\">\r\n    <states>\r\n      <state key=\"ScopeChooserConfigurable.UI\">\r\n        <settings>\r\n          <splitter-proportions>\r\n            <option name=\"proportions\">\r\n              <list>\r\n                <option value=\"0.2\" />\r\n              </list>\r\n            </option>\r\n          </splitter-proportions>\r\n        </settings>\r\n      </state>\r\n    </states>\r\n  </component>\r\n</project>
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/.idea/workspace.xml b/.idea/workspace.xml
--- a/.idea/workspace.xml	(revision 5cf7d78a1d94023d00661edf5517badacc064130)
+++ b/.idea/workspace.xml	(date 1720741762654)
@@ -7,15 +7,23 @@
     <option name="autoReloadType" value="SELECTIVE" />
   </component>
   <component name="ChangeListManager">
-    <list default="true" id="3aa7c90b-1f87-403f-bac4-0b49ed107be9" name="Default" comment="VP4 Notification Changes&#10;&#10;Simplified vp4 notifications into 1 single method to handle air temp, rh, and vpd notifications. This allows us to just create a single notification if any of those are None instead of one for each type since it is almost never the case that just 1 fails.&#10;New method is called vp4_notifications.&#10;Removed old individual methods for air temp notifications, rh notifications, and vpd notifications.&#10;Formatting changes.">
+    <list default="true" id="3aa7c90b-1f87-403f-bac4-0b49ed107be9" name="Default" comment="Soil Type Flow Fix&#10;&#10;In Logger Setups if there is a soil type submitted we use that instead of calling the soil survey, else use the soil survey">
+      <change afterPath="$PROJECT_DIR$/.env" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/SlackBot.py" afterDir="false" />
+      <change afterPath="$PROJECT_DIR$/SoilAPI.py" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/.idea/Stomato.iml" beforeDir="false" afterPath="$PROJECT_DIR$/.idea/Stomato.iml" afterDir="false" />
       <change beforePath="$PROJECT_DIR$/.idea/misc.xml" beforeDir="false" afterPath="$PROJECT_DIR$/.idea/misc.xml" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/.idea/vcs.xml" beforeDir="false" afterPath="$PROJECT_DIR$/.idea/vcs.xml" afterDir="false" />
       <change beforePath="$PROJECT_DIR$/.idea/workspace.xml" beforeDir="false" afterPath="$PROJECT_DIR$/.idea/workspace.xml" afterDir="false" />
-      <change beforePath="$PROJECT_DIR$/CimisUpdate.py" beforeDir="false" afterPath="$PROJECT_DIR$/CimisUpdate.py" afterDir="false" />
       <change beforePath="$PROJECT_DIR$/Decagon.py" beforeDir="false" afterPath="$PROJECT_DIR$/Decagon.py" afterDir="false" />
-      <change beforePath="$PROJECT_DIR$/ai_data.csv" beforeDir="false" afterPath="$PROJECT_DIR$/ai_data.csv" afterDir="false" />
-      <change beforePath="$PROJECT_DIR$/all et.csv" beforeDir="false" afterPath="$PROJECT_DIR$/all et.csv" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/Field.py" beforeDir="false" afterPath="$PROJECT_DIR$/Field.py" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/KMLHandler.py" beforeDir="false" afterPath="$PROJECT_DIR$/KMLHandler.py" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/Logger.py" beforeDir="false" afterPath="$PROJECT_DIR$/Logger.py" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/LoggerSetups.py" beforeDir="false" afterPath="$PROJECT_DIR$/LoggerSetups.py" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/PickleHandler.py" beforeDir="false" afterPath="$PROJECT_DIR$/PickleHandler.py" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/SQLScripts.py" beforeDir="false" afterPath="$PROJECT_DIR$/SQLScripts.py" afterDir="false" />
       <change beforePath="$PROJECT_DIR$/data.csv" beforeDir="false" afterPath="$PROJECT_DIR$/data.csv" afterDir="false" />
-      <change beforePath="$PROJECT_DIR$/irrScheduling.csv" beforeDir="false" afterPath="$PROJECT_DIR$/irrScheduling.csv" afterDir="false" />
+      <change beforePath="$PROJECT_DIR$/historicalET.csv" beforeDir="false" afterPath="$PROJECT_DIR$/historicalET.csv" afterDir="false" />
       <change beforePath="$PROJECT_DIR$/portal_data.csv" beforeDir="false" afterPath="$PROJECT_DIR$/portal_data.csv" afterDir="false" />
       <change beforePath="$PROJECT_DIR$/weather forecast.csv" beforeDir="false" afterPath="$PROJECT_DIR$/weather forecast.csv" afterDir="false" />
     </list>
@@ -77,11 +85,17 @@
   <component name="Git.Settings">
     <option name="RECENT_BRANCH_BY_REPOSITORY">
       <map>
-        <entry key="$PROJECT_DIR$" value="master" />
+        <entry key="$PROJECT_DIR$" value="backup-branch" />
       </map>
     </option>
     <option name="RECENT_GIT_ROOT_PATH" value="$PROJECT_DIR$" />
   </component>
+  <component name="GitHubPullRequestSearchHistory"><![CDATA[{
+  "lastFilter": {
+    "state": "OPEN",
+    "assignee": "odolan-gradient"
+  }
+}]]></component>
   <component name="GitSEFilterConfiguration">
     <file-type-list>
       <filtered-out-file-type name="LOCAL_BRANCH" />
@@ -90,6 +104,12 @@
       <filtered-out-file-type name="COMMIT_BY_MESSAGE" />
     </file-type-list>
   </component>
+  <component name="GithubPullRequestsUISettings"><![CDATA[{
+  "selectedUrlAndAccountId": {
+    "url": "https://github.com/jgarrido-ms/Stomato.git",
+    "accountId": "860cc125-bb45-46ea-8c5c-c9197d9067dd"
+  }
+}]]></component>
   <component name="HighlightingSettingsPerFile">
     <setting file="mock:///" root0="FORCE_HIGHLIGHTING" />
   </component>
@@ -100,6 +120,10 @@
   <component name="ProblemsViewState">
     <option name="selectedIndex" value="2" />
   </component>
+  <component name="ProjectColorInfo"><![CDATA[{
+  "customColor": "",
+  "associatedIndex": 0
+}]]></component>
   <component name="ProjectFrameBounds">
     <option name="x" value="-8" />
     <option name="y" value="-8" />
@@ -114,21 +138,27 @@
     <option name="hideEmptyMiddlePackages" value="true" />
     <option name="showLibraryContents" value="true" />
   </component>
-  <component name="PropertiesComponent">{
-  &quot;keyToString&quot;: {
-    &quot;WebServerToolWindowFactoryState&quot;: &quot;false&quot;,
-    &quot;git-widget-placeholder&quot;: &quot;master&quot;,
-    &quot;ignore.virus.scanning.warn.message&quot;: &quot;true&quot;,
-    &quot;last_opened_file_path&quot;: &quot;C:/Users/javie/PycharmProjects/Stomato&quot;,
-    &quot;node.js.detected.package.eslint&quot;: &quot;true&quot;,
-    &quot;node.js.detected.package.tslint&quot;: &quot;true&quot;,
-    &quot;node.js.selected.package.eslint&quot;: &quot;(autodetect)&quot;,
-    &quot;node.js.selected.package.tslint&quot;: &quot;(autodetect)&quot;,
-    &quot;nodejs_package_manager_path&quot;: &quot;npm&quot;,
-    &quot;settings.editor.selected.configurable&quot;: &quot;configurable.group.language&quot;,
-    &quot;vue.rearranger.settings.migration&quot;: &quot;true&quot;
+  <component name="PropertiesComponent"><![CDATA[{
+  "keyToString": {
+    "Python.Decagon.executor": "Run",
+    "Python.KMLHandler.executor": "Run",
+    "Python.LoggerSetups.executor": "Run",
+    "Python.PrerunMaintenance.executor": "Debug",
+    "Python.SQLScripts.executor": "Run",
+    "Python.SlackBot.executor": "Run",
+    "WebServerToolWindowFactoryState": "false",
+    "git-widget-placeholder": "master",
+    "ignore.virus.scanning.warn.message": "true",
+    "last_opened_file_path": "C:/Users/javie/PycharmProjects/Stomato",
+    "node.js.detected.package.eslint": "true",
+    "node.js.detected.package.tslint": "true",
+    "node.js.selected.package.eslint": "(autodetect)",
+    "node.js.selected.package.tslint": "(autodetect)",
+    "nodejs_package_manager_path": "npm",
+    "settings.editor.selected.configurable": "reference.idesettings.debugger.python",
+    "vue.rearranger.settings.migration": "true"
   }
-}</component>
+}]]></component>
   <component name="PyConsoleOptionsProvider">
     <option name="myPythonConsoleState">
       <console-settings module-name="Stomato" is-module-sdk="true">
@@ -136,9 +166,6 @@
         <option name="myModuleName" value="Stomato" />
       </console-settings>
     </option>
-  </component>
-  <component name="PyDebuggerOptionsProvider">
-    <option name="mySupportGeventDebugging" value="true" />
   </component>
   <component name="RecentsManager">
     <key name="CopyFile.RECENT_KEYS">
@@ -174,20 +201,21 @@
       <option name="USE_PATTERN" value="false" />
       <method />
     </configuration>
-    <configuration name="AIIrrigationGameV3 (2)" type="PythonConfigurationType" factoryName="Python" temporary="true" nameIsGenerated="true">
+    <configuration name="Decagon" type="PythonConfigurationType" factoryName="Python" temporary="true" nameIsGenerated="true">
       <module name="Stomato" />
+      <option name="ENV_FILES" value="" />
       <option name="INTERPRETER_OPTIONS" value="" />
       <option name="PARENT_ENVS" value="true" />
       <envs>
         <env name="PYTHONUNBUFFERED" value="1" />
       </envs>
       <option name="SDK_HOME" value="" />
-      <option name="WORKING_DIRECTORY" value="$PROJECT_DIR$/AI Game" />
+      <option name="WORKING_DIRECTORY" value="$PROJECT_DIR$" />
       <option name="IS_MODULE_SDK" value="true" />
       <option name="ADD_CONTENT_ROOTS" value="true" />
       <option name="ADD_SOURCE_ROOTS" value="true" />
       <EXTENSION ID="PythonCoverageRunConfigurationExtension" runner="coverage.py" />
-      <option name="SCRIPT_NAME" value="$PROJECT_DIR$/AI Game/AIIrrigationGameV3.py" />
+      <option name="SCRIPT_NAME" value="$PROJECT_DIR$/Decagon.py" />
       <option name="PARAMETERS" value="" />
       <option name="SHOW_COMMAND_LINE" value="false" />
       <option name="EMULATE_TERMINAL" value="false" />
@@ -196,20 +224,21 @@
       <option name="INPUT_FILE" value="" />
       <method v="2" />
     </configuration>
-    <configuration name="AIIrrigationGameV3 (3)" type="PythonConfigurationType" factoryName="Python" temporary="true" nameIsGenerated="true">
+    <configuration name="KMLHandler" type="PythonConfigurationType" factoryName="Python" temporary="true" nameIsGenerated="true">
       <module name="Stomato" />
+      <option name="ENV_FILES" value="" />
       <option name="INTERPRETER_OPTIONS" value="" />
       <option name="PARENT_ENVS" value="true" />
       <envs>
         <env name="PYTHONUNBUFFERED" value="1" />
       </envs>
       <option name="SDK_HOME" value="" />
-      <option name="WORKING_DIRECTORY" value="$PROJECT_DIR$/AIGame" />
+      <option name="WORKING_DIRECTORY" value="$PROJECT_DIR$" />
       <option name="IS_MODULE_SDK" value="true" />
       <option name="ADD_CONTENT_ROOTS" value="true" />
       <option name="ADD_SOURCE_ROOTS" value="true" />
       <EXTENSION ID="PythonCoverageRunConfigurationExtension" runner="coverage.py" />
-      <option name="SCRIPT_NAME" value="$PROJECT_DIR$/AIGame/AIIrrigationGameV3.py" />
+      <option name="SCRIPT_NAME" value="$PROJECT_DIR$/KMLHandler.py" />
       <option name="PARAMETERS" value="" />
       <option name="SHOW_COMMAND_LINE" value="false" />
       <option name="EMULATE_TERMINAL" value="false" />
@@ -218,8 +247,9 @@
       <option name="INPUT_FILE" value="" />
       <method v="2" />
     </configuration>
-    <configuration name="AIIrrigationGameV3 (4)" type="PythonConfigurationType" factoryName="Python" temporary="true" nameIsGenerated="true">
+    <configuration name="LoggerSetups" type="PythonConfigurationType" factoryName="Python" temporary="true" nameIsGenerated="true">
       <module name="Stomato" />
+      <option name="ENV_FILES" value="" />
       <option name="INTERPRETER_OPTIONS" value="" />
       <option name="PARENT_ENVS" value="true" />
       <envs>
@@ -231,7 +261,7 @@
       <option name="ADD_CONTENT_ROOTS" value="true" />
       <option name="ADD_SOURCE_ROOTS" value="true" />
       <EXTENSION ID="PythonCoverageRunConfigurationExtension" runner="coverage.py" />
-      <option name="SCRIPT_NAME" value="$PROJECT_DIR$/AIIrrigationGameV3.py" />
+      <option name="SCRIPT_NAME" value="$PROJECT_DIR$/LoggerSetups.py" />
       <option name="PARAMETERS" value="" />
       <option name="SHOW_COMMAND_LINE" value="false" />
       <option name="EMULATE_TERMINAL" value="false" />
@@ -240,8 +270,9 @@
       <option name="INPUT_FILE" value="" />
       <method v="2" />
     </configuration>
-    <configuration name="Decagon" type="PythonConfigurationType" factoryName="Python" temporary="true" nameIsGenerated="true">
+    <configuration name="PrerunMaintenance" type="PythonConfigurationType" factoryName="Python" temporary="true" nameIsGenerated="true">
       <module name="Stomato" />
+      <option name="ENV_FILES" value="" />
       <option name="INTERPRETER_OPTIONS" value="" />
       <option name="PARENT_ENVS" value="true" />
       <envs>
@@ -253,7 +284,7 @@
       <option name="ADD_CONTENT_ROOTS" value="true" />
       <option name="ADD_SOURCE_ROOTS" value="true" />
       <EXTENSION ID="PythonCoverageRunConfigurationExtension" runner="coverage.py" />
-      <option name="SCRIPT_NAME" value="$PROJECT_DIR$/Decagon.py" />
+      <option name="SCRIPT_NAME" value="$PROJECT_DIR$/PrerunMaintenance.py" />
       <option name="PARAMETERS" value="" />
       <option name="SHOW_COMMAND_LINE" value="false" />
       <option name="EMULATE_TERMINAL" value="false" />
@@ -262,8 +293,9 @@
       <option name="INPUT_FILE" value="" />
       <method v="2" />
     </configuration>
-    <configuration name="HeatUnits" type="PythonConfigurationType" factoryName="Python" temporary="true" nameIsGenerated="true">
+    <configuration name="SQLScripts" type="PythonConfigurationType" factoryName="Python" temporary="true" nameIsGenerated="true">
       <module name="Stomato" />
+      <option name="ENV_FILES" value="" />
       <option name="INTERPRETER_OPTIONS" value="" />
       <option name="PARENT_ENVS" value="true" />
       <envs>
@@ -275,7 +307,7 @@
       <option name="ADD_CONTENT_ROOTS" value="true" />
       <option name="ADD_SOURCE_ROOTS" value="true" />
       <EXTENSION ID="PythonCoverageRunConfigurationExtension" runner="coverage.py" />
-      <option name="SCRIPT_NAME" value="$PROJECT_DIR$/HeatUnits.py" />
+      <option name="SCRIPT_NAME" value="$PROJECT_DIR$/SQLScripts.py" />
       <option name="PARAMETERS" value="" />
       <option name="SHOW_COMMAND_LINE" value="false" />
       <option name="EMULATE_TERMINAL" value="false" />
@@ -286,6 +318,7 @@
     </configuration>
     <configuration default="true" type="tests" factoryName="Nosetests">
       <module name="Stomato" />
+      <option name="ENV_FILES" value="" />
       <option name="INTERPRETER_OPTIONS" value="" />
       <option name="PARENT_ENVS" value="true" />
       <option name="SDK_HOME" value="" />
@@ -302,6 +335,7 @@
     </configuration>
     <configuration default="true" type="tests" factoryName="Unittests">
       <module name="Stomato" />
+      <option name="ENV_FILES" value="" />
       <option name="INTERPRETER_OPTIONS" value="" />
       <option name="PARENT_ENVS" value="true" />
       <option name="SDK_HOME" value="" />
@@ -317,6 +351,7 @@
     </configuration>
     <configuration default="true" type="tests" factoryName="py.test">
       <module name="Stomato" />
+      <option name="ENV_FILES" value="" />
       <option name="INTERPRETER_OPTIONS" value="" />
       <option name="PARENT_ENVS" value="true" />
       <option name="SDK_HOME" value="" />
@@ -333,21 +368,29 @@
       <method v="2" />
     </configuration>
     <list>
-      <item itemvalue="Python.HeatUnits" />
-      <item itemvalue="Python.AIIrrigationGameV3 (2)" />
-      <item itemvalue="Python.AIIrrigationGameV3 (3)" />
-      <item itemvalue="Python.AIIrrigationGameV3 (4)" />
+      <item itemvalue="Python.LoggerSetups" />
+      <item itemvalue="Python.KMLHandler" />
+      <item itemvalue="Python.PrerunMaintenance" />
+      <item itemvalue="Python.SQLScripts" />
       <item itemvalue="Python.Decagon" />
     </list>
     <recent_temporary>
       <list>
         <item itemvalue="Python.Decagon" />
-        <item itemvalue="Python.HeatUnits" />
-        <item itemvalue="Python.AIIrrigationGameV3 (4)" />
-        <item itemvalue="Python.AIIrrigationGameV3 (3)" />
-        <item itemvalue="Python.AIIrrigationGameV3 (2)" />
+        <item itemvalue="Python.SQLScripts" />
+        <item itemvalue="Python.LoggerSetups" />
+        <item itemvalue="Python.KMLHandler" />
+        <item itemvalue="Python.PrerunMaintenance" />
       </list>
     </recent_temporary>
+  </component>
+  <component name="SharedIndexes">
+    <attachedChunks>
+      <set>
+        <option value="bundled-js-predefined-1d06a55b98c1-0b3e54e931b4-JavaScript-PY-241.17890.14" />
+        <option value="bundled-python-sdk-5b207ade9991-7e9c3bbb6e34-com.jetbrains.pycharm.pro.sharedIndexes.bundled-PY-241.17890.14" />
+      </set>
+    </attachedChunks>
   </component>
   <component name="SpellCheckerSettings" RuntimeDictionaries="0" Folders="0" CustomDictionaries="0" DefaultDictionary="project-level" UseSingleDictionary="true" transferred="true" />
   <component name="SvnConfiguration">
@@ -483,6 +526,7 @@
       <workItem from="1686693323310" duration="23345000" />
       <workItem from="1686855364679" duration="18047000" />
       <workItem from="1686875965074" duration="6029000" />
+      <workItem from="1720206561606" duration="43043000" />
     </task>
     <task id="LOCAL-00297" summary="Fixed Emails&#10;&#10;Emailing wasn't working past the first recipient. Fixed it now">
       <created>1681780581866</created>
@@ -827,7 +871,7 @@
       <option name="project" value="LOCAL" />
       <updated>1686858169672</updated>
     </task>
-    <option name="localTasksCounter" value="346" />
+    <option name="localTasksCounter" value="347" />
     <servers />
   </component>
   <component name="TodoView">
@@ -865,6 +909,18 @@
   <component name="TypeScriptGeneratedFilesManager">
     <option name="version" value="3" />
   </component>
+  <component name="Vcs.Log.History.Properties">
+    <option name="COLUMN_ID_ORDER">
+      <list>
+        <option value="Default.Root" />
+        <option value="Default.Author" />
+        <option value="Default.Date" />
+        <option value="Default.Subject" />
+        <option value="Space.CommitStatus" />
+        <option value="GitHub.CommitStatus" />
+      </list>
+    </option>
+  </component>
   <component name="Vcs.Log.Tabs.Properties">
     <option name="RECENT_FILTERS">
       <map>
@@ -910,7 +966,6 @@
   </component>
   <component name="VcsManagerConfiguration">
     <option name="ADD_EXTERNAL_FILES_SILENTLY" value="true" />
-    <MESSAGE value="Field Type&#10;&#10;Added field_type to Field object to track if the field is a demo. &#10;field_type not holds a string that can be 'Commercial', 'Demo', or 'R&amp;D' depending on what the field is.&#10;Modified setup_field() in Decagon to incorporate new optional parameter for field_type.&#10;Modified Field objet init to have the new optional parameter field_type&#10;Added consec_psis to Logger to_string().&#10;Changed logger usage of rnd boolean to check Field.field_type instead." />
     <MESSAGE value="Notification updates&#10;&#10;Updated notifications for vwc to use the new soil class and the thresholds defined in there for the specific soil type.&#10;Updated canopy temp notifications.&#10;Added new PSI notifications for high thresholds and for when an IR should be turned on but hasn't been activated yet by our system." />
     <MESSAGE value="Formatting" />
     <MESSAGE value="Changes to Decagon setup_ai_game_data()&#10;&#10;Simplified setting up new data for AI game by just passing along the logger instead of individual data points" />
@@ -935,7 +990,8 @@
     <MESSAGE value="BS" />
     <MESSAGE value="Started HeatUnits.py&#10;&#10;This file will be used to do the processing work for the Heat Units trials this year 2023.&#10;Started by setting up Logger instances for each of the current Weather Stations we have installed.&#10;Setup download of data for each into its on directory." />
     <MESSAGE value="VP4 Notification Changes&#10;&#10;Simplified vp4 notifications into 1 single method to handle air temp, rh, and vpd notifications. This allows us to just create a single notification if any of those are None instead of one for each type since it is almost never the case that just 1 fails.&#10;New method is called vp4_notifications.&#10;Removed old individual methods for air temp notifications, rh notifications, and vpd notifications.&#10;Formatting changes." />
-    <option name="LAST_COMMIT_MESSAGE" value="VP4 Notification Changes&#10;&#10;Simplified vp4 notifications into 1 single method to handle air temp, rh, and vpd notifications. This allows us to just create a single notification if any of those are None instead of one for each type since it is almost never the case that just 1 fails.&#10;New method is called vp4_notifications.&#10;Removed old individual methods for air temp notifications, rh notifications, and vpd notifications.&#10;Formatting changes." />
+    <MESSAGE value="Soil Type Flow Fix&#10;&#10;In Logger Setups if there is a soil type submitted we use that instead of calling the soil survey, else use the soil survey" />
+    <option name="LAST_COMMIT_MESSAGE" value="Soil Type Flow Fix&#10;&#10;In Logger Setups if there is a soil type submitted we use that instead of calling the soil survey, else use the soil survey" />
     <option name="OPTIMIZE_IMPORTS_BEFORE_PROJECT_COMMIT" value="true" />
   </component>
   <component name="XDebuggerManager">
@@ -945,11 +1001,6 @@
           <url>file://$PROJECT_DIR$/WeatherTest.py</url>
           <line>233</line>
           <option name="timeStamp" value="66" />
-        </line-breakpoint>
-        <line-breakpoint enabled="true" suspend="THREAD" type="python-line">
-          <url>file://$PROJECT_DIR$/Decagon.py</url>
-          <line>3367</line>
-          <option name="timeStamp" value="109" />
         </line-breakpoint>
         <line-breakpoint enabled="true" suspend="THREAD" type="python-line">
           <url>file://$PROJECT_DIR$/CIMIS.py</url>
@@ -958,12 +1009,7 @@
         </line-breakpoint>
         <line-breakpoint enabled="true" suspend="THREAD" type="python-line">
           <url>file://$PROJECT_DIR$/Decagon.py</url>
-          <line>3324</line>
-          <option name="timeStamp" value="144" />
-        </line-breakpoint>
-        <line-breakpoint enabled="true" suspend="THREAD" type="python-line">
-          <url>file://$PROJECT_DIR$/Decagon.py</url>
-          <line>3287</line>
+          <line>3288</line>
           <option name="timeStamp" value="147" />
         </line-breakpoint>
         <line-breakpoint enabled="true" suspend="THREAD" type="python-line">
@@ -978,7 +1024,7 @@
         </line-breakpoint>
         <line-breakpoint enabled="true" suspend="THREAD" type="python-line">
           <url>file://$PROJECT_DIR$/Decagon.py</url>
-          <line>2927</line>
+          <line>2928</line>
           <option name="timeStamp" value="205" />
         </line-breakpoint>
         <line-breakpoint enabled="true" suspend="THREAD" type="python-line">
@@ -998,7 +1044,7 @@
         </line-breakpoint>
         <line-breakpoint enabled="true" suspend="THREAD" type="python-line">
           <url>file://$PROJECT_DIR$/Decagon.py</url>
-          <line>549</line>
+          <line>550</line>
           <option name="timeStamp" value="234" />
         </line-breakpoint>
         <line-breakpoint enabled="true" suspend="THREAD" type="python-line">
@@ -1025,15 +1071,10 @@
           <url>file://$PROJECT_DIR$/YearAnalysis.py</url>
           <line>1507</line>
           <option name="timeStamp" value="249" />
-        </line-breakpoint>
-        <line-breakpoint enabled="true" suspend="THREAD" type="python-line">
-          <url>file://$PROJECT_DIR$/Decagon.py</url>
-          <line>3328</line>
-          <option name="timeStamp" value="251" />
         </line-breakpoint>
         <line-breakpoint enabled="true" suspend="THREAD" type="python-line">
           <url>file://$PROJECT_DIR$/Logger.py</url>
-          <line>790</line>
+          <line>759</line>
           <option name="timeStamp" value="259" />
         </line-breakpoint>
         <line-breakpoint enabled="true" suspend="THREAD" type="python-line">
@@ -1051,6 +1092,36 @@
           <line>13</line>
           <option name="timeStamp" value="267" />
         </line-breakpoint>
+        <line-breakpoint enabled="true" suspend="THREAD" type="python-line">
+          <url>file://$PROJECT_DIR$/SlackBot.py</url>
+          <line>130</line>
+          <option name="timeStamp" value="351" />
+        </line-breakpoint>
+        <line-breakpoint enabled="true" suspend="THREAD" type="python-line">
+          <url>file://$PROJECT_DIR$/Logger.py</url>
+          <line>2038</line>
+          <option name="timeStamp" value="352" />
+        </line-breakpoint>
+        <line-breakpoint enabled="true" suspend="THREAD" type="python-line">
+          <url>file://$PROJECT_DIR$/LoggerSetups.py</url>
+          <line>652</line>
+          <option name="timeStamp" value="353" />
+        </line-breakpoint>
+        <line-breakpoint enabled="true" suspend="THREAD" type="python-line">
+          <url>file://$PROJECT_DIR$/LoggerSetups.py</url>
+          <line>966</line>
+          <option name="timeStamp" value="354" />
+        </line-breakpoint>
+        <line-breakpoint enabled="true" suspend="THREAD" type="python-line">
+          <url>file://$PROJECT_DIR$/LoggerSetups.py</url>
+          <line>1183</line>
+          <option name="timeStamp" value="355" />
+        </line-breakpoint>
+        <line-breakpoint enabled="true" suspend="THREAD" type="python-line">
+          <url>file://$PROJECT_DIR$/Logger.py</url>
+          <line>1372</line>
+          <option name="timeStamp" value="356" />
+        </line-breakpoint>
       </breakpoints>
       <default-breakpoints>
         <breakpoint type="python-exception">
@@ -1060,61 +1131,73 @@
         </breakpoint>
       </default-breakpoints>
     </breakpoint-manager>
+    <watches-manager>
+      <configuration name="PythonConfigurationType">
+        <watch expression="field.loggers[-1]" language="Python" />
+      </configuration>
+    </watches-manager>
   </component>
   <component name="com.intellij.coverage.CoverageDataManagerImpl">
-    <SUITE FILE_PATH="coverage/S_TOMAto$TKInter.coverage" NAME="TKInter Coverage Results" MODIFIED="1631131790797" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
-    <SUITE FILE_PATH="coverage/Stomato$SwitchTestCase.coverage" NAME="SwitchTestCase Coverage Results" MODIFIED="1678730749480" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
-    <SUITE FILE_PATH="coverage/Stomato$DBWriter.coverage" NAME="DBWriter Coverage Results" MODIFIED="1662592976330" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
-    <SUITE FILE_PATH="coverage/S_TOMAto$CwsiFormulaTester.coverage" NAME="CwsiFormulaTester Coverage Results" MODIFIED="1630459322067" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
-    <SUITE FILE_PATH="coverage/S_TOMAto$STOMAtoUpdate.coverage" NAME="STOMAtoUpdate Coverage Results" MODIFIED="1642038156945" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
-    <SUITE FILE_PATH="coverage/Stomato$SQLScripts.coverage" NAME="SQLScripts Coverage Results" MODIFIED="1660142569019" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
-    <SUITE FILE_PATH="coverage/S_TOMAto$formatHistoricalET.coverage" NAME="formatHistoricalET Coverage Results" MODIFIED="1620265637963" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
-    <SUITE FILE_PATH="coverage/Stomato$AIIrrigationGameV3__3_.coverage" NAME="AIIrrigationGameV3 (3) Coverage Results" MODIFIED="1686323145119" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$/AIGame" />
-    <SUITE FILE_PATH="coverage/Stomato$Soils.coverage" NAME="Soils Coverage Results" MODIFIED="1683932387848" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
+    <SUITE FILE_PATH="coverage/Stomato$TKInter.coverage" NAME="TKInter Coverage Results" MODIFIED="1678243631453" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
+    <SUITE FILE_PATH="coverage/Stomato$SQLScripts.coverage" NAME="SQLScripts Coverage Results" MODIFIED="1720741184810" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="false" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
+    <SUITE FILE_PATH="coverage/S_TOMAto$CimisUpdate.coverage" NAME="CimisUpdate Coverage Results" MODIFIED="1621340772933" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
+    <SUITE FILE_PATH="coverage/S_TOMAto$CwsiProcessor.coverage" NAME="CwsiProcessor Coverage Results" MODIFIED="1619518007322" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
+    <SUITE FILE_PATH="coverage/Stomato$LoggerSetups.coverage" NAME="LoggerSetups Coverage Results" MODIFIED="1720739061158" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="false" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
     <SUITE FILE_PATH="coverage/Stomato$formatHistoricalET.coverage" NAME="formatHistoricalET Coverage Results" MODIFIED="1615220646151" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
-    <SUITE FILE_PATH="coverage/Stomato$PrerunMaintenance.coverage" NAME="PrerunMaintenance Coverage Results" MODIFIED="1654037151077" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
-    <SUITE FILE_PATH="coverage/Stomato$gmailTesting.coverage" NAME="gmailTesting Coverage Results" MODIFIED="1655757137703" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
+    <SUITE FILE_PATH="coverage/Stomato$AIIrrigationGameV3.coverage" NAME="AIIrrigationGameV3 Coverage Results" MODIFIED="1686272739977" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
+    <SUITE FILE_PATH="coverage/Stomato$technicianPortal.coverage" NAME="technicianPortal Coverage Results" MODIFIED="1670450349216" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
+    <SUITE FILE_PATH="coverage/S_TOMAto$Field.coverage" NAME="Field Coverage Results" MODIFIED="1618891943834" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
+    <SUITE FILE_PATH="coverage/Stomato$STOMAtoUpdateRetry.coverage" NAME="STOMAtoUpdateRetry Coverage Results" MODIFIED="1659446223871" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
+    <SUITE FILE_PATH="coverage/Stomato$SoilAPI2.coverage" NAME="SoilAPI2 Coverage Results" MODIFIED="1719514996646" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="false" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
+    <SUITE FILE_PATH="coverage/Stomato$KMLHandler.coverage" NAME="KMLHandler Coverage Results" MODIFIED="1720475798247" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="false" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
     <SUITE FILE_PATH="coverage/Stomato$uninstallFields.coverage" NAME="uninstallFields Coverage Results" MODIFIED="1666393598857" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
-    <SUITE FILE_PATH="coverage/Stomato$sqlTesting.coverage" NAME="sqlTesting Coverage Results" MODIFIED="1612901497912" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
-    <SUITE FILE_PATH="coverage/Stomato$TKInter.coverage" NAME="TKInter Coverage Results" MODIFIED="1678243631453" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
-    <SUITE FILE_PATH="coverage/S_TOMAto$STOMAtoUpdateRetry.coverage" NAME="STOMAtoUpdateRetry Coverage Results" MODIFIED="1632331197894" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
+    <SUITE FILE_PATH="coverage/Stomato$DBWriter.coverage" NAME="DBWriter Coverage Results" MODIFIED="1662592976330" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
+    <SUITE FILE_PATH="coverage/S_TOMAto$loggerSetups.coverage" NAME="loggerSetups Coverage Results" MODIFIED="1639095975188" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
+    <SUITE FILE_PATH="coverage/Stomato$PrerunMaintenance.coverage" NAME="PrerunMaintenance Coverage Results" MODIFIED="1720472579205" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="false" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
+    <SUITE FILE_PATH="coverage/Stomato$Decagon.coverage" NAME="Decagon Coverage Results" MODIFIED="1720741741311" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="false" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
+    <SUITE FILE_PATH="coverage/Stomato$AIIrrigationGameV3__4_.coverage" NAME="AIIrrigationGameV3 (4) Coverage Results" MODIFIED="1686334569719" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
+    <SUITE FILE_PATH="coverage/Stomato$loggerSetups.coverage" NAME="loggerSetups Coverage Results" MODIFIED="1679606405842" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
+    <SUITE FILE_PATH="coverage/Stomato$gSheetTest.coverage" NAME="gSheetTest Coverage Results" MODIFIED="1609745391990" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
+    <SUITE FILE_PATH="coverage/Stomato$Soils.coverage" NAME="Soils Coverage Results" MODIFIED="1683932387848" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
+    <SUITE FILE_PATH="coverage/Stomato$AIIIrrigationGameV2.coverage" NAME="AIIIrrigationGameV2 Coverage Results" MODIFIED="1684454590969" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
+    <SUITE FILE_PATH="coverage/S_TOMAto$CwsiFormulaTester.coverage" NAME="CwsiFormulaTester Coverage Results" MODIFIED="1630459322067" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
+    <SUITE FILE_PATH="coverage/S_TOMAto$Decagon.coverage" NAME="Decagon Coverage Results" MODIFIED="1687765303466" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
     <SUITE FILE_PATH="coverage/S_TOMAto$DBWriter.coverage" NAME="DBWriter Coverage Results" MODIFIED="1621249399322" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
-    <SUITE FILE_PATH="coverage/Stomato$HeatUnits.coverage" NAME="HeatUnits Coverage Results" MODIFIED="1686881280835" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
+    <SUITE FILE_PATH="coverage/Stomato$AIIrrigationGameV3__1_.coverage" NAME="AIIrrigationGameV3 (1) Coverage Results" MODIFIED="1686272827720" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$/AI" />
+    <SUITE FILE_PATH="coverage/Stomato$UninstallFields.coverage" NAME="UninstallFields Coverage Results" MODIFIED="1719519450705" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="false" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
     <SUITE FILE_PATH="coverage/S_TOMAto$.coverage" NAME=" Coverage Results" MODIFIED="1634172899963" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
-    <SUITE FILE_PATH="coverage/S_TOMAto$CwsiProcessor.coverage" NAME="CwsiProcessor Coverage Results" MODIFIED="1619518007322" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
-    <SUITE FILE_PATH="coverage/Stomato$AIIrrigationGameV3__2_.coverage" NAME="AIIrrigationGameV3 (2) Coverage Results" MODIFIED="1686273006519" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$/AI Game" />
-    <SUITE FILE_PATH="coverage/Stomato$EmailTester.coverage" NAME="EmailTester Coverage Results" MODIFIED="1681779541242" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
-    <SUITE FILE_PATH="coverage/Stomato$loggerSetups.coverage" NAME="loggerSetups Coverage Results" MODIFIED="1679606405842" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
-    <SUITE FILE_PATH="coverage/S_TOMAto$loggerSetups.coverage" NAME="loggerSetups Coverage Results" MODIFIED="1639095975188" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
     <SUITE FILE_PATH="coverage/Stomato$AIExpertSystemTesting.coverage" NAME="AIExpertSystemTesting Coverage Results" MODIFIED="1683677065044" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
-    <SUITE FILE_PATH="coverage/S_TOMAto$CimisUpdate.coverage" NAME="CimisUpdate Coverage Results" MODIFIED="1621340772933" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
-    <SUITE FILE_PATH="coverage/S_TOMAto$Decagon.coverage" NAME="Decagon Coverage Results" MODIFIED="1687765303466" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
-    <SUITE FILE_PATH="coverage/Stomato$gSheetTest.coverage" NAME="gSheetTest Coverage Results" MODIFIED="1609745391990" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
-    <SUITE FILE_PATH="coverage/Stomato$WeatherTest.coverage" NAME="WeatherTest Coverage Results" MODIFIED="1678321995221" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
     <SUITE FILE_PATH="coverage/Stomato$AIGameData.coverage" NAME="AIGameData Coverage Results" MODIFIED="1681321697188" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
+    <SUITE FILE_PATH="coverage/Stomato$AIIrrigationGameV3__3_.coverage" NAME="AIIrrigationGameV3 (3) Coverage Results" MODIFIED="1686323145119" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$/AIGame" />
+    <SUITE FILE_PATH="coverage/S_TOMAto$formatHistoricalET.coverage" NAME="formatHistoricalET Coverage Results" MODIFIED="1620265637963" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
+    <SUITE FILE_PATH="coverage/Stomato$EmailTester.coverage" NAME="EmailTester Coverage Results" MODIFIED="1681779541242" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
+    <SUITE FILE_PATH="coverage/Stomato$CimisUpdate.coverage" NAME="CimisUpdate Coverage Results" MODIFIED="1719431661442" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="false" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
+    <SUITE FILE_PATH="coverage/Stomato$schedulerTest.coverage" NAME="schedulerTest Coverage Results" MODIFIED="1659650548778" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
+    <SUITE FILE_PATH="coverage/Stomato$sqlTesting.coverage" NAME="sqlTesting Coverage Results" MODIFIED="1612901497912" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
     <SUITE FILE_PATH="coverage/S_TOMAto$AIGameData.coverage" NAME="AIGameData Coverage Results" MODIFIED="1632877501080" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
-    <SUITE FILE_PATH="coverage/Stomato$AIIrrigationGameV3.coverage" NAME="AIIrrigationGameV3 Coverage Results" MODIFIED="1686272739977" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
+    <SUITE FILE_PATH="coverage/Stomato$SoilAPI.coverage" NAME="SoilAPI Coverage Results" MODIFIED="1719516361157" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="false" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
+    <SUITE FILE_PATH="coverage/Stomato$SwitchTestCase.coverage" NAME="SwitchTestCase Coverage Results" MODIFIED="1678730749480" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
+    <SUITE FILE_PATH="coverage/Stomato_dev$Decagon.coverage" NAME="Decagon Coverage Results" MODIFIED="1611789454686" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
+    <SUITE FILE_PATH="coverage/Stomato$WeatherTest.coverage" NAME="WeatherTest Coverage Results" MODIFIED="1678321995221" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
+    <SUITE FILE_PATH="coverage/S_TOMAto$STOMAtoUpdate.coverage" NAME="STOMAtoUpdate Coverage Results" MODIFIED="1642038156945" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
+    <SUITE FILE_PATH="coverage/S_TOMAto$SQLScripts.coverage" NAME="SQLScripts Coverage Results" MODIFIED="1642230710365" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
+    <SUITE FILE_PATH="coverage/Stomato$SlackBot.coverage" NAME="SlackBot Coverage Results" MODIFIED="1720064187734" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="false" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
+    <SUITE FILE_PATH="coverage/Stomato$STOMAtoUpdate.coverage" NAME="STOMAtoUpdate Coverage Results" MODIFIED="1643531784052" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
     <SUITE FILE_PATH="coverage/S_TOMAto$AIExpertSystemTesting.coverage" NAME="AIExpertSystemTesting Coverage Results" MODIFIED="1636765308468" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
-    <SUITE FILE_PATH="coverage/Stomato$AIIrrigationGameV3__1_.coverage" NAME="AIIrrigationGameV3 (1) Coverage Results" MODIFIED="1686272827720" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$/AI" />
-    <SUITE FILE_PATH="coverage/Stomato$Decagon.coverage" NAME="Decagon Coverage Results" MODIFIED="1683932813529" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
-    <SUITE FILE_PATH="coverage/S_TOMAto$Field.coverage" NAME="Field Coverage Results" MODIFIED="1618891943834" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
-    <SUITE FILE_PATH="coverage/Stomato$CimisUpdate.coverage" NAME="CimisUpdate Coverage Results" MODIFIED="1660869984017" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
-    <SUITE FILE_PATH="coverage/Stomato$STOMAtoUpdateRetry.coverage" NAME="STOMAtoUpdateRetry Coverage Results" MODIFIED="1659446223871" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
+    <SUITE FILE_PATH="coverage/S_TOMAto$CIMIS.coverage" NAME="CIMIS Coverage Results" MODIFIED="1618829670273" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
+    <SUITE FILE_PATH="coverage/Stomato$AIIrrigationGameV3__2_.coverage" NAME="AIIrrigationGameV3 (2) Coverage Results" MODIFIED="1686273006519" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$/AI Game" />
+    <SUITE FILE_PATH="coverage/Stomato$HeatUnits.coverage" NAME="HeatUnits Coverage Results" MODIFIED="1686881280835" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
+    <SUITE FILE_PATH="coverage/Stomato$Logger.coverage" NAME="Logger Coverage Results" MODIFIED="1719429829598" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="false" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
+    <SUITE FILE_PATH="coverage/Stomato$DataTests.coverage" NAME="DataTests Coverage Results" MODIFIED="1679072769429" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
     <SUITE FILE_PATH="coverage/Stomato$CimisStation.coverage" NAME="CimisStation Coverage Results" MODIFIED="1657387377095" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
-    <SUITE FILE_PATH="coverage/S_TOMAto$SQLScripts.coverage" NAME="SQLScripts Coverage Results" MODIFIED="1642230710365" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
-    <SUITE FILE_PATH="coverage/Stomato$EmailProcessor.coverage" NAME="EmailProcessor Coverage Results" MODIFIED="1655838431141" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
-    <SUITE FILE_PATH="coverage/Stomato$KivyTesting.coverage" NAME="KivyTesting Coverage Results" MODIFIED="1685580311134" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
-    <SUITE FILE_PATH="coverage/Stomato_dev$Decagon.coverage" NAME="Decagon Coverage Results" MODIFIED="1611789454686" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
+    <SUITE FILE_PATH="coverage/S_TOMAto$TKInter.coverage" NAME="TKInter Coverage Results" MODIFIED="1631131790797" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
     <SUITE FILE_PATH="coverage/Stomato$bigQueryTest.coverage" NAME="bigQueryTest Coverage Results" MODIFIED="1608572673786" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
-    <SUITE FILE_PATH="coverage/Stomato$DataTests.coverage" NAME="DataTests Coverage Results" MODIFIED="1679072769429" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
     <SUITE FILE_PATH="coverage/Stomato$YearAnalysis.coverage" NAME="YearAnalysis Coverage Results" MODIFIED="1683672160830" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
+    <SUITE FILE_PATH="coverage/Stomato$gmailTesting.coverage" NAME="gmailTesting Coverage Results" MODIFIED="1655757137703" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
+    <SUITE FILE_PATH="coverage/S_TOMAto$STOMAtoUpdateRetry.coverage" NAME="STOMAtoUpdateRetry Coverage Results" MODIFIED="1632331197894" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
+    <SUITE FILE_PATH="coverage/Stomato$EmailProcessor.coverage" NAME="EmailProcessor Coverage Results" MODIFIED="1655838431141" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
     <SUITE FILE_PATH="coverage/S_TOMAto$Testing.coverage" NAME="Testing Coverage Results" MODIFIED="1628198021719" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
-    <SUITE FILE_PATH="coverage/Stomato$AIIIrrigationGameV2.coverage" NAME="AIIIrrigationGameV2 Coverage Results" MODIFIED="1684454590969" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
-    <SUITE FILE_PATH="coverage/Stomato$AIIrrigationGameV3__4_.coverage" NAME="AIIrrigationGameV3 (4) Coverage Results" MODIFIED="1686334569719" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
-    <SUITE FILE_PATH="coverage/Stomato$technicianPortal.coverage" NAME="technicianPortal Coverage Results" MODIFIED="1670450349216" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
-    <SUITE FILE_PATH="coverage/Stomato$schedulerTest.coverage" NAME="schedulerTest Coverage Results" MODIFIED="1659650548778" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
-    <SUITE FILE_PATH="coverage/S_TOMAto$CIMIS.coverage" NAME="CIMIS Coverage Results" MODIFIED="1618829670273" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
-    <SUITE FILE_PATH="coverage/Stomato$STOMAtoUpdate.coverage" NAME="STOMAtoUpdate Coverage Results" MODIFIED="1643531784052" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
+    <SUITE FILE_PATH="coverage/Stomato$KivyTesting.coverage" NAME="KivyTesting Coverage Results" MODIFIED="1685580311134" SOURCE_PROVIDER="com.intellij.coverage.DefaultCoverageFileProvider" RUNNER="coverage.py" COVERAGE_BY_TEST_ENABLED="true" COVERAGE_TRACING_ENABLED="false" WORKING_DIRECTORY="$PROJECT_DIR$" />
   </component>
   <component name="masterDetails">
     <states>
Index: data.csv
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>logger_id,date,time,canopy_temperature,ambient_temperature,vpd,vwc_1,vwc_2,vwc_3,field_capacity,wilting_point,daily_gallons,daily_switch,daily_hours,daily_pressure,daily_inches,psi,psi_threshold,psi_critical,sdd,rh,eto,kc,etc,et_hours,phase1_adjustment,phase1_adjusted,phase2_adjustment,phase2_adjusted,phase3_adjustment,phase3_adjusted,vwc_1_ec,vwc_2_ec,vwc_3_ec,lowest_ambient_temperature,gdd,crop_stage,id,planting_date,variety\r\nz6-07156,2023-06-24,06:00 PM,,84.70400000000001,2.878745920483505,19.854270349879698,18.0140852962003,21.56450467022204,12,5,,409.0,6.8,,0.3,,1.6,2.2,,29.264726183655803,,0.528,,,,,,,,,,,,57.2,20.951999999999998,Bloom,be0dc883-bb70-4d9e-afba-483840f7f548,2023-06-15,\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/data.csv b/data.csv
--- a/data.csv	(revision 5cf7d78a1d94023d00661edf5517badacc064130)
+++ b/data.csv	(date 1720474292950)
@@ -1,2 +1,1 @@
-logger_id,date,time,canopy_temperature,ambient_temperature,vpd,vwc_1,vwc_2,vwc_3,field_capacity,wilting_point,daily_gallons,daily_switch,daily_hours,daily_pressure,daily_inches,psi,psi_threshold,psi_critical,sdd,rh,eto,kc,etc,et_hours,phase1_adjustment,phase1_adjusted,phase2_adjustment,phase2_adjusted,phase3_adjustment,phase3_adjusted,vwc_1_ec,vwc_2_ec,vwc_3_ec,lowest_ambient_temperature,gdd,crop_stage,id,planting_date,variety
-z6-07156,2023-06-24,06:00 PM,,84.70400000000001,2.878745920483505,19.854270349879698,18.0140852962003,21.56450467022204,12,5,,409.0,6.8,,0.3,,1.6,2.2,,29.264726183655803,,0.528,,,,,,,,,,,,57.2,20.951999999999998,Bloom,be0dc883-bb70-4d9e-afba-483840f7f548,2023-06-15,
+logger_id,date,time,canopy_temperature,canopy_temperature_celsius,ambient_temperature,ambient_temperature_celsius,vpd,vwc_1,vwc_2,vwc_3,field_capacity,wilting_point,daily_gallons,daily_switch,daily_hours,daily_pressure,daily_inches,psi,psi_threshold,psi_critical,sdd,sdd_celsius,rh,eto,kc,etc,et_hours,phase1_adjustment,phase1_adjusted,phase2_adjustment,phase2_adjusted,phase3_adjustment,phase3_adjusted,vwc_1_ec,vwc_2_ec,vwc_3_ec,lowest_ambient_temperature,lowest_ambient_temperature_celsius,gdd,crop_stage,id,planting_date,variety
Index: PickleHandler.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>from os import path\r\nimport pickle\r\nfrom datetime import datetime\r\nfrom shutil import copyfile\r\n\r\nDIRECTORY_YEAR = \"2024\"\r\nPICKLE_DIRECTORY = \"H:\\\\Shared drives\\\\Stomato\\\\\" + DIRECTORY_YEAR + \"\\\\Pickle\\\\\"\r\nBACKUP_PICKLE_DIRECTORY = \"H:\\\\Shared drives\\\\Stomato\\\\\" + DIRECTORY_YEAR + \"\\\\Pickle\\\\Backup\\\\\"\r\nPICKLE_NAME = DIRECTORY_YEAR + \"_pickle.pickle\"\r\nPICKLE_PATH = PICKLE_DIRECTORY + PICKLE_NAME\r\n\r\nNOTIFICATIONS_DIRECTORY = \"H:\\\\Shared drives\\\\Stomato\\\\\" + DIRECTORY_YEAR + \"\\\\Notifications\"\r\n\r\n\r\ndef open_pickle(filename: str = PICKLE_NAME, specific_file_path: str = PICKLE_DIRECTORY):\r\n    \"\"\"\r\n    Function to open a pickle and return its contents.\r\n\r\n    :return:\r\n        List fields\r\n    \"\"\"\r\n    print(\"Opening pickle\")\r\n    if path.exists(specific_file_path + filename):\r\n        with open(specific_file_path + filename, 'rb') as f:\r\n            content = pickle.load(f)\r\n        return content\r\n\r\n\r\ndef write_pickle(data, filename: str = PICKLE_NAME, specific_file_path: str = PICKLE_DIRECTORY):\r\n    \"\"\"\r\n    Function to write to a pickle.\r\n\r\n    A pickle is a form of permanent storage used to store any data structure. In this case, it's storing\r\n    the list of fields.\r\n\r\n    :param specific_file_path:\r\n    :param filename:\r\n    :param data: List that you want to have writen\r\n    :return:\r\n    \"\"\"\r\n\r\n    # Backup the old pickle before writing to it\r\n    # backup_pickle()\r\n\r\n    if path.exists(specific_file_path):\r\n        with open(specific_file_path + filename, 'wb') as f:\r\n            pickle.dump(data, f)\r\n\r\n\r\ndef backup_pickle(specific_name=None):\r\n    \"\"\"\r\n\r\n    :return:\r\n    \"\"\"\r\n    now = datetime.today()\r\n    if specific_name is not None:\r\n        file_name = specific_name + \"_pickle_backup_\" + str(now.strftime(\"%m-%d-%y  %I_%M_%S %p\")) + \".pickle\"\r\n    else:\r\n        file_name = \"pickle_backup_\" + str(now.strftime(\"%m-%d-%y  %I_%M_%S %p\")) + \".pickle\"\r\n\r\n    print('Backing up Pickle...')\r\n\r\n    # Check if the pickle we want to copy exists and if it does, copy it\r\n    if path.exists(PICKLE_PATH):\r\n        copyfile(\r\n            PICKLE_PATH,\r\n            BACKUP_PICKLE_DIRECTORY + file_name\r\n        )\r\n\r\n    print('Pickle Backed Up - ', file_name)\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/PickleHandler.py b/PickleHandler.py
--- a/PickleHandler.py	(revision 5cf7d78a1d94023d00661edf5517badacc064130)
+++ b/PickleHandler.py	(date 1719875970017)
@@ -2,6 +2,8 @@
 import pickle
 from datetime import datetime
 from shutil import copyfile
+from Field import Field
+from Grower import Grower
 
 DIRECTORY_YEAR = "2024"
 PICKLE_DIRECTORY = "H:\\Shared drives\\Stomato\\" + DIRECTORY_YEAR + "\\Pickle\\"
@@ -68,3 +70,35 @@
         )
 
     print('Pickle Backed Up - ', file_name)
+
+def get_grower(grower_name: str) -> Grower:
+    """
+    Function to get a grower object from the pickle
+
+    :param grower_name: String of grower name
+    :return: Grower object
+    """
+    growers = open_pickle()
+    for grower in growers:
+        if grower.name == grower_name:
+            return grower
+
+def get_field(field_name: str, grower_name: str = '') -> Field:
+    """
+    Function to get a field
+
+    :param field_name: String for the field name
+    :param grower_name: Optional parameter of the string for the grower name
+    :return: Field object of the field
+    """
+    if grower_name:
+        grower = get_grower(grower_name)
+        for field in grower.fields:
+            if field.name == field_name:
+                return field
+    else:
+        growers = open_pickle()
+        for grower in growers:
+            for field in grower.fields:
+                if field.name == field_name:
+                    return field
\ No newline at end of file
Index: historicalET.csv
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>Year4,Year4ET,Year3,Year3ET,Year2,Year2ET,Year1,Year1ET,Average\r\n2018-01-01,0.06,2019-01-01,0.09,2020-01-01,0.05,2021-01-01,0.03,0.0575\r\n2018-01-02,0.03,2019-01-02,0.05,2020-01-02,0.06,2021-01-02,0,0.035\r\n2018-01-03,0.03,2019-01-03,0.04,2020-01-03,0.05,2021-01-03,0,0.03\r\n2018-01-04,0,2019-01-04,0.05,2020-01-04,0.05,2021-01-04,0.02,0.03\r\n2018-01-05,0.01,2019-01-05,0,2020-01-05,0.05,2021-01-05,0.06,0.03\r\n2018-01-06,0.04,2019-01-06,0,2020-01-06,0.08,2021-01-06,0.01,0.0325\r\n2018-01-07,0.03,2019-01-07,0,2020-01-07,0.04,2021-01-07,0.02,0.0225\r\n2018-01-08,0,2019-01-08,0,2020-01-08,0.04,2021-01-08,0.04,0.02\r\n2018-01-09,0.02,2019-01-09,0.03,2020-01-09,0.04,2021-01-09,0.07,0.04\r\n2018-01-10,0.01,2019-01-10,0.03,2020-01-10,0.05,2021-01-10,0.04,0.0325\r\n2018-01-11,0.01,2019-01-11,0.01,2020-01-11,0.04,2021-01-11,0.04,0.025\r\n2018-01-12,0.06,2019-01-12,0.05,2020-01-12,0.05,2021-01-12,0.01,0.0425\r\n2018-01-13,0.04,2019-01-13,0.06,2020-01-13,0.04,2021-01-13,0.03,0.0425\r\n2018-01-14,0.03,2019-01-14,0.04,2020-01-14,0.07,2021-01-14,0.07,0.0525\r\n2018-01-15,0.02,2019-01-15,0,2020-01-15,0.05,2021-01-15,0.04,0.0275\r\n2018-01-16,0.03,2019-01-16,0,2020-01-16,0.03,2021-01-16,0.12,0.045\r\n2018-01-17,0.04,2019-01-17,0.04,2020-01-17,0.02,2021-01-17,0.13,0.0575\r\n2018-01-18,0,2019-01-18,0.01,2020-01-18,0.05,2021-01-18,0.24,0.075\r\n2018-01-19,0.05,2019-01-19,0,2020-01-19,0.03,2021-01-19,0.17,0.0625\r\n2018-01-20,0.07,2019-01-20,0.02,2020-01-20,0.02,2021-01-20,0.1,0.0525\r\n2018-01-21,0.02,2019-01-21,0.06,2020-01-21,0.01,2021-01-21,0.05,0.035\r\n2018-01-22,0.01,2019-01-22,0.09,2020-01-22,0.03,2021-01-22,0.02,0.0375\r\n2018-01-23,0.05,2019-01-23,0.06,2020-01-23,0.01,2021-01-23,0.09,0.0525\r\n2018-01-24,0,2019-01-24,0.1,2020-01-24,0.01,2021-01-24,0.02,0.0325\r\n2018-01-25,0.02,2019-01-25,0.09,2020-01-25,0.02,2021-01-25,0.09,0.055\r\n2018-01-26,0.03,2019-01-26,0.08,2020-01-26,0.05,2021-01-26,0.03,0.0475\r\n2018-01-27,0.04,2019-01-27,0.09,2020-01-27,0.05,2021-01-27,0.03,0.0525\r\n2018-01-28,0.07,2019-01-28,0.02,2020-01-28,0.05,2021-01-28,0.01,0.0375\r\n2018-01-29,0.03,2019-01-29,0.06,2020-01-29,0.07,2021-01-29,0.02,0.045\r\n2018-01-30,0.06,2019-01-30,0.06,2020-01-30,0.07,2021-01-30,0.04,0.0575\r\n2018-01-31,0.06,2019-01-31,0.08,2020-01-31,0.07,2021-01-31,0.02,0.0575\r\n2018-02-01,0.08,2019-02-01,0.02,2020-02-01,0.05,2021-02-01,0.04,0.0475\r\n2018-02-02,0.13,2019-02-02,0.03,2020-02-02,0.12,2021-02-02,0.05,0.0825\r\n2018-02-03,0.15,2019-02-03,0.02,2020-02-03,0.11,2021-02-03,0.04,0.08\r\n2018-02-04,0.1,2019-02-04,0.05,2020-02-04,0.11,2021-02-04,0.11,0.0925\r\n2018-02-05,0.13,2019-02-05,0.06,2020-02-05,0.05,2021-02-05,0.07,0.0775\r\n2018-02-06,0.14,2019-02-06,0.05,2020-02-06,0.07,2021-02-06,0.08,0.085\r\n2018-02-07,0.11,2019-02-07,0.04,2020-02-07,0.07,2021-02-07,0.08,0.075\r\n2018-02-08,0.08,2019-02-08,0.03,2020-02-08,0.14,2021-02-08,0.05,0.075\r\n2018-02-09,0.08,2019-02-09,0.05,2020-02-09,0.17,2021-02-09,0.06,0.09\r\n2018-02-10,0.15,2019-02-10,0.05,2020-02-10,0.15,2021-02-10,0.08,0.1075\r\n2018-02-11,0.07,2019-02-11,0.07,2020-02-11,0.17,2021-02-11,0,0.0775\r\n2018-02-12,0.14,2019-02-12,0.01,2020-02-12,0.12,2021-02-12,0.07,0.085\r\n2018-02-13,0.11,2019-02-13,0,2020-02-13,0.09,2021-02-13,0.11,0.0775\r\n2018-02-14,0.08,2019-02-14,0.05,2020-02-14,0.09,2021-02-14,0.05,0.0675\r\n2018-02-15,0.14,2019-02-15,0.07,2020-02-15,0.08,2021-02-15,0.01,0.075\r\n2018-02-16,0.1,2019-02-16,0.07,2020-02-16,0.08,2021-02-16,0.1,0.0875\r\n2018-02-17,0.1,2019-02-17,0.09,2020-02-17,0.19,2021-02-17,0.14,0.13\r\n2018-02-18,0.11,2019-02-18,0.12,2020-02-18,0.11,2021-02-18,0.03,0.0925\r\n2018-02-19,0.11,2019-02-19,0.09,2020-02-19,0.11,2021-02-19,0.01,0.08\r\n2018-02-20,0.07,2019-02-20,0.08,2020-02-20,0.09,2021-02-20,0.1,0.085\r\n2018-02-21,0.09,2019-02-21,0.16,2020-02-21,0.1,2021-02-21,0.17,0.13\r\n2018-02-22,0.07,2019-02-22,0.11,2020-02-22,0.12,2021-02-22,0.14,0.11\r\n2018-02-23,0.12,2019-02-23,0.07,2020-02-23,0.08,2021-02-23,0.2,0.1175\r\n2018-02-24,0.09,2019-02-24,0.04,2020-02-24,0.16,2021-02-24,0.2,0.1225\r\n2018-02-25,0.11,2019-02-25,0.02,2020-02-25,0.18,2021-02-25,0.14,0.1125\r\n2018-02-26,0.15,2019-02-26,0,2020-02-26,0.12,2021-02-26,0.16,0.1075\r\n2018-02-27,0.18,2019-02-27,0.05,2020-02-27,0.13,2021-02-27,0.15,0.1275\r\n2018-02-28,0.07,2019-02-28,0.1,2020-02-28,0.1,2021-02-28,0.15,0.105\r\n2018-03-01,0.08,2019-03-01,0.05,2020-02-29,0.13,2021-03-01,0.12,0.095\r\n2018-03-02,0.08,2019-03-02,0.02,2020-03-01,0.23,2021-03-02,0.13,0.115\r\n2018-03-03,0.07,2019-03-03,0.02,2020-03-02,0.21,2021-03-03,0.14,0.11\r\n2018-03-04,0.08,2019-03-04,0.05,2020-03-03,0.2,2021-03-04,0.13,0.115\r\n2018-03-05,0.12,2019-03-05,0,2020-03-04,0.13,2021-03-05,0.16,0.1025\r\n2018-03-06,0.11,2019-03-06,0.06,2020-03-05,0.13,2021-03-06,0.1,0.1\r\n2018-03-07,0.12,2019-03-07,0.08,2020-03-06,0.09,2021-03-07,0.12,0.1025\r\n2018-03-08,0.09,2019-03-08,0.07,2020-03-07,0.05,2021-03-08,0.1,0.0775\r\n2018-03-09,0.06,2019-03-09,0.03,2020-03-08,0.07,2021-03-09,0.07,0.0575\r\n2018-03-10,0.11,2019-03-10,0.11,2020-03-09,0.11,2021-03-10,0.04,0.0925\r\n2018-03-11,0.13,2019-03-11,0.15,2020-03-10,0.16,2021-03-11,0.12,0.14\r\n2018-03-12,0.11,2019-03-12,0.16,2020-03-11,0.13,2021-03-12,0.16,0.14\r\n2018-03-13,0.03,2019-03-13,0.16,2020-03-12,0.22,2021-03-13,0.13,0.135\r\n2018-03-14,0.06,2019-03-14,0.15,2020-03-13,0.19,2021-03-14,0.05,0.1125\r\n2018-03-15,0.03,2019-03-15,0.14,2020-03-14,0.13,2021-03-15,0.09,0.0975\r\n2018-03-16,0.07,2019-03-16,0.14,2020-03-15,0.07,2021-03-16,0.12,0.1\r\n2018-03-17,0.1,2019-03-17,0.14,2020-03-16,0.06,2021-03-17,0.12,0.105\r\n2018-03-18,0.11,2019-03-18,0.14,2020-03-17,0.01,2021-03-18,0.01,0.0675\r\n2018-03-19,0.13,2019-03-19,0.09,2020-03-18,0.02,2021-03-19,0.11,0.0875\r\n2018-03-20,0.03,2019-03-20,0.07,2020-03-19,0.12,2021-03-20,0.13,0.0875\r\n2018-03-21,0.01,2019-03-21,0.14,2020-03-20,0.13,2021-03-21,0.16,0.11\r\n2018-03-22,0.12,2019-03-22,0.02,2020-03-21,0.14,2021-03-22,0.13,0.1025\r\n2018-03-23,0.15,2019-03-23,0.07,2020-03-22,0.15,2021-03-23,0.23,0.15\r\n2018-03-24,0.13,2019-03-24,0.12,2020-03-23,0.16,2021-03-24,0.16,0.1425\r\n2018-03-25,0.13,2019-03-25,0.01,2020-03-24,0.06,2021-03-25,0.2,0.1\r\n2018-03-26,0.19,2019-03-26,0.07,2020-03-25,0.12,2021-03-26,0.17,0.1375\r\n2018-03-27,0.21,2019-03-27,0.13,2020-03-26,0.13,2021-03-27,0.17,0.16\r\n2018-03-28,0.23,2019-03-28,0.1,2020-03-27,0.14,2021-03-28,0.17,0.16\r\n2018-03-29,0.2,2019-03-29,0.14,2020-03-28,0.07,2021-03-29,0.29,0.175\r\n2018-03-30,0.16,2019-03-30,0.18,2020-03-29,0.09,2021-03-30,0.25,0.17\r\n2018-03-31,0.16,2019-03-31,0.17,2020-03-30,0.07,2021-03-31,0.2,0.15\r\n2018-04-01,0.18,2019-04-01,0.07,2020-03-31,0.18,2021-04-01,0.18,0.1525\r\n2018-04-02,0.22,2019-04-02,0.07,2020-04-01,0.2,2021-04-02,0.16,0.1625\r\n2018-04-03,0.18,2019-04-03,0.07,2020-04-02,0.19,2021-04-03,0.17,0.1525\r\n2018-04-04,0.15,2019-04-04,0.11,2020-04-03,0.16,2021-04-04,0.14,0.14\r\n2018-04-05,0.05,2019-04-05,0.06,2020-04-04,0.03,2021-04-05,0.15,0.0725\r\n2018-04-06,0,2019-04-06,0.08,2020-04-05,0.09,2021-04-06,0.17,0.085\r\n2018-04-07,0.19,2019-04-07,0.13,2020-04-06,0.14,2021-04-07,0.17,0.1575\r\n2018-04-08,0.16,2019-04-08,0.08,2020-04-07,0.14,2021-04-08,0.17,0.1375\r\n2018-04-09,0.18,2019-04-09,0.21,2020-04-08,0.19,2021-04-09,0.18,0.19\r\n2018-04-10,0.09,2019-04-10,0.22,2020-04-09,0.08,2021-04-10,0.19,0.145\r\n2018-04-11,0.06,2019-04-11,0.15,2020-04-10,0.18,2021-04-11,0.25,0.16\r\n2018-04-12,0.16,2019-04-12,0.22,2020-04-11,0.18,2021-04-12,0.24,0.2\r\n2018-04-13,0.18,2019-04-13,0.19,2020-04-12,0.21,2021-04-13,0.27,0.2125\r\n2018-04-14,0.19,2019-04-14,0.08,2020-04-13,0.24,2021-04-14,0.21,0.18\r\n2018-04-15,0.09,2019-04-15,0.08,2020-04-14,0.22,2021-04-15,0.18,0.1425\r\n2018-04-16,0.16,2019-04-16,0.17,2020-04-15,0.19,2021-04-16,0.23,0.1875\r\n2018-04-17,0.16,2019-04-17,0.19,2020-04-16,0.22,2021-04-17,0.25,0.205\r\n2018-04-18,0.12,2019-04-18,0.2,2020-04-17,0.2,2021-04-18,0.24,0.19\r\n2018-04-19,0.22,2019-04-19,0.19,2020-04-18,0.14,2021-04-19,0.21,0.19\r\n2018-04-20,0.21,2019-04-20,0.12,2020-04-19,0.16,2021-04-20,0.23,0.18\r\n2018-04-21,0.22,2019-04-21,0.24,2020-04-20,0.14,2021-04-21,0.28,0.22\r\n2018-04-22,0.21,2019-04-22,0.27,2020-04-21,0.19,2021-04-22,0.2,0.2175\r\n2018-04-23,0.25,2019-04-23,0.25,2020-04-22,0.2,2021-04-23,0.17,0.2175\r\n2018-04-24,0.21,2019-04-24,0.23,2020-04-23,0.24,2021-04-24,0.18,0.215\r\n2018-04-25,0.21,2019-04-25,0.23,2020-04-24,0.24,2021-04-25,0.09,0.1925\r\n2018-04-26,0.22,2019-04-26,0.22,2020-04-25,0.15,2021-04-26,0.18,0.1925\r\n2018-04-27,0.14,2019-04-27,0.22,2020-04-26,0.23,2021-04-27,0.24,0.2075\r\n2018-04-28,0.14,2019-04-28,0.21,2020-04-27,0.22,2021-04-28,0.23,0.2\r\n2018-04-29,0.18,2019-04-29,0.25,2020-04-28,0.24,2021-04-29,0.22,0.2225\r\n2018-04-30,0.2,2019-04-30,0.21,2020-04-29,0.18,2021-04-30,0.21,0.2\r\n2018-05-01,0.23,2019-05-01,0.22,2020-04-30,0.21,2021-05-01,0.23,0.2225\r\n2018-05-02,0.23,2019-05-02,0.22,2020-05-01,0.22,2021-05-02,0.32,0.2475\r\n2018-05-03,0.21,2019-05-03,0.23,2020-05-02,0.21,2021-05-03,0.33,0.245\r\n2018-05-04,0.21,2019-05-04,0.23,2020-05-03,0.22,2021-05-04,0.27,0.2325\r\n2018-05-05,0.17,2019-05-05,0.25,2020-05-04,0.22,2021-05-05,0.27,0.2275\r\n2018-05-06,0.23,2019-05-06,0.23,2020-05-05,0.27,2021-05-06,0.27,0.25\r\n2018-05-07,0.23,2019-05-07,0.22,2020-05-06,0.34,2021-05-07,0.31,0.275\r\n2018-05-08,0.26,2019-05-08,0.23,2020-05-07,0.32,2021-05-08,0.38,0.2975\r\n2018-05-09,0.24,2019-05-09,0.28,2020-05-08,0.26,2021-05-09,0.39,0.2925\r\n2018-05-10,0.24,2019-05-10,0.31,2020-05-09,0.26,2021-05-10,0.35,0.29\r\n2018-05-11,0.39,2019-05-11,0.23,2020-05-10,0.29,2021-05-11,0.29,0.3\r\n2018-05-12,0.3,2019-05-12,0.24,2020-05-11,0.07,2021-05-12,0.29,0.225\r\n2018-05-13,0.24,2019-05-13,0.24,2020-05-12,0.16,2021-05-13,0.26,0.225\r\n2018-05-14,0.24,2019-05-14,0.16,2020-05-13,0.1,2021-05-14,0.24,0.185\r\n2018-05-15,0.23,2019-05-15,0.07,2020-05-14,0.19,2021-05-15,0.2,0.1725\r\n2018-05-16,0.17,2019-05-16,0.15,2020-05-15,0.23,2021-05-16,0.26,0.2025\r\n2018-05-17,0.23,2019-05-17,0.16,2020-05-16,0.12,2021-05-17,0.25,0.19\r\n2018-05-18,0.25,2019-05-18,0.02,2020-05-17,0.18,2021-05-18,0.26,0.1775\r\n2018-05-19,0.25,2019-05-19,0.12,2020-05-18,0.09,2021-05-19,0.28,0.185\r\n2018-05-20,0.22,2019-05-20,0.21,2020-05-19,0.21,2021-05-20,0.33,0.2425\r\n2018-05-21,0.27,2019-05-21,0.13,2020-05-20,0.21,2021-05-21,0.31,0.23\r\n2018-05-22,0.26,2019-05-22,0.25,2020-05-21,0.25,2021-05-22,0.27,0.2575\r\n2018-05-23,0.23,2019-05-23,0.27,2020-05-22,0.32,2021-05-23,0.25,0.2675\r\n2018-05-24,0.25,2019-05-24,0.22,2020-05-23,0.29,2021-05-24,0.28,0.26\r\n2018-05-25,0.01,2019-05-25,0.24,2020-05-24,0.29,2021-05-25,0.26,0.2\r\n2018-05-26,0.18,2019-05-26,0.1,2020-05-25,0.29,2021-05-26,0.29,0.215\r\n2018-05-27,0.31,2019-05-27,0.18,2020-05-26,0.25,2021-05-27,0.22,0.24\r\n2018-05-28,0.34,2019-05-28,0.26,2020-05-27,0.28,2021-05-28,0.27,0.2875\r\n2018-05-29,0.31,2019-05-29,0.25,2020-05-28,0.28,2021-05-29,0.26,0.275\r\n2018-05-30,0.3,2019-05-30,0.2,2020-05-29,0.28,2021-05-30,0.3,0.27\r\n2018-05-31,0.21,2019-05-31,0.26,2020-05-30,0.15,2021-05-31,0.32,0.235\r\n2018-06-01,0.28,2019-06-01,0.28,2020-05-31,0.21,2021-06-01,0.29,0.265\r\n2018-06-02,0.3,2019-06-02,0.26,2020-06-01,0.24,2021-06-02,0.29,0.2725\r\n2018-06-03,0.3,2019-06-03,0.26,2020-06-02,0.27,2021-06-03,0.28,0.2775\r\n2018-06-04,0.29,2019-06-04,0.3,2020-06-03,0.27,2021-06-04,0.29,0.2875\r\n2018-06-05,0.26,2019-06-05,0.3,2020-06-04,0.3,2021-06-05,0.29,0.2875\r\n2018-06-06,0.27,2019-06-06,0.3,2020-06-05,0.3,2021-06-06,0.34,0.3025\r\n2018-06-07,0.25,2019-06-07,0.37,2020-06-06,0.24,2021-06-07,0.3,0.29\r\n2018-06-08,0.29,2019-06-08,0.37,2020-06-07,0.27,2021-06-08,0.28,0.3025\r\n2018-06-09,0.28,2019-06-09,0.33,2020-06-08,0.27,2021-06-09,0.23,0.2775\r\n2018-06-10,0.26,2019-06-10,0.29,2020-06-09,0.23,2021-06-10,0.25,0.2575\r\n2018-06-11,0.31,2019-06-11,0.28,2020-06-10,0.29,2021-06-11,0.24,0.28\r\n2018-06-12,0.3,2019-06-12,0.2,2020-06-11,0.28,2021-06-12,0.26,0.26\r\n2018-06-13,0.3,2019-06-13,0.24,2020-06-12,0.23,2021-06-13,0.33,0.275\r\n2018-06-14,0.3,2019-06-14,0.26,2020-06-13,0.18,2021-06-14,0.3,0.26\r\n2018-06-15,0.27,2019-06-15,0.2,2020-06-14,0.24,2021-06-15,0.28,0.2475\r\n2018-06-16,0.27,2019-06-16,0.25,2020-06-15,0.13,2021-06-16,0.31,0.24\r\n2018-06-17,0.25,2019-06-17,0.32,2020-06-16,0.3,2021-06-17,0.35,0.305\r\n2018-06-18,0.25,2019-06-18,0.35,2020-06-17,0.37,2021-06-18,0.3,0.3175\r\n2018-06-19,0.27,2019-06-19,0.34,2020-06-18,0.33,2021-06-19,0.3,0.31\r\n2018-06-20,0.3,2019-06-20,0.4,2020-06-19,0.28,2021-06-20,0.31,0.3225\r\n2018-06-21,0.27,2019-06-21,0.43,2020-06-20,0.27,2021-06-21,0.32,0.3225\r\n2018-06-22,0.35,2019-06-22,0.35,2020-06-21,0.28,2021-06-22,0.29,0.3175\r\n2018-06-23,0.46,2019-06-23,0.29,2020-06-22,0.29,2021-06-23,0.28,0.33\r\n2018-06-24,0.37,2019-06-24,0.25,2020-06-23,0.29,2021-06-24,0.27,0.295\r\n2018-06-25,0.27,2019-06-25,0.28,2020-06-24,0.26,2021-06-25,0.28,0.2725\r\n2018-06-26,0.29,2019-06-26,0.31,2020-06-25,0.33,2021-06-26,0.29,0.305\r\n2018-06-27,0.3,2019-06-27,0.24,2020-06-26,0.31,2021-06-27,0.29,0.285\r\n2018-06-28,0.27,2019-06-28,0.27,2020-06-27,0.31,2021-06-28,0.29,0.285\r\n2018-06-29,0.39,2019-06-29,0.27,2020-06-28,0.33,2021-06-29,0.29,0.32\r\n2018-06-30,0.44,2019-06-30,0.29,2020-06-29,0.37,2021-06-30,0.32,0.355\r\n2018-07-01,0.35,2019-07-01,0.25,2020-06-30,0.3,2021-07-01,0.28,0.295\r\n2018-07-02,0.31,2019-07-02,0.27,2020-07-01,0.29,2021-07-02,0.29,0.29\r\n2018-07-03,0.3,2019-07-03,0.27,2020-07-02,0.29,2021-07-03,0.31,0.2925\r\n2018-07-04,0.29,2019-07-04,0.27,2020-07-03,0.28,2021-07-04,0.3,0.285\r\n2018-07-05,0.3,2019-07-05,0.27,2020-07-04,0.28,2021-07-05,0.3,0.2875\r\n2018-07-06,0.14,2019-07-06,0.27,2020-07-05,0.3,2021-07-06,0.28,0.2475\r\n2018-07-07,0.28,2019-07-07,0.27,2020-07-06,0.32,2021-07-07,0.29,0.29\r\n2018-07-08,0.29,2019-07-08,0.26,2020-07-07,0.28,2021-07-08,0.29,0.28\r\n2018-07-09,0.29,2019-07-09,0.22,2020-07-08,0.3,2021-07-09,0.31,0.28\r\n2018-07-10,0.32,2019-07-10,0.26,2020-07-09,0.29,2021-07-10,0.3,0.2925\r\n2018-07-11,0.29,2019-07-11,0.27,2020-07-10,0.3,2021-07-11,0.31,0.2925\r\n2018-07-12,0.29,2019-07-12,0.27,2020-07-11,0.3,2021-07-12,0.29,0.2875\r\n2018-07-13,0.16,2019-07-13,0.27,2020-07-12,0.3,2021-07-13,0.3,0.2575\r\n2018-07-14,0.28,2019-07-14,0.28,2020-07-13,0.29,2021-07-14,0.28,0.2825\r\n2018-07-15,0.28,2019-07-15,0.27,2020-07-14,0.27,2021-07-15,0.28,0.275\r\n2018-07-16,0.28,2019-07-16,0.27,2020-07-15,0.29,2021-07-16,0.26,0.275\r\n2018-07-17,0.28,2019-07-17,0.27,2020-07-16,0.28,2021-07-17,0.28,0.2775\r\n2018-07-18,0.27,2019-07-18,0.28,2020-07-17,0.28,2021-07-18,0.31,0.285\r\n2018-07-19,0.27,2019-07-19,0.27,2020-07-18,0.26,2021-07-19,0.28,0.27\r\n2018-07-20,0.29,2019-07-20,0.26,2020-07-19,0.28,2021-07-20,0.3,0.2825\r\n2018-07-21,0.28,2019-07-21,0.25,2020-07-20,0.27,2021-07-21,0.27,0.2675\r\n2018-07-22,0.28,2019-07-22,0.28,2020-07-21,0.26,2021-07-22,0.29,0.2775\r\n2018-07-23,0.28,2019-07-23,0.29,2020-07-22,0.25,2021-07-23,0.28,0.275\r\n2018-07-24,0.28,2019-07-24,0.28,2020-07-23,0.26,2021-07-24,0.18,0.25\r\n2018-07-25,0.27,2019-07-25,0.27,2020-07-24,0.26,2021-07-25,0.24,0.26\r\n2018-07-26,0.25,2019-07-26,0.27,2020-07-25,0.26,2021-07-26,0.25,0.2575\r\n2018-07-27,0.23,2019-07-27,0.27,2020-07-26,0.27,2021-07-27,0.17,0.235\r\n2018-07-28,0.2,2019-07-28,0.28,2020-07-27,0.27,2021-07-28,0.28,0.2575\r\n2018-07-29,0.22,2019-07-29,0.28,2020-07-28,0.26,2021-07-29,0.26,0.255\r\n2018-07-30,0.19,2019-07-30,0.26,2020-07-29,0.28,2021-07-30,0.24,0.2425\r\n2018-07-31,0.2,2019-07-31,0.26,2020-07-30,0.27,2021-07-31,0.28,0.2525\r\n2018-08-01,0.23,2019-08-01,0.26,2020-07-31,0.27,2021-08-01,0.28,0.26\r\n2018-08-02,0.25,2019-08-02,0.24,2020-08-01,0.25,2021-08-02,0.26,0.25\r\n2018-08-03,0.24,2019-08-03,0.26,2020-08-02,0.26,2021-08-03,0.26,0.255\r\n2018-08-04,0.25,2019-08-04,0.27,2020-08-03,0.26,2021-08-04,0.29,0.2675\r\n2018-08-05,0.18,2019-08-05,0.24,2020-08-04,0.26,2021-08-05,0.22,0.225\r\n2018-08-06,0.21,2019-08-06,0.27,2020-08-05,0.25,2021-08-06,0.24,0.2425\r\n2018-08-07,0.2,2019-08-07,0.26,2020-08-06,0.23,2021-08-07,0.21,0.225\r\n2018-08-08,0.21,2019-08-08,0.24,2020-08-07,0.26,2021-08-08,0.22,0.2325\r\n2018-08-09,0.19,2019-08-09,0.18,2020-08-08,0.25,2021-08-09,0.24,0.215\r\n2018-08-10,0.23,2019-08-10,0.13,2020-08-09,0.26,2021-08-10,0.23,0.2125\r\n2018-08-11,0.23,2019-08-11,0.24,2020-08-10,0.26,2021-08-11,0.2,0.2325\r\n2018-08-12,0.24,2019-08-12,0.23,2020-08-11,0.25,2021-08-12,0.2,0.23\r\n2018-08-13,0.2,2019-08-13,0.24,2020-08-12,0.25,2021-08-13,0.19,0.22\r\n2018-08-14,0.23,2019-08-14,0.25,2020-08-13,0.27,2021-08-14,0.22,0.2425\r\n2018-08-15,0.23,2019-08-15,0.27,2020-08-14,0.25,2021-08-15,0.22,0.2425\r\n2018-08-16,0.21,2019-08-16,0.28,2020-08-15,0.27,2021-08-16,0.18,0.235\r\n2018-08-17,0.23,2019-08-17,0.25,2020-08-16,0.13,2021-08-17,0.14,0.1875\r\n2018-08-18,0.21,2019-08-18,0.23,2020-08-17,0.09,2021-08-18,0.35,0.22\r\n2018-08-19,0.2,2019-08-19,0.22,2020-08-18,0.23,2021-08-19,0.25,0.225\r\n2018-08-20,0.19,2019-08-20,0.21,2020-08-19,0.12,2021-08-20,0.17,0.1725\r\n2018-08-21,0.2,2019-08-21,0.2,2020-08-20,0.11,2021-08-21,0.17,0.17\r\n2018-08-22,0.2,2019-08-22,0.26,2020-08-21,0.08,2021-08-22,0.19,0.1825\r\n2018-08-23,0.21,2019-08-23,0.24,2020-08-22,0.16,2021-08-23,0.22,0.2075\r\n2018-08-24,0.18,2019-08-24,0.23,2020-08-23,0.13,2021-08-24,0.22,0.19\r\n2018-08-25,0.19,2019-08-25,0.23,2020-08-24,0.18,2021-08-25,0.21,0.2025\r\n2018-08-26,0.18,2019-08-26,0.24,2020-08-25,0.16,2021-08-26,0.22,0.2\r\n2018-08-27,0.19,2019-08-27,0.23,2020-08-26,0.16,2021-08-27,0.27,0.2125\r\n2018-08-28,0.21,2019-08-28,0.22,2020-08-27,0.2,2021-08-28,0.2,0.2075\r\n2018-08-29,0.21,2019-08-29,0.23,2020-08-28,0.22,2021-08-29,0.2,0.215\r\n2018-08-30,0.2,2019-08-30,0.21,2020-08-29,0.15,2021-08-30,0.21,0.1925\r\n2018-08-31,0.2,2019-08-31,0.21,2020-08-30,0.17,2021-08-31,0.22,0.2\r\n2018-09-01,0.22,2019-09-01,0.23,2020-08-31,0.24,2021-09-01,0.21,0.225\r\n2018-09-02,0.21,2019-09-02,0.22,2020-09-01,0.28,2021-09-02,0.18,0.2225\r\n2018-09-03,0.19,2019-09-03,0.2,2020-09-02,0.18,2021-09-03,0.19,0.19\r\n2018-09-04,0.21,2019-09-04,0.16,2020-09-03,0.18,2021-09-04,0.2,0.1875\r\n2018-09-05,0.22,2019-09-05,0.24,2020-09-04,0.16,2021-09-05,0.21,0.2075\r\n2018-09-06,0.2,2019-09-06,0.2,2020-09-05,0.21,2021-09-06,0.22,0.2075\r\n2018-09-07,0.21,2019-09-07,0.19,2020-09-06,0.22,2021-09-07,0.24,0.215\r\n2018-09-08,0.19,2019-09-08,0.18,2020-09-07,0.2,2021-09-08,0.18,0.1875\r\n2018-09-09,0.2,2019-09-09,0.18,2020-09-08,0.37,2021-09-09,0.15,0.225\r\n2018-09-10,0.19,2019-09-10,0.17,2020-09-09,0.08,2021-09-10,0.18,0.155\r\n2018-09-11,0.22,2019-09-11,0.2,2020-09-10,0.09,2021-09-11,0.19,0.175\r\n2018-09-12,0.17,2019-09-12,0.19,2020-09-11,0.13,2021-09-12,0.18,0.1675\r\n2018-09-13,0.17,2019-09-13,0.19,2020-09-12,0.15,2021-09-13,0.18,0.1725\r\n2018-09-14,0.17,2019-09-14,0.21,2020-09-13,0.15,2021-09-14,0.22,0.1875\r\n2018-09-15,0.19,2019-09-15,0.21,2020-09-14,0.15,2021-09-15,0.18,0.1825\r\n2018-09-16,0.17,2019-09-16,0.15,2020-09-15,0.17,2021-09-16,0.17,0.165\r\n2018-09-17,0.17,2019-09-17,0.17,2020-09-16,0.15,2021-09-17,0.19,0.17\r\n2018-09-18,0.16,2019-09-18,0.1,2020-09-17,0.2,2021-09-18,0.06,0.13\r\n2018-09-19,0.21,2019-09-19,0.17,2020-09-18,0.16,2021-09-19,0.18,0.18\r\n2018-09-20,0.22,2019-09-20,0.2,2020-09-19,0.17,2021-09-20,0.28,0.2175\r\n2018-09-21,0.19,2019-09-21,0.2,2020-09-20,0.17,2021-09-21,0.2,0.19\r\n2018-09-22,0.17,2019-09-22,0.13,2020-09-21,0.13,2021-09-22,0.18,0.1525\r\n2018-09-23,0.17,2019-09-23,0.24,2020-09-22,0.15,2021-09-23,0.26,0.205\r\n2018-09-24,0.22,2019-09-24,0.26,2020-09-23,0.15,2021-09-24,0.19,0.205\r\n2018-09-25,0.25,2019-09-25,0.3,2020-09-24,0.14,2021-09-25,0.19,0.22\r\n2018-09-26,0.17,2019-09-26,0.23,2020-09-25,0.14,2021-09-26,0.16,0.175\r\n2018-09-27,0.17,2019-09-27,0.05,2020-09-26,0.22,2021-09-27,0.13,0.1425\r\n2018-09-28,0.17,2019-09-28,0.17,2020-09-27,0.32,2021-09-28,0.21,0.2175\r\n2018-09-29,0.13,2019-09-29,0.1,2020-09-28,0.29,2021-09-29,0.22,0.185\r\n2018-09-30,0.14,2019-09-30,0.12,2020-09-29,0.13,2021-09-30,0.21,0.15\r\n2018-10-01,0.07,2019-10-01,0.16,2020-09-30,0.19,2021-10-01,0.15,0.1425\r\n2018-10-02,0.1,2019-10-02,0.13,2020-10-01,0.18,2021-10-02,0.15,0.14\r\n2018-10-03,0.1,2019-10-03,0.16,2020-10-02,0.12,2021-10-03,0.14,0.13\r\n2018-10-04,0.03,2019-10-04,0.16,2020-10-03,0.11,2021-10-04,0.16,0.115\r\n2018-10-05,0.11,2019-10-05,0.2,2020-10-04,0.11,2021-10-05,0.17,0.1475\r\n2018-10-06,0.18,2019-10-06,0.18,2020-10-05,0.15,2021-10-06,0.09,0.15\r\n2018-10-07,0.26,2019-10-07,0.14,2020-10-06,0.14,2021-10-07,0.1,0.16\r\n2018-10-08,0.22,2019-10-08,0.15,2020-10-07,0.14,2021-10-08,0.13,0.16\r\n2018-10-09,0.16,2019-10-09,0.29,2020-10-08,0.09,2021-10-09,0.14,0.17\r\n2018-10-10,0.14,2019-10-10,0.22,2020-10-09,0.11,2021-10-10,0.16,0.1575\r\n2018-10-11,0.17,2019-10-11,0.14,2020-10-10,0.07,2021-10-11,0.25,0.1575\r\n2018-10-12,0.2,2019-10-12,0.12,2020-10-11,0.17,2021-10-12,0.19,0.17\r\n2018-10-13,0.21,2019-10-13,0.12,2020-10-12,0.15,2021-10-13,0.11,0.1475\r\n2018-10-14,0.24,2019-10-14,0.12,2020-10-13,0.13,2021-10-14,0.2,0.1725\r\n2018-10-15,0.2,2019-10-15,0.12,2020-10-14,0.25,2021-10-15,0.12,0.1725\r\n2018-10-16,0.19,2019-10-16,0.11,2020-10-15,0.31,2021-10-16,0.12,0.1825\r\n2018-10-17,0.11,2019-10-17,0.13,2020-10-16,0.26,2021-10-17,0.12,0.155\r\n2018-10-18,0.11,2019-10-18,0.1,2020-10-17,0.18,2021-10-18,0.05,0.11\r\n2018-10-19,0.11,2019-10-19,0.08,2020-10-18,0.15,2021-10-19,0.09,0.1075\r\n2018-10-20,0.12,2019-10-20,0.12,2020-10-19,0.13,2021-10-20,0,0.0925\r\n2018-10-21,0.11,2019-10-21,0.18,2020-10-20,0.22,2021-10-21,0.01,0.13\r\n2018-10-22,0.11,2019-10-22,0.18,2020-10-21,0.2,2021-10-22,0.08,0.1425\r\n2018-10-23,0.07,2019-10-23,0.24,2020-10-22,0.29,2021-10-23,0.01,0.1525\r\n2018-10-24,0.08,2019-10-24,0.21,2020-10-23,0.16,2021-10-24,0,0.1125\r\n2018-10-25,0.11,2019-10-25,0.11,2020-10-24,0.09,2021-10-25,0.08,0.0975\r\n2018-10-26,0.09,2019-10-26,0.2,2020-10-25,0.2,2021-10-26,0.05,0.135\r\n2018-10-27,0.1,2019-10-27,0.22,2020-10-26,0.16,2021-10-27,0.07,0.1375\r\n2018-10-28,0.06,2019-10-28,0.12,2020-10-27,0.15,2021-10-28,0.09,0.105\r\n2018-10-29,0.12,2019-10-29,0.18,2020-10-28,0.09,2021-10-29,0.07,0.115\r\n2018-10-30,0.14,2019-10-30,0.13,2020-10-29,0.09,2021-10-30,0.07,0.1075\r\n2018-10-31,0.16,2019-10-31,0.1,2020-10-30,0.09,2021-10-31,0.05,0.1\r\n2018-11-01,0.14,2019-11-01,0.09,2020-10-31,0.1,2021-11-01,0.02,0.0875\r\n2018-11-02,0.11,2019-11-02,0.1,2020-11-01,0.09,2021-11-02,0.07,0.0925\r\n2018-11-03,0.2,2019-11-03,0.09,2020-11-02,0.1,2021-11-03,0.08,0.1175\r\n2018-11-04,0.12,2019-11-04,0.09,2020-11-03,0.09,2021-11-04,0.1,0.1\r\n2018-11-05,0.17,2019-11-05,0.09,2020-11-04,0.09,2021-11-05,0.01,0.09\r\n2018-11-06,0.18,2019-11-06,0.09,2020-11-05,0.1,2021-11-06,0.02,0.0975\r\n2018-11-07,0.17,2019-11-07,0.08,2020-11-06,0.13,2021-11-07,0.08,0.115\r\n2018-11-08,0.2,2019-11-08,0.09,2020-11-07,0.07,2021-11-08,0.04,0.1\r\n2018-11-09,0.1,2019-11-09,0.08,2020-11-08,0.14,2021-11-09,0,0.08\r\n2018-11-10,0.11,2019-11-10,0.09,2020-11-09,0.11,2021-11-10,0.03,0.085\r\n2018-11-11,0.16,2019-11-11,0.12,2020-11-10,0.07,2021-11-11,0.08,0.1075\r\n2018-11-12,0.11,2019-11-12,0.07,2020-11-11,0.07,2021-11-12,0.02,0.0675\r\n2018-11-13,0.04,2019-11-13,0.06,2020-11-12,0.06,2021-11-13,0.05,0.0525\r\n2018-11-14,0.05,2019-11-14,0.07,2020-11-13,0.04,2021-11-14,0.03,0.0475\r\n2018-11-15,0.06,2019-11-15,0.08,2020-11-14,0.04,2021-11-15,0,0.045\r\n2018-11-16,0.05,2019-11-16,0.11,2020-11-15,0.06,2021-11-16,0.1,0.08\r\n2018-11-17,0.04,2019-11-17,0.07,2020-11-16,0.07,2021-11-17,0.07,0.0625\r\n2018-11-18,0.05,2019-11-18,0.08,2020-11-17,0.01,2021-11-18,0,0.035\r\n2018-11-19,0.07,2019-11-19,0.14,2020-11-18,0.07,2021-11-19,0.01,0.0725\r\n2018-11-20,0.06,2019-11-20,0.23,2020-11-19,0.08,2021-11-20,0.1,0.1175\r\n2018-11-21,0,2019-11-21,0.08,2020-11-20,0.1,2021-11-21,0.09,0.0675\r\n2018-11-22,0.04,2019-11-22,0.07,2020-11-21,0.11,2021-11-22,0.06,0.07\r\n2018-11-23,0,2019-11-23,0.09,2020-11-22,0.05,2021-11-23,0.09,0.0575\r\n2018-11-24,0.06,2019-11-24,0.06,2020-11-23,0.08,2021-11-24,0.1,0.075\r\n2018-11-25,0.07,2019-11-25,0.12,2020-11-24,0.08,2021-11-25,0.05,0.08\r\n2018-11-26,0.02,2019-11-26,0.02,2020-11-25,0.09,2021-11-26,0.06,0.0475\r\n2018-11-27,0,2019-11-27,0.04,2020-11-26,0.1,2021-11-27,0.06,0.05\r\n2018-11-28,0.01,2019-11-28,0.04,2020-11-27,0.08,2021-11-28,0.06,0.0475\r\n2018-11-29,0,2019-11-29,0.04,2020-11-28,0.07,2021-11-29,0.06,0.0425\r\n2018-11-30,0.04,2019-11-30,0,2020-11-29,0.05,2021-11-30,0.07,0.04\r\n2018-12-01,0.05,2019-12-01,0,2020-11-30,0.06,2021-12-01,0.07,0.045\r\n2018-12-02,0.05,2019-12-02,0.02,2020-12-01,0.11,2021-12-02,0.06,0.06\r\n2018-12-03,0.04,2019-12-03,0.08,2020-12-02,0.09,2021-12-03,0.05,0.065\r\n2018-12-04,0.06,2019-12-04,0.07,2020-12-03,0.05,2021-12-04,0.05,0.0575\r\n2018-12-05,0.02,2019-12-05,0.05,2020-12-04,0.08,2021-12-05,0.04,0.0475\r\n2018-12-06,0.07,2019-12-06,0.08,2020-12-05,0.04,2021-12-06,0,0.0475\r\n2018-12-07,0.04,2019-12-07,0.11,2020-12-06,0.11,2021-12-07,0.05,0.0775\r\n2018-12-08,0.06,2019-12-08,0.07,2020-12-07,0.15,2021-12-08,0,0.07\r\n2018-12-09,0.02,2019-12-09,0.05,2020-12-08,0.09,2021-12-09,0.07,0.0575\r\n2018-12-10,0.07,2019-12-10,0.04,2020-12-09,0.11,2021-12-10,0.06,0.07\r\n2018-12-11,0.06,2019-12-11,0.04,2020-12-10,0.1,2021-12-11,0.01,0.0525\r\n2018-12-12,0.08,2019-12-12,0.04,2020-12-11,0.03,2021-12-12,0,0.0375\r\n2018-12-13,0.05,2019-12-13,0.07,2020-12-12,0.05,2021-12-13,0.01,0.045\r\n2018-12-14,0.03,2019-12-14,0.05,2020-12-13,0,2021-12-14,0.03,0.0275\r\n2018-12-15,0,2019-12-15,0.04,2020-12-14,0.06,2021-12-15,0.01,0.0275\r\n2018-12-16,0,2019-12-16,0.06,2020-12-15,0.03,2021-12-16,0.01,0.025\r\n2018-12-17,0.03,2019-12-17,0.04,2020-12-16,0.01,2021-12-17,0.04,0.03\r\n2018-12-18,0.03,2019-12-18,0.05,2020-12-17,0.06,2021-12-18,0.03,0.0425\r\n2018-12-19,0.05,2019-12-19,0.04,2020-12-18,0.07,2021-12-19,0,0.04\r\n2018-12-20,0,2019-12-20,0.04,2020-12-19,0.04,2021-12-20,0.02,0.025\r\n2018-12-21,0.04,2019-12-21,0.06,2020-12-20,0.03,2021-12-21,0.01,0.035\r\n2018-12-22,0.03,2019-12-22,0.06,2020-12-21,0.04,2021-12-22,0,0.0325\r\n2018-12-23,0,2019-12-23,0.07,2020-12-22,0.09,2021-12-23,0,0.04\r\n2018-12-24,0,2019-12-24,0.04,2020-12-23,0.11,2021-12-24,0.06,0.0525\r\n2018-12-25,0.08,2019-12-25,0.05,2020-12-24,0.04,2021-12-25,0.02,0.0475\r\n2018-12-26,0.06,2019-12-26,0.07,2020-12-25,0,2021-12-26,0.03,0.04\r\n2018-12-27,0.08,2019-12-27,0.06,2020-12-26,0.04,2021-12-27,0.05,0.0575\r\n2018-12-28,0.09,2019-12-28,0.05,2020-12-27,0.04,2021-12-28,0.01,0.0475\r\n2018-12-29,0.06,2019-12-29,0.05,2020-12-28,0.04,2021-12-29,0.01,0.04\r\n2018-12-30,0.05,2019-12-30,0.09,2020-12-29,0.08,2021-12-30,0.02,0.06\r\n2018-12-31,0.11,2019-12-31,0.06,2020-12-30,0.04,2021-12-31,0.04,0.0625\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/historicalET.csv b/historicalET.csv
--- a/historicalET.csv	(revision 5cf7d78a1d94023d00661edf5517badacc064130)
+++ b/historicalET.csv	(date 1720048514084)
@@ -1,366 +1,367 @@
-Year4,Year4ET,Year3,Year3ET,Year2,Year2ET,Year1,Year1ET,Average
-2018-01-01,0.06,2019-01-01,0.09,2020-01-01,0.05,2021-01-01,0.03,0.0575
-2018-01-02,0.03,2019-01-02,0.05,2020-01-02,0.06,2021-01-02,0,0.035
-2018-01-03,0.03,2019-01-03,0.04,2020-01-03,0.05,2021-01-03,0,0.03
-2018-01-04,0,2019-01-04,0.05,2020-01-04,0.05,2021-01-04,0.02,0.03
-2018-01-05,0.01,2019-01-05,0,2020-01-05,0.05,2021-01-05,0.06,0.03
-2018-01-06,0.04,2019-01-06,0,2020-01-06,0.08,2021-01-06,0.01,0.0325
-2018-01-07,0.03,2019-01-07,0,2020-01-07,0.04,2021-01-07,0.02,0.0225
-2018-01-08,0,2019-01-08,0,2020-01-08,0.04,2021-01-08,0.04,0.02
-2018-01-09,0.02,2019-01-09,0.03,2020-01-09,0.04,2021-01-09,0.07,0.04
-2018-01-10,0.01,2019-01-10,0.03,2020-01-10,0.05,2021-01-10,0.04,0.0325
-2018-01-11,0.01,2019-01-11,0.01,2020-01-11,0.04,2021-01-11,0.04,0.025
-2018-01-12,0.06,2019-01-12,0.05,2020-01-12,0.05,2021-01-12,0.01,0.0425
-2018-01-13,0.04,2019-01-13,0.06,2020-01-13,0.04,2021-01-13,0.03,0.0425
-2018-01-14,0.03,2019-01-14,0.04,2020-01-14,0.07,2021-01-14,0.07,0.0525
-2018-01-15,0.02,2019-01-15,0,2020-01-15,0.05,2021-01-15,0.04,0.0275
-2018-01-16,0.03,2019-01-16,0,2020-01-16,0.03,2021-01-16,0.12,0.045
-2018-01-17,0.04,2019-01-17,0.04,2020-01-17,0.02,2021-01-17,0.13,0.0575
-2018-01-18,0,2019-01-18,0.01,2020-01-18,0.05,2021-01-18,0.24,0.075
-2018-01-19,0.05,2019-01-19,0,2020-01-19,0.03,2021-01-19,0.17,0.0625
-2018-01-20,0.07,2019-01-20,0.02,2020-01-20,0.02,2021-01-20,0.1,0.0525
-2018-01-21,0.02,2019-01-21,0.06,2020-01-21,0.01,2021-01-21,0.05,0.035
-2018-01-22,0.01,2019-01-22,0.09,2020-01-22,0.03,2021-01-22,0.02,0.0375
-2018-01-23,0.05,2019-01-23,0.06,2020-01-23,0.01,2021-01-23,0.09,0.0525
-2018-01-24,0,2019-01-24,0.1,2020-01-24,0.01,2021-01-24,0.02,0.0325
-2018-01-25,0.02,2019-01-25,0.09,2020-01-25,0.02,2021-01-25,0.09,0.055
-2018-01-26,0.03,2019-01-26,0.08,2020-01-26,0.05,2021-01-26,0.03,0.0475
-2018-01-27,0.04,2019-01-27,0.09,2020-01-27,0.05,2021-01-27,0.03,0.0525
-2018-01-28,0.07,2019-01-28,0.02,2020-01-28,0.05,2021-01-28,0.01,0.0375
-2018-01-29,0.03,2019-01-29,0.06,2020-01-29,0.07,2021-01-29,0.02,0.045
-2018-01-30,0.06,2019-01-30,0.06,2020-01-30,0.07,2021-01-30,0.04,0.0575
-2018-01-31,0.06,2019-01-31,0.08,2020-01-31,0.07,2021-01-31,0.02,0.0575
-2018-02-01,0.08,2019-02-01,0.02,2020-02-01,0.05,2021-02-01,0.04,0.0475
-2018-02-02,0.13,2019-02-02,0.03,2020-02-02,0.12,2021-02-02,0.05,0.0825
-2018-02-03,0.15,2019-02-03,0.02,2020-02-03,0.11,2021-02-03,0.04,0.08
-2018-02-04,0.1,2019-02-04,0.05,2020-02-04,0.11,2021-02-04,0.11,0.0925
-2018-02-05,0.13,2019-02-05,0.06,2020-02-05,0.05,2021-02-05,0.07,0.0775
-2018-02-06,0.14,2019-02-06,0.05,2020-02-06,0.07,2021-02-06,0.08,0.085
-2018-02-07,0.11,2019-02-07,0.04,2020-02-07,0.07,2021-02-07,0.08,0.075
-2018-02-08,0.08,2019-02-08,0.03,2020-02-08,0.14,2021-02-08,0.05,0.075
-2018-02-09,0.08,2019-02-09,0.05,2020-02-09,0.17,2021-02-09,0.06,0.09
-2018-02-10,0.15,2019-02-10,0.05,2020-02-10,0.15,2021-02-10,0.08,0.1075
-2018-02-11,0.07,2019-02-11,0.07,2020-02-11,0.17,2021-02-11,0,0.0775
-2018-02-12,0.14,2019-02-12,0.01,2020-02-12,0.12,2021-02-12,0.07,0.085
-2018-02-13,0.11,2019-02-13,0,2020-02-13,0.09,2021-02-13,0.11,0.0775
-2018-02-14,0.08,2019-02-14,0.05,2020-02-14,0.09,2021-02-14,0.05,0.0675
-2018-02-15,0.14,2019-02-15,0.07,2020-02-15,0.08,2021-02-15,0.01,0.075
-2018-02-16,0.1,2019-02-16,0.07,2020-02-16,0.08,2021-02-16,0.1,0.0875
-2018-02-17,0.1,2019-02-17,0.09,2020-02-17,0.19,2021-02-17,0.14,0.13
-2018-02-18,0.11,2019-02-18,0.12,2020-02-18,0.11,2021-02-18,0.03,0.0925
-2018-02-19,0.11,2019-02-19,0.09,2020-02-19,0.11,2021-02-19,0.01,0.08
-2018-02-20,0.07,2019-02-20,0.08,2020-02-20,0.09,2021-02-20,0.1,0.085
-2018-02-21,0.09,2019-02-21,0.16,2020-02-21,0.1,2021-02-21,0.17,0.13
-2018-02-22,0.07,2019-02-22,0.11,2020-02-22,0.12,2021-02-22,0.14,0.11
-2018-02-23,0.12,2019-02-23,0.07,2020-02-23,0.08,2021-02-23,0.2,0.1175
-2018-02-24,0.09,2019-02-24,0.04,2020-02-24,0.16,2021-02-24,0.2,0.1225
-2018-02-25,0.11,2019-02-25,0.02,2020-02-25,0.18,2021-02-25,0.14,0.1125
-2018-02-26,0.15,2019-02-26,0,2020-02-26,0.12,2021-02-26,0.16,0.1075
-2018-02-27,0.18,2019-02-27,0.05,2020-02-27,0.13,2021-02-27,0.15,0.1275
-2018-02-28,0.07,2019-02-28,0.1,2020-02-28,0.1,2021-02-28,0.15,0.105
-2018-03-01,0.08,2019-03-01,0.05,2020-02-29,0.13,2021-03-01,0.12,0.095
-2018-03-02,0.08,2019-03-02,0.02,2020-03-01,0.23,2021-03-02,0.13,0.115
-2018-03-03,0.07,2019-03-03,0.02,2020-03-02,0.21,2021-03-03,0.14,0.11
-2018-03-04,0.08,2019-03-04,0.05,2020-03-03,0.2,2021-03-04,0.13,0.115
-2018-03-05,0.12,2019-03-05,0,2020-03-04,0.13,2021-03-05,0.16,0.1025
-2018-03-06,0.11,2019-03-06,0.06,2020-03-05,0.13,2021-03-06,0.1,0.1
-2018-03-07,0.12,2019-03-07,0.08,2020-03-06,0.09,2021-03-07,0.12,0.1025
-2018-03-08,0.09,2019-03-08,0.07,2020-03-07,0.05,2021-03-08,0.1,0.0775
-2018-03-09,0.06,2019-03-09,0.03,2020-03-08,0.07,2021-03-09,0.07,0.0575
-2018-03-10,0.11,2019-03-10,0.11,2020-03-09,0.11,2021-03-10,0.04,0.0925
-2018-03-11,0.13,2019-03-11,0.15,2020-03-10,0.16,2021-03-11,0.12,0.14
-2018-03-12,0.11,2019-03-12,0.16,2020-03-11,0.13,2021-03-12,0.16,0.14
-2018-03-13,0.03,2019-03-13,0.16,2020-03-12,0.22,2021-03-13,0.13,0.135
-2018-03-14,0.06,2019-03-14,0.15,2020-03-13,0.19,2021-03-14,0.05,0.1125
-2018-03-15,0.03,2019-03-15,0.14,2020-03-14,0.13,2021-03-15,0.09,0.0975
-2018-03-16,0.07,2019-03-16,0.14,2020-03-15,0.07,2021-03-16,0.12,0.1
-2018-03-17,0.1,2019-03-17,0.14,2020-03-16,0.06,2021-03-17,0.12,0.105
-2018-03-18,0.11,2019-03-18,0.14,2020-03-17,0.01,2021-03-18,0.01,0.0675
-2018-03-19,0.13,2019-03-19,0.09,2020-03-18,0.02,2021-03-19,0.11,0.0875
-2018-03-20,0.03,2019-03-20,0.07,2020-03-19,0.12,2021-03-20,0.13,0.0875
-2018-03-21,0.01,2019-03-21,0.14,2020-03-20,0.13,2021-03-21,0.16,0.11
-2018-03-22,0.12,2019-03-22,0.02,2020-03-21,0.14,2021-03-22,0.13,0.1025
-2018-03-23,0.15,2019-03-23,0.07,2020-03-22,0.15,2021-03-23,0.23,0.15
-2018-03-24,0.13,2019-03-24,0.12,2020-03-23,0.16,2021-03-24,0.16,0.1425
-2018-03-25,0.13,2019-03-25,0.01,2020-03-24,0.06,2021-03-25,0.2,0.1
-2018-03-26,0.19,2019-03-26,0.07,2020-03-25,0.12,2021-03-26,0.17,0.1375
-2018-03-27,0.21,2019-03-27,0.13,2020-03-26,0.13,2021-03-27,0.17,0.16
-2018-03-28,0.23,2019-03-28,0.1,2020-03-27,0.14,2021-03-28,0.17,0.16
-2018-03-29,0.2,2019-03-29,0.14,2020-03-28,0.07,2021-03-29,0.29,0.175
-2018-03-30,0.16,2019-03-30,0.18,2020-03-29,0.09,2021-03-30,0.25,0.17
-2018-03-31,0.16,2019-03-31,0.17,2020-03-30,0.07,2021-03-31,0.2,0.15
-2018-04-01,0.18,2019-04-01,0.07,2020-03-31,0.18,2021-04-01,0.18,0.1525
-2018-04-02,0.22,2019-04-02,0.07,2020-04-01,0.2,2021-04-02,0.16,0.1625
-2018-04-03,0.18,2019-04-03,0.07,2020-04-02,0.19,2021-04-03,0.17,0.1525
-2018-04-04,0.15,2019-04-04,0.11,2020-04-03,0.16,2021-04-04,0.14,0.14
-2018-04-05,0.05,2019-04-05,0.06,2020-04-04,0.03,2021-04-05,0.15,0.0725
-2018-04-06,0,2019-04-06,0.08,2020-04-05,0.09,2021-04-06,0.17,0.085
-2018-04-07,0.19,2019-04-07,0.13,2020-04-06,0.14,2021-04-07,0.17,0.1575
-2018-04-08,0.16,2019-04-08,0.08,2020-04-07,0.14,2021-04-08,0.17,0.1375
-2018-04-09,0.18,2019-04-09,0.21,2020-04-08,0.19,2021-04-09,0.18,0.19
-2018-04-10,0.09,2019-04-10,0.22,2020-04-09,0.08,2021-04-10,0.19,0.145
-2018-04-11,0.06,2019-04-11,0.15,2020-04-10,0.18,2021-04-11,0.25,0.16
-2018-04-12,0.16,2019-04-12,0.22,2020-04-11,0.18,2021-04-12,0.24,0.2
-2018-04-13,0.18,2019-04-13,0.19,2020-04-12,0.21,2021-04-13,0.27,0.2125
-2018-04-14,0.19,2019-04-14,0.08,2020-04-13,0.24,2021-04-14,0.21,0.18
-2018-04-15,0.09,2019-04-15,0.08,2020-04-14,0.22,2021-04-15,0.18,0.1425
-2018-04-16,0.16,2019-04-16,0.17,2020-04-15,0.19,2021-04-16,0.23,0.1875
-2018-04-17,0.16,2019-04-17,0.19,2020-04-16,0.22,2021-04-17,0.25,0.205
-2018-04-18,0.12,2019-04-18,0.2,2020-04-17,0.2,2021-04-18,0.24,0.19
-2018-04-19,0.22,2019-04-19,0.19,2020-04-18,0.14,2021-04-19,0.21,0.19
-2018-04-20,0.21,2019-04-20,0.12,2020-04-19,0.16,2021-04-20,0.23,0.18
-2018-04-21,0.22,2019-04-21,0.24,2020-04-20,0.14,2021-04-21,0.28,0.22
-2018-04-22,0.21,2019-04-22,0.27,2020-04-21,0.19,2021-04-22,0.2,0.2175
-2018-04-23,0.25,2019-04-23,0.25,2020-04-22,0.2,2021-04-23,0.17,0.2175
-2018-04-24,0.21,2019-04-24,0.23,2020-04-23,0.24,2021-04-24,0.18,0.215
-2018-04-25,0.21,2019-04-25,0.23,2020-04-24,0.24,2021-04-25,0.09,0.1925
-2018-04-26,0.22,2019-04-26,0.22,2020-04-25,0.15,2021-04-26,0.18,0.1925
-2018-04-27,0.14,2019-04-27,0.22,2020-04-26,0.23,2021-04-27,0.24,0.2075
-2018-04-28,0.14,2019-04-28,0.21,2020-04-27,0.22,2021-04-28,0.23,0.2
-2018-04-29,0.18,2019-04-29,0.25,2020-04-28,0.24,2021-04-29,0.22,0.2225
-2018-04-30,0.2,2019-04-30,0.21,2020-04-29,0.18,2021-04-30,0.21,0.2
-2018-05-01,0.23,2019-05-01,0.22,2020-04-30,0.21,2021-05-01,0.23,0.2225
-2018-05-02,0.23,2019-05-02,0.22,2020-05-01,0.22,2021-05-02,0.32,0.2475
-2018-05-03,0.21,2019-05-03,0.23,2020-05-02,0.21,2021-05-03,0.33,0.245
-2018-05-04,0.21,2019-05-04,0.23,2020-05-03,0.22,2021-05-04,0.27,0.2325
-2018-05-05,0.17,2019-05-05,0.25,2020-05-04,0.22,2021-05-05,0.27,0.2275
-2018-05-06,0.23,2019-05-06,0.23,2020-05-05,0.27,2021-05-06,0.27,0.25
-2018-05-07,0.23,2019-05-07,0.22,2020-05-06,0.34,2021-05-07,0.31,0.275
-2018-05-08,0.26,2019-05-08,0.23,2020-05-07,0.32,2021-05-08,0.38,0.2975
-2018-05-09,0.24,2019-05-09,0.28,2020-05-08,0.26,2021-05-09,0.39,0.2925
-2018-05-10,0.24,2019-05-10,0.31,2020-05-09,0.26,2021-05-10,0.35,0.29
-2018-05-11,0.39,2019-05-11,0.23,2020-05-10,0.29,2021-05-11,0.29,0.3
-2018-05-12,0.3,2019-05-12,0.24,2020-05-11,0.07,2021-05-12,0.29,0.225
-2018-05-13,0.24,2019-05-13,0.24,2020-05-12,0.16,2021-05-13,0.26,0.225
-2018-05-14,0.24,2019-05-14,0.16,2020-05-13,0.1,2021-05-14,0.24,0.185
-2018-05-15,0.23,2019-05-15,0.07,2020-05-14,0.19,2021-05-15,0.2,0.1725
-2018-05-16,0.17,2019-05-16,0.15,2020-05-15,0.23,2021-05-16,0.26,0.2025
-2018-05-17,0.23,2019-05-17,0.16,2020-05-16,0.12,2021-05-17,0.25,0.19
-2018-05-18,0.25,2019-05-18,0.02,2020-05-17,0.18,2021-05-18,0.26,0.1775
-2018-05-19,0.25,2019-05-19,0.12,2020-05-18,0.09,2021-05-19,0.28,0.185
-2018-05-20,0.22,2019-05-20,0.21,2020-05-19,0.21,2021-05-20,0.33,0.2425
-2018-05-21,0.27,2019-05-21,0.13,2020-05-20,0.21,2021-05-21,0.31,0.23
-2018-05-22,0.26,2019-05-22,0.25,2020-05-21,0.25,2021-05-22,0.27,0.2575
-2018-05-23,0.23,2019-05-23,0.27,2020-05-22,0.32,2021-05-23,0.25,0.2675
-2018-05-24,0.25,2019-05-24,0.22,2020-05-23,0.29,2021-05-24,0.28,0.26
-2018-05-25,0.01,2019-05-25,0.24,2020-05-24,0.29,2021-05-25,0.26,0.2
-2018-05-26,0.18,2019-05-26,0.1,2020-05-25,0.29,2021-05-26,0.29,0.215
-2018-05-27,0.31,2019-05-27,0.18,2020-05-26,0.25,2021-05-27,0.22,0.24
-2018-05-28,0.34,2019-05-28,0.26,2020-05-27,0.28,2021-05-28,0.27,0.2875
-2018-05-29,0.31,2019-05-29,0.25,2020-05-28,0.28,2021-05-29,0.26,0.275
-2018-05-30,0.3,2019-05-30,0.2,2020-05-29,0.28,2021-05-30,0.3,0.27
-2018-05-31,0.21,2019-05-31,0.26,2020-05-30,0.15,2021-05-31,0.32,0.235
-2018-06-01,0.28,2019-06-01,0.28,2020-05-31,0.21,2021-06-01,0.29,0.265
-2018-06-02,0.3,2019-06-02,0.26,2020-06-01,0.24,2021-06-02,0.29,0.2725
-2018-06-03,0.3,2019-06-03,0.26,2020-06-02,0.27,2021-06-03,0.28,0.2775
-2018-06-04,0.29,2019-06-04,0.3,2020-06-03,0.27,2021-06-04,0.29,0.2875
-2018-06-05,0.26,2019-06-05,0.3,2020-06-04,0.3,2021-06-05,0.29,0.2875
-2018-06-06,0.27,2019-06-06,0.3,2020-06-05,0.3,2021-06-06,0.34,0.3025
-2018-06-07,0.25,2019-06-07,0.37,2020-06-06,0.24,2021-06-07,0.3,0.29
-2018-06-08,0.29,2019-06-08,0.37,2020-06-07,0.27,2021-06-08,0.28,0.3025
-2018-06-09,0.28,2019-06-09,0.33,2020-06-08,0.27,2021-06-09,0.23,0.2775
-2018-06-10,0.26,2019-06-10,0.29,2020-06-09,0.23,2021-06-10,0.25,0.2575
-2018-06-11,0.31,2019-06-11,0.28,2020-06-10,0.29,2021-06-11,0.24,0.28
-2018-06-12,0.3,2019-06-12,0.2,2020-06-11,0.28,2021-06-12,0.26,0.26
-2018-06-13,0.3,2019-06-13,0.24,2020-06-12,0.23,2021-06-13,0.33,0.275
-2018-06-14,0.3,2019-06-14,0.26,2020-06-13,0.18,2021-06-14,0.3,0.26
-2018-06-15,0.27,2019-06-15,0.2,2020-06-14,0.24,2021-06-15,0.28,0.2475
-2018-06-16,0.27,2019-06-16,0.25,2020-06-15,0.13,2021-06-16,0.31,0.24
-2018-06-17,0.25,2019-06-17,0.32,2020-06-16,0.3,2021-06-17,0.35,0.305
-2018-06-18,0.25,2019-06-18,0.35,2020-06-17,0.37,2021-06-18,0.3,0.3175
-2018-06-19,0.27,2019-06-19,0.34,2020-06-18,0.33,2021-06-19,0.3,0.31
-2018-06-20,0.3,2019-06-20,0.4,2020-06-19,0.28,2021-06-20,0.31,0.3225
-2018-06-21,0.27,2019-06-21,0.43,2020-06-20,0.27,2021-06-21,0.32,0.3225
-2018-06-22,0.35,2019-06-22,0.35,2020-06-21,0.28,2021-06-22,0.29,0.3175
-2018-06-23,0.46,2019-06-23,0.29,2020-06-22,0.29,2021-06-23,0.28,0.33
-2018-06-24,0.37,2019-06-24,0.25,2020-06-23,0.29,2021-06-24,0.27,0.295
-2018-06-25,0.27,2019-06-25,0.28,2020-06-24,0.26,2021-06-25,0.28,0.2725
-2018-06-26,0.29,2019-06-26,0.31,2020-06-25,0.33,2021-06-26,0.29,0.305
-2018-06-27,0.3,2019-06-27,0.24,2020-06-26,0.31,2021-06-27,0.29,0.285
-2018-06-28,0.27,2019-06-28,0.27,2020-06-27,0.31,2021-06-28,0.29,0.285
-2018-06-29,0.39,2019-06-29,0.27,2020-06-28,0.33,2021-06-29,0.29,0.32
-2018-06-30,0.44,2019-06-30,0.29,2020-06-29,0.37,2021-06-30,0.32,0.355
-2018-07-01,0.35,2019-07-01,0.25,2020-06-30,0.3,2021-07-01,0.28,0.295
-2018-07-02,0.31,2019-07-02,0.27,2020-07-01,0.29,2021-07-02,0.29,0.29
-2018-07-03,0.3,2019-07-03,0.27,2020-07-02,0.29,2021-07-03,0.31,0.2925
-2018-07-04,0.29,2019-07-04,0.27,2020-07-03,0.28,2021-07-04,0.3,0.285
-2018-07-05,0.3,2019-07-05,0.27,2020-07-04,0.28,2021-07-05,0.3,0.2875
-2018-07-06,0.14,2019-07-06,0.27,2020-07-05,0.3,2021-07-06,0.28,0.2475
-2018-07-07,0.28,2019-07-07,0.27,2020-07-06,0.32,2021-07-07,0.29,0.29
-2018-07-08,0.29,2019-07-08,0.26,2020-07-07,0.28,2021-07-08,0.29,0.28
-2018-07-09,0.29,2019-07-09,0.22,2020-07-08,0.3,2021-07-09,0.31,0.28
-2018-07-10,0.32,2019-07-10,0.26,2020-07-09,0.29,2021-07-10,0.3,0.2925
-2018-07-11,0.29,2019-07-11,0.27,2020-07-10,0.3,2021-07-11,0.31,0.2925
-2018-07-12,0.29,2019-07-12,0.27,2020-07-11,0.3,2021-07-12,0.29,0.2875
-2018-07-13,0.16,2019-07-13,0.27,2020-07-12,0.3,2021-07-13,0.3,0.2575
-2018-07-14,0.28,2019-07-14,0.28,2020-07-13,0.29,2021-07-14,0.28,0.2825
-2018-07-15,0.28,2019-07-15,0.27,2020-07-14,0.27,2021-07-15,0.28,0.275
-2018-07-16,0.28,2019-07-16,0.27,2020-07-15,0.29,2021-07-16,0.26,0.275
-2018-07-17,0.28,2019-07-17,0.27,2020-07-16,0.28,2021-07-17,0.28,0.2775
-2018-07-18,0.27,2019-07-18,0.28,2020-07-17,0.28,2021-07-18,0.31,0.285
-2018-07-19,0.27,2019-07-19,0.27,2020-07-18,0.26,2021-07-19,0.28,0.27
-2018-07-20,0.29,2019-07-20,0.26,2020-07-19,0.28,2021-07-20,0.3,0.2825
-2018-07-21,0.28,2019-07-21,0.25,2020-07-20,0.27,2021-07-21,0.27,0.2675
-2018-07-22,0.28,2019-07-22,0.28,2020-07-21,0.26,2021-07-22,0.29,0.2775
-2018-07-23,0.28,2019-07-23,0.29,2020-07-22,0.25,2021-07-23,0.28,0.275
-2018-07-24,0.28,2019-07-24,0.28,2020-07-23,0.26,2021-07-24,0.18,0.25
-2018-07-25,0.27,2019-07-25,0.27,2020-07-24,0.26,2021-07-25,0.24,0.26
-2018-07-26,0.25,2019-07-26,0.27,2020-07-25,0.26,2021-07-26,0.25,0.2575
-2018-07-27,0.23,2019-07-27,0.27,2020-07-26,0.27,2021-07-27,0.17,0.235
-2018-07-28,0.2,2019-07-28,0.28,2020-07-27,0.27,2021-07-28,0.28,0.2575
-2018-07-29,0.22,2019-07-29,0.28,2020-07-28,0.26,2021-07-29,0.26,0.255
-2018-07-30,0.19,2019-07-30,0.26,2020-07-29,0.28,2021-07-30,0.24,0.2425
-2018-07-31,0.2,2019-07-31,0.26,2020-07-30,0.27,2021-07-31,0.28,0.2525
-2018-08-01,0.23,2019-08-01,0.26,2020-07-31,0.27,2021-08-01,0.28,0.26
-2018-08-02,0.25,2019-08-02,0.24,2020-08-01,0.25,2021-08-02,0.26,0.25
-2018-08-03,0.24,2019-08-03,0.26,2020-08-02,0.26,2021-08-03,0.26,0.255
-2018-08-04,0.25,2019-08-04,0.27,2020-08-03,0.26,2021-08-04,0.29,0.2675
-2018-08-05,0.18,2019-08-05,0.24,2020-08-04,0.26,2021-08-05,0.22,0.225
-2018-08-06,0.21,2019-08-06,0.27,2020-08-05,0.25,2021-08-06,0.24,0.2425
-2018-08-07,0.2,2019-08-07,0.26,2020-08-06,0.23,2021-08-07,0.21,0.225
-2018-08-08,0.21,2019-08-08,0.24,2020-08-07,0.26,2021-08-08,0.22,0.2325
-2018-08-09,0.19,2019-08-09,0.18,2020-08-08,0.25,2021-08-09,0.24,0.215
-2018-08-10,0.23,2019-08-10,0.13,2020-08-09,0.26,2021-08-10,0.23,0.2125
-2018-08-11,0.23,2019-08-11,0.24,2020-08-10,0.26,2021-08-11,0.2,0.2325
-2018-08-12,0.24,2019-08-12,0.23,2020-08-11,0.25,2021-08-12,0.2,0.23
-2018-08-13,0.2,2019-08-13,0.24,2020-08-12,0.25,2021-08-13,0.19,0.22
-2018-08-14,0.23,2019-08-14,0.25,2020-08-13,0.27,2021-08-14,0.22,0.2425
-2018-08-15,0.23,2019-08-15,0.27,2020-08-14,0.25,2021-08-15,0.22,0.2425
-2018-08-16,0.21,2019-08-16,0.28,2020-08-15,0.27,2021-08-16,0.18,0.235
-2018-08-17,0.23,2019-08-17,0.25,2020-08-16,0.13,2021-08-17,0.14,0.1875
-2018-08-18,0.21,2019-08-18,0.23,2020-08-17,0.09,2021-08-18,0.35,0.22
-2018-08-19,0.2,2019-08-19,0.22,2020-08-18,0.23,2021-08-19,0.25,0.225
-2018-08-20,0.19,2019-08-20,0.21,2020-08-19,0.12,2021-08-20,0.17,0.1725
-2018-08-21,0.2,2019-08-21,0.2,2020-08-20,0.11,2021-08-21,0.17,0.17
-2018-08-22,0.2,2019-08-22,0.26,2020-08-21,0.08,2021-08-22,0.19,0.1825
-2018-08-23,0.21,2019-08-23,0.24,2020-08-22,0.16,2021-08-23,0.22,0.2075
-2018-08-24,0.18,2019-08-24,0.23,2020-08-23,0.13,2021-08-24,0.22,0.19
-2018-08-25,0.19,2019-08-25,0.23,2020-08-24,0.18,2021-08-25,0.21,0.2025
-2018-08-26,0.18,2019-08-26,0.24,2020-08-25,0.16,2021-08-26,0.22,0.2
-2018-08-27,0.19,2019-08-27,0.23,2020-08-26,0.16,2021-08-27,0.27,0.2125
-2018-08-28,0.21,2019-08-28,0.22,2020-08-27,0.2,2021-08-28,0.2,0.2075
-2018-08-29,0.21,2019-08-29,0.23,2020-08-28,0.22,2021-08-29,0.2,0.215
-2018-08-30,0.2,2019-08-30,0.21,2020-08-29,0.15,2021-08-30,0.21,0.1925
-2018-08-31,0.2,2019-08-31,0.21,2020-08-30,0.17,2021-08-31,0.22,0.2
-2018-09-01,0.22,2019-09-01,0.23,2020-08-31,0.24,2021-09-01,0.21,0.225
-2018-09-02,0.21,2019-09-02,0.22,2020-09-01,0.28,2021-09-02,0.18,0.2225
-2018-09-03,0.19,2019-09-03,0.2,2020-09-02,0.18,2021-09-03,0.19,0.19
-2018-09-04,0.21,2019-09-04,0.16,2020-09-03,0.18,2021-09-04,0.2,0.1875
-2018-09-05,0.22,2019-09-05,0.24,2020-09-04,0.16,2021-09-05,0.21,0.2075
-2018-09-06,0.2,2019-09-06,0.2,2020-09-05,0.21,2021-09-06,0.22,0.2075
-2018-09-07,0.21,2019-09-07,0.19,2020-09-06,0.22,2021-09-07,0.24,0.215
-2018-09-08,0.19,2019-09-08,0.18,2020-09-07,0.2,2021-09-08,0.18,0.1875
-2018-09-09,0.2,2019-09-09,0.18,2020-09-08,0.37,2021-09-09,0.15,0.225
-2018-09-10,0.19,2019-09-10,0.17,2020-09-09,0.08,2021-09-10,0.18,0.155
-2018-09-11,0.22,2019-09-11,0.2,2020-09-10,0.09,2021-09-11,0.19,0.175
-2018-09-12,0.17,2019-09-12,0.19,2020-09-11,0.13,2021-09-12,0.18,0.1675
-2018-09-13,0.17,2019-09-13,0.19,2020-09-12,0.15,2021-09-13,0.18,0.1725
-2018-09-14,0.17,2019-09-14,0.21,2020-09-13,0.15,2021-09-14,0.22,0.1875
-2018-09-15,0.19,2019-09-15,0.21,2020-09-14,0.15,2021-09-15,0.18,0.1825
-2018-09-16,0.17,2019-09-16,0.15,2020-09-15,0.17,2021-09-16,0.17,0.165
-2018-09-17,0.17,2019-09-17,0.17,2020-09-16,0.15,2021-09-17,0.19,0.17
-2018-09-18,0.16,2019-09-18,0.1,2020-09-17,0.2,2021-09-18,0.06,0.13
-2018-09-19,0.21,2019-09-19,0.17,2020-09-18,0.16,2021-09-19,0.18,0.18
-2018-09-20,0.22,2019-09-20,0.2,2020-09-19,0.17,2021-09-20,0.28,0.2175
-2018-09-21,0.19,2019-09-21,0.2,2020-09-20,0.17,2021-09-21,0.2,0.19
-2018-09-22,0.17,2019-09-22,0.13,2020-09-21,0.13,2021-09-22,0.18,0.1525
-2018-09-23,0.17,2019-09-23,0.24,2020-09-22,0.15,2021-09-23,0.26,0.205
-2018-09-24,0.22,2019-09-24,0.26,2020-09-23,0.15,2021-09-24,0.19,0.205
-2018-09-25,0.25,2019-09-25,0.3,2020-09-24,0.14,2021-09-25,0.19,0.22
-2018-09-26,0.17,2019-09-26,0.23,2020-09-25,0.14,2021-09-26,0.16,0.175
-2018-09-27,0.17,2019-09-27,0.05,2020-09-26,0.22,2021-09-27,0.13,0.1425
-2018-09-28,0.17,2019-09-28,0.17,2020-09-27,0.32,2021-09-28,0.21,0.2175
-2018-09-29,0.13,2019-09-29,0.1,2020-09-28,0.29,2021-09-29,0.22,0.185
-2018-09-30,0.14,2019-09-30,0.12,2020-09-29,0.13,2021-09-30,0.21,0.15
-2018-10-01,0.07,2019-10-01,0.16,2020-09-30,0.19,2021-10-01,0.15,0.1425
-2018-10-02,0.1,2019-10-02,0.13,2020-10-01,0.18,2021-10-02,0.15,0.14
-2018-10-03,0.1,2019-10-03,0.16,2020-10-02,0.12,2021-10-03,0.14,0.13
-2018-10-04,0.03,2019-10-04,0.16,2020-10-03,0.11,2021-10-04,0.16,0.115
-2018-10-05,0.11,2019-10-05,0.2,2020-10-04,0.11,2021-10-05,0.17,0.1475
-2018-10-06,0.18,2019-10-06,0.18,2020-10-05,0.15,2021-10-06,0.09,0.15
-2018-10-07,0.26,2019-10-07,0.14,2020-10-06,0.14,2021-10-07,0.1,0.16
-2018-10-08,0.22,2019-10-08,0.15,2020-10-07,0.14,2021-10-08,0.13,0.16
-2018-10-09,0.16,2019-10-09,0.29,2020-10-08,0.09,2021-10-09,0.14,0.17
-2018-10-10,0.14,2019-10-10,0.22,2020-10-09,0.11,2021-10-10,0.16,0.1575
-2018-10-11,0.17,2019-10-11,0.14,2020-10-10,0.07,2021-10-11,0.25,0.1575
-2018-10-12,0.2,2019-10-12,0.12,2020-10-11,0.17,2021-10-12,0.19,0.17
-2018-10-13,0.21,2019-10-13,0.12,2020-10-12,0.15,2021-10-13,0.11,0.1475
-2018-10-14,0.24,2019-10-14,0.12,2020-10-13,0.13,2021-10-14,0.2,0.1725
-2018-10-15,0.2,2019-10-15,0.12,2020-10-14,0.25,2021-10-15,0.12,0.1725
-2018-10-16,0.19,2019-10-16,0.11,2020-10-15,0.31,2021-10-16,0.12,0.1825
-2018-10-17,0.11,2019-10-17,0.13,2020-10-16,0.26,2021-10-17,0.12,0.155
-2018-10-18,0.11,2019-10-18,0.1,2020-10-17,0.18,2021-10-18,0.05,0.11
-2018-10-19,0.11,2019-10-19,0.08,2020-10-18,0.15,2021-10-19,0.09,0.1075
-2018-10-20,0.12,2019-10-20,0.12,2020-10-19,0.13,2021-10-20,0,0.0925
-2018-10-21,0.11,2019-10-21,0.18,2020-10-20,0.22,2021-10-21,0.01,0.13
-2018-10-22,0.11,2019-10-22,0.18,2020-10-21,0.2,2021-10-22,0.08,0.1425
-2018-10-23,0.07,2019-10-23,0.24,2020-10-22,0.29,2021-10-23,0.01,0.1525
-2018-10-24,0.08,2019-10-24,0.21,2020-10-23,0.16,2021-10-24,0,0.1125
-2018-10-25,0.11,2019-10-25,0.11,2020-10-24,0.09,2021-10-25,0.08,0.0975
-2018-10-26,0.09,2019-10-26,0.2,2020-10-25,0.2,2021-10-26,0.05,0.135
-2018-10-27,0.1,2019-10-27,0.22,2020-10-26,0.16,2021-10-27,0.07,0.1375
-2018-10-28,0.06,2019-10-28,0.12,2020-10-27,0.15,2021-10-28,0.09,0.105
-2018-10-29,0.12,2019-10-29,0.18,2020-10-28,0.09,2021-10-29,0.07,0.115
-2018-10-30,0.14,2019-10-30,0.13,2020-10-29,0.09,2021-10-30,0.07,0.1075
-2018-10-31,0.16,2019-10-31,0.1,2020-10-30,0.09,2021-10-31,0.05,0.1
-2018-11-01,0.14,2019-11-01,0.09,2020-10-31,0.1,2021-11-01,0.02,0.0875
-2018-11-02,0.11,2019-11-02,0.1,2020-11-01,0.09,2021-11-02,0.07,0.0925
-2018-11-03,0.2,2019-11-03,0.09,2020-11-02,0.1,2021-11-03,0.08,0.1175
-2018-11-04,0.12,2019-11-04,0.09,2020-11-03,0.09,2021-11-04,0.1,0.1
-2018-11-05,0.17,2019-11-05,0.09,2020-11-04,0.09,2021-11-05,0.01,0.09
-2018-11-06,0.18,2019-11-06,0.09,2020-11-05,0.1,2021-11-06,0.02,0.0975
-2018-11-07,0.17,2019-11-07,0.08,2020-11-06,0.13,2021-11-07,0.08,0.115
-2018-11-08,0.2,2019-11-08,0.09,2020-11-07,0.07,2021-11-08,0.04,0.1
-2018-11-09,0.1,2019-11-09,0.08,2020-11-08,0.14,2021-11-09,0,0.08
-2018-11-10,0.11,2019-11-10,0.09,2020-11-09,0.11,2021-11-10,0.03,0.085
-2018-11-11,0.16,2019-11-11,0.12,2020-11-10,0.07,2021-11-11,0.08,0.1075
-2018-11-12,0.11,2019-11-12,0.07,2020-11-11,0.07,2021-11-12,0.02,0.0675
-2018-11-13,0.04,2019-11-13,0.06,2020-11-12,0.06,2021-11-13,0.05,0.0525
-2018-11-14,0.05,2019-11-14,0.07,2020-11-13,0.04,2021-11-14,0.03,0.0475
-2018-11-15,0.06,2019-11-15,0.08,2020-11-14,0.04,2021-11-15,0,0.045
-2018-11-16,0.05,2019-11-16,0.11,2020-11-15,0.06,2021-11-16,0.1,0.08
-2018-11-17,0.04,2019-11-17,0.07,2020-11-16,0.07,2021-11-17,0.07,0.0625
-2018-11-18,0.05,2019-11-18,0.08,2020-11-17,0.01,2021-11-18,0,0.035
-2018-11-19,0.07,2019-11-19,0.14,2020-11-18,0.07,2021-11-19,0.01,0.0725
-2018-11-20,0.06,2019-11-20,0.23,2020-11-19,0.08,2021-11-20,0.1,0.1175
-2018-11-21,0,2019-11-21,0.08,2020-11-20,0.1,2021-11-21,0.09,0.0675
-2018-11-22,0.04,2019-11-22,0.07,2020-11-21,0.11,2021-11-22,0.06,0.07
-2018-11-23,0,2019-11-23,0.09,2020-11-22,0.05,2021-11-23,0.09,0.0575
-2018-11-24,0.06,2019-11-24,0.06,2020-11-23,0.08,2021-11-24,0.1,0.075
-2018-11-25,0.07,2019-11-25,0.12,2020-11-24,0.08,2021-11-25,0.05,0.08
-2018-11-26,0.02,2019-11-26,0.02,2020-11-25,0.09,2021-11-26,0.06,0.0475
-2018-11-27,0,2019-11-27,0.04,2020-11-26,0.1,2021-11-27,0.06,0.05
-2018-11-28,0.01,2019-11-28,0.04,2020-11-27,0.08,2021-11-28,0.06,0.0475
-2018-11-29,0,2019-11-29,0.04,2020-11-28,0.07,2021-11-29,0.06,0.0425
-2018-11-30,0.04,2019-11-30,0,2020-11-29,0.05,2021-11-30,0.07,0.04
-2018-12-01,0.05,2019-12-01,0,2020-11-30,0.06,2021-12-01,0.07,0.045
-2018-12-02,0.05,2019-12-02,0.02,2020-12-01,0.11,2021-12-02,0.06,0.06
-2018-12-03,0.04,2019-12-03,0.08,2020-12-02,0.09,2021-12-03,0.05,0.065
-2018-12-04,0.06,2019-12-04,0.07,2020-12-03,0.05,2021-12-04,0.05,0.0575
-2018-12-05,0.02,2019-12-05,0.05,2020-12-04,0.08,2021-12-05,0.04,0.0475
-2018-12-06,0.07,2019-12-06,0.08,2020-12-05,0.04,2021-12-06,0,0.0475
-2018-12-07,0.04,2019-12-07,0.11,2020-12-06,0.11,2021-12-07,0.05,0.0775
-2018-12-08,0.06,2019-12-08,0.07,2020-12-07,0.15,2021-12-08,0,0.07
-2018-12-09,0.02,2019-12-09,0.05,2020-12-08,0.09,2021-12-09,0.07,0.0575
-2018-12-10,0.07,2019-12-10,0.04,2020-12-09,0.11,2021-12-10,0.06,0.07
-2018-12-11,0.06,2019-12-11,0.04,2020-12-10,0.1,2021-12-11,0.01,0.0525
-2018-12-12,0.08,2019-12-12,0.04,2020-12-11,0.03,2021-12-12,0,0.0375
-2018-12-13,0.05,2019-12-13,0.07,2020-12-12,0.05,2021-12-13,0.01,0.045
-2018-12-14,0.03,2019-12-14,0.05,2020-12-13,0,2021-12-14,0.03,0.0275
-2018-12-15,0,2019-12-15,0.04,2020-12-14,0.06,2021-12-15,0.01,0.0275
-2018-12-16,0,2019-12-16,0.06,2020-12-15,0.03,2021-12-16,0.01,0.025
-2018-12-17,0.03,2019-12-17,0.04,2020-12-16,0.01,2021-12-17,0.04,0.03
-2018-12-18,0.03,2019-12-18,0.05,2020-12-17,0.06,2021-12-18,0.03,0.0425
-2018-12-19,0.05,2019-12-19,0.04,2020-12-18,0.07,2021-12-19,0,0.04
-2018-12-20,0,2019-12-20,0.04,2020-12-19,0.04,2021-12-20,0.02,0.025
-2018-12-21,0.04,2019-12-21,0.06,2020-12-20,0.03,2021-12-21,0.01,0.035
-2018-12-22,0.03,2019-12-22,0.06,2020-12-21,0.04,2021-12-22,0,0.0325
-2018-12-23,0,2019-12-23,0.07,2020-12-22,0.09,2021-12-23,0,0.04
-2018-12-24,0,2019-12-24,0.04,2020-12-23,0.11,2021-12-24,0.06,0.0525
-2018-12-25,0.08,2019-12-25,0.05,2020-12-24,0.04,2021-12-25,0.02,0.0475
-2018-12-26,0.06,2019-12-26,0.07,2020-12-25,0,2021-12-26,0.03,0.04
-2018-12-27,0.08,2019-12-27,0.06,2020-12-26,0.04,2021-12-27,0.05,0.0575
-2018-12-28,0.09,2019-12-28,0.05,2020-12-27,0.04,2021-12-28,0.01,0.0475
-2018-12-29,0.06,2019-12-29,0.05,2020-12-28,0.04,2021-12-29,0.01,0.04
-2018-12-30,0.05,2019-12-30,0.09,2020-12-29,0.08,2021-12-30,0.02,0.06
-2018-12-31,0.11,2019-12-31,0.06,2020-12-30,0.04,2021-12-31,0.04,0.0625
+Year_2023,Year_2023_ET,Year_2022,Year_2022_ET,Year_2021,Year_2021_ET,Year_2020,Year_2020_ET,Year_2019,Year_2019_ET,Average
+2023-01-01,0.1,2022-01-01,0.06,2021-01-01,0.04,2020-01-01,0.04,2019-01-01,0.1,0.068
+2023-01-02,0.01,2022-01-02,0.04,2021-01-02,0.02,2020-01-02,0.06,2019-01-02,0.05,0.036
+2023-01-03,0,2022-01-03,0.03,2021-01-03,0.04,2020-01-03,0.05,2019-01-03,0.04,0.032
+2023-01-04,0.02,2022-01-04,0.03,2021-01-04,0.02,2020-01-04,0.04,2019-01-04,0.05,0.03200000000000001
+2023-01-05,0.01,2022-01-05,0.05,2021-01-05,0.03,2020-01-05,0.07,2019-01-05,0.02,0.036
+2023-01-06,0.03,2022-01-06,0,2021-01-06,0.03,2020-01-06,0.06,2019-01-06,0,0.024
+2023-01-07,0.02,2022-01-07,0.02,2021-01-07,0.06,2020-01-07,0.02,2019-01-07,0.04,0.032
+2023-01-08,0.04,2022-01-08,0.02,2021-01-08,0.04,2020-01-08,0.03,2019-01-08,0,0.026000000000000002
+2023-01-09,0.02,2022-01-09,0.03,2021-01-09,0.06,2020-01-09,0.02,2019-01-09,0.04,0.034
+2023-01-10,0.02,2022-01-10,0.06,2021-01-10,0.02,2020-01-10,0.06,2019-01-10,0.01,0.034
+2023-01-11,0,2022-01-11,0.05,2021-01-11,0.05,2020-01-11,0.07,2019-01-11,0.02,0.038
+2023-01-12,0.05,2022-01-12,0.05,2021-01-12,0.02,2020-01-12,0.04,2019-01-12,0.04,0.04
+2023-01-13,0,2022-01-13,0.04,2021-01-13,0.07,2020-01-13,0.06,2019-01-13,0.05,0.044000000000000004
+2023-01-14,0.02,2022-01-14,0.06,2021-01-14,0.07,2020-01-14,0.07,2019-01-14,0.01,0.046000000000000006
+2023-01-15,0.01,2022-01-15,0.02,2021-01-15,0.05,2020-01-15,0.05,2019-01-15,0,0.026000000000000002
+2023-01-16,0.06,2022-01-16,0.05,2021-01-16,0.08,2020-01-16,0.02,2019-01-16,0,0.041999999999999996
+2023-01-17,0.06,2022-01-17,0.04,2021-01-17,0.07,2020-01-17,0.02,2019-01-17,0.05,0.048
+2023-01-18,0.04,2022-01-18,0.06,2021-01-18,0.21,2020-01-18,0.05,2019-01-18,0.04,0.07999999999999999
+2023-01-19,0.05,2022-01-19,0.06,2021-01-19,0.17,2020-01-19,0,2019-01-19,0.06,0.068
+2023-01-20,0.07,2022-01-20,0.06,2021-01-20,0.1,2020-01-20,0.01,2019-01-20,0.03,0.054000000000000006
+2023-01-21,0.06,2022-01-21,0.13,2021-01-21,0.08,2020-01-21,0.04,2019-01-21,0.08,0.078
+2023-01-22,0.11,2022-01-22,0.11,2021-01-22,0.04,2020-01-22,0.07,2019-01-22,0.09,0.084
+2023-01-23,0.11,2022-01-23,0.05,2021-01-23,0.06,2020-01-23,0.03,2019-01-23,0.07,0.064
+2023-01-24,0.06,2022-01-24,0.06,2021-01-24,0.04,2020-01-24,0.07,2019-01-24,0.07,0.06000000000000001
+2023-01-25,0.08,2022-01-25,0.08,2021-01-25,0.08,2020-01-25,0.03,2019-01-25,0.08,0.07
+2023-01-26,0.07,2022-01-26,0.07,2021-01-26,0.07,2020-01-26,0.08,2019-01-26,0.05,0.068
+2023-01-27,0.06,2022-01-27,0.07,2021-01-27,0.01,2020-01-27,0.08,2019-01-27,0.07,0.05800000000000001
+2023-01-28,0.07,2022-01-28,0.07,2021-01-28,0.01,2020-01-28,0.04,2019-01-28,0.04,0.046000000000000006
+2023-01-29,0.01,2022-01-29,0.07,2021-01-29,0.06,2020-01-29,0.09,2019-01-29,0.07,0.06000000000000001
+2023-01-30,0.13,2022-01-30,0.07,2021-01-30,0.07,2020-01-30,0.06,2019-01-30,0.06,0.078
+2023-01-31,0.08,2022-01-31,0.07,2021-01-31,0.07,2020-01-31,0.07,2019-01-31,0.07,0.07200000000000001
+2023-02-01,0.07,2022-02-01,0.17,2021-02-01,0.09,2020-02-01,0.05,2019-02-01,0.02,0.08
+2023-02-02,0.04,2022-02-02,0.15,2021-02-02,0.05,2020-02-02,0.1,2019-02-02,0.06,0.07999999999999999
+2023-02-03,0.02,2022-02-03,0.08,2021-02-03,0.07,2020-02-03,0.13,2019-02-03,0.05,0.07
+2023-02-04,0.02,2022-02-04,0.07,2021-02-04,0.1,2020-02-04,0.1,2019-02-04,0.04,0.066
+2023-02-05,0.07,2022-02-05,0.08,2021-02-05,0.09,2020-02-05,0.09,2019-02-05,0.07,0.08
+2023-02-06,0.09,2022-02-06,0.08,2021-02-06,0.09,2020-02-06,0.08,2019-02-06,0.08,0.084
+2023-02-07,0.07,2022-02-07,0.09,2021-02-07,0.09,2020-02-07,0.09,2019-02-07,0.06,0.07999999999999999
+2023-02-08,0.08,2022-02-08,0.09,2021-02-08,0.07,2020-02-08,0.09,2019-02-08,0.04,0.07399999999999998
+2023-02-09,0.08,2022-02-09,0.08,2021-02-09,0.08,2020-02-09,0.19,2019-02-09,0.03,0.092
+2023-02-10,0.1,2022-02-10,0.09,2021-02-10,0.1,2020-02-10,0.19,2019-02-10,0.09,0.11400000000000002
+2023-02-11,0.06,2022-02-11,0.09,2021-02-11,0.03,2020-02-11,0.14,2019-02-11,0.07,0.078
+2023-02-12,0.11,2022-02-12,0.09,2021-02-12,0.1,2020-02-12,0.11,2019-02-12,0.01,0.084
+2023-02-13,0.11,2022-02-13,0.1,2021-02-13,0.12,2020-02-13,0.1,2019-02-13,0.02,0.09000000000000001
+2023-02-14,0.11,2022-02-14,0.09,2021-02-14,0.05,2020-02-14,0.09,2019-02-14,0.05,0.07799999999999999
+2023-02-15,0.12,2022-02-15,0.14,2021-02-15,0.05,2020-02-15,0.09,2019-02-15,0.05,0.09
+2023-02-16,0.08,2022-02-16,0.14,2021-02-16,0.08,2020-02-16,0.12,2019-02-16,0.09,0.10200000000000001
+2023-02-17,0.11,2022-02-17,0.1,2021-02-17,0.16,2020-02-17,0.18,2019-02-17,0.08,0.126
+2023-02-18,0.09,2022-02-18,0.1,2021-02-18,0.09,2020-02-18,0.13,2019-02-18,0.12,0.10600000000000001
+2023-02-19,0.11,2022-02-19,0.1,2021-02-19,0.04,2020-02-19,0.12,2019-02-19,0.11,0.096
+2023-02-20,0.09,2022-02-20,0.09,2021-02-20,0.12,2020-02-20,0.12,2019-02-20,0.08,0.1
+2023-02-21,0.14,2022-02-21,0.12,2021-02-21,0.13,2020-02-21,0.14,2019-02-21,0.14,0.134
+2023-02-22,0.13,2022-02-22,0.09,2021-02-22,0.12,2020-02-22,0.13,2019-02-22,0.13,0.12
+2023-02-23,0.08,2022-02-23,0.13,2021-02-23,0.2,2020-02-23,0.12,2019-02-23,0.08,0.122
+2023-02-24,0.04,2022-02-24,0.1,2021-02-24,0.23,2020-02-24,0.15,2019-02-24,0.07,0.11800000000000002
+2023-02-25,0.03,2022-02-25,0.11,2021-02-25,0.17,2020-02-25,0.16,2019-02-25,0.04,0.10200000000000002
+2023-02-26,0.02,2022-02-26,0.08,2021-02-26,0.14,2020-02-26,0.13,2019-02-26,0.01,0.076
+2023-02-27,0.02,2022-02-27,0.12,2021-02-27,0.17,2020-02-27,0.15,2019-02-27,0.04,0.09999999999999999
+2023-02-28,0.02,2022-02-28,0.12,2021-02-28,0.15,2020-02-28,0.12,2019-02-28,0.1,0.10200000000000001
+2023-03-01,0.14,2022-03-01,0.12,2021-03-01,0.13,2020-03-01,0.17,2019-03-01,0.05,0.12200000000000003
+2023-03-02,0.11,2022-03-02,0.13,2021-03-02,0.13,2020-03-02,0.15,2019-03-02,0.01,0.10600000000000001
+2023-03-03,0.1,2022-03-03,0.08,2021-03-03,0.12,2020-03-03,0.23,2019-03-03,0.05,0.11600000000000002
+2023-03-04,0.09,2022-03-04,0.13,2021-03-04,0.12,2020-03-04,0.15,2019-03-04,0.07,0.11200000000000002
+2023-03-05,0.11,2022-03-05,0.08,2021-03-05,0.13,2020-03-05,0.15,2019-03-05,0.02,0.098
+2023-03-06,0.07,2022-03-06,0.12,2021-03-06,0.13,2020-03-06,0.13,2019-03-06,0.07,0.10400000000000001
+2023-03-07,0.1,2022-03-07,0.18,2021-03-07,0.15,2020-03-07,0.11,2019-03-07,0.05,0.11800000000000002
+2023-03-08,0.12,2022-03-08,0.14,2021-03-08,0.13,2020-03-08,0.06,2019-03-08,0.15,0.12
+2023-03-09,0.01,2022-03-09,0.15,2021-03-09,0.09,2020-03-09,0.06,2019-03-09,0.07,0.076
+2023-03-10,0.08,2022-03-10,0.22,2021-03-10,0.08,2020-03-10,0.13,2019-03-10,0.07,0.11600000000000002
+2023-03-11,0.03,2022-03-11,0.15,2021-03-11,0.13,2020-03-11,0.16,2019-03-11,0.16,0.126
+2023-03-12,0.05,2022-03-12,0.15,2021-03-12,0.16,2020-03-12,0.15,2019-03-12,0.15,0.132
+2023-03-13,0.11,2022-03-13,0.15,2021-03-13,0.16,2020-03-13,0.19,2019-03-13,0.19,0.16
+2023-03-14,0.05,2022-03-14,0.14,2021-03-14,0.1,2020-03-14,0.16,2019-03-14,0.17,0.12400000000000003
+2023-03-15,0.12,2022-03-15,0.11,2021-03-15,0.1,2020-03-15,0.04,2019-03-15,0.15,0.10399999999999998
+2023-03-16,0.14,2022-03-16,0.16,2021-03-16,0.14,2020-03-16,0.06,2019-03-16,0.15,0.13
+2023-03-17,0.13,2022-03-17,0.16,2021-03-17,0.09,2020-03-17,0.03,2019-03-17,0.15,0.11200000000000002
+2023-03-18,0.09,2022-03-18,0.15,2021-03-18,0.05,2020-03-18,0.09,2019-03-18,0.16,0.10800000000000001
+2023-03-19,0.03,2022-03-19,0.06,2021-03-19,0.07,2020-03-19,0.07,2019-03-19,0.13,0.072
+2023-03-20,0.14,2022-03-20,0.21,2021-03-20,0.16,2020-03-20,0.13,2019-03-20,0.12,0.152
+2023-03-21,0.04,2022-03-21,0.2,2021-03-21,0.19,2020-03-21,0.13,2019-03-21,0.15,0.14200000000000002
+2023-03-22,0.09,2022-03-22,0.19,2021-03-22,0.16,2020-03-22,0.13,2019-03-22,0.04,0.12200000000000003
+2023-03-23,0.16,2022-03-23,0.18,2021-03-23,0.24,2020-03-23,0.14,2019-03-23,0.15,0.174
+2023-03-24,0.15,2022-03-24,0.18,2021-03-24,0.2,2020-03-24,0.15,2019-03-24,0.13,0.162
+2023-03-25,0.17,2022-03-25,0.18,2021-03-25,0.17,2020-03-25,0.09,2019-03-25,0.06,0.13399999999999998
+2023-03-26,0.16,2022-03-26,0.19,2021-03-26,0.22,2020-03-26,0.16,2019-03-26,0.14,0.174
+2023-03-27,0.15,2022-03-27,0.13,2021-03-27,0.19,2020-03-27,0.15,2019-03-27,0.07,0.13799999999999998
+2023-03-28,0.02,2022-03-28,0.08,2021-03-28,0.21,2020-03-28,0.17,2019-03-28,0.11,0.118
+2023-03-29,0.05,2022-03-29,0.17,2021-03-29,0.23,2020-03-29,0.07,2019-03-29,0.16,0.136
+2023-03-30,0.14,2022-03-30,0.13,2021-03-30,0.29,2020-03-30,0.11,2019-03-30,0.18,0.17
+2023-03-31,0.14,2022-03-31,0.17,2021-03-31,0.21,2020-03-31,0.17,2019-03-31,0.19,0.17600000000000002
+2023-04-01,0.16,2022-04-01,0.2,2021-04-01,0.22,2020-04-01,0.17,2019-04-01,0.07,0.164
+2023-04-02,0.18,2022-04-02,0.21,2021-04-02,0.24,2020-04-02,0.18,2019-04-02,0.11,0.184
+2023-04-03,0.17,2022-04-03,0.24,2021-04-03,0.21,2020-04-03,0.22,2019-04-03,0.1,0.188
+2023-04-04,0.16,2022-04-04,0.21,2021-04-04,0.18,2020-04-04,0.2,2019-04-04,0.07,0.164
+2023-04-05,0.15,2022-04-05,0.22,2021-04-05,0.19,2020-04-05,0.06,2019-04-05,0.05,0.13400000000000004
+2023-04-06,0.14,2022-04-06,0.22,2021-04-06,0.2,2020-04-06,0.04,2019-04-06,0.13,0.14600000000000002
+2023-04-07,0.06,2022-04-07,0.2,2021-04-07,0.22,2020-04-07,0.07,2019-04-07,0.17,0.14400000000000002
+2023-04-08,0.15,2022-04-08,0.24,2021-04-08,0.22,2020-04-08,0.17,2019-04-08,0.13,0.182
+2023-04-09,0.18,2022-04-09,0.29,2021-04-09,0.23,2020-04-09,0.12,2019-04-09,0.21,0.20600000000000002
+2023-04-10,0.19,2022-04-10,0.28,2021-04-10,0.22,2020-04-10,0.08,2019-04-10,0.24,0.202
+2023-04-11,0.19,2022-04-11,0.19,2021-04-11,0.22,2020-04-11,0.18,2019-04-11,0.14,0.184
+2023-04-12,0.19,2022-04-12,0.18,2021-04-12,0.23,2020-04-12,0.18,2019-04-12,0.23,0.202
+2023-04-13,0.24,2022-04-13,0.15,2021-04-13,0.23,2020-04-13,0.17,2019-04-13,0.19,0.196
+2023-04-14,0.18,2022-04-14,0.06,2021-04-14,0.21,2020-04-14,0.21,2019-04-14,0.2,0.172
+2023-04-15,0.18,2022-04-15,0.19,2021-04-15,0.22,2020-04-15,0.26,2019-04-15,0.08,0.186
+2023-04-16,0.2,2022-04-16,0.15,2021-04-16,0.24,2020-04-16,0.22,2019-04-16,0.17,0.196
+2023-04-17,0.16,2022-04-17,0.2,2021-04-17,0.26,2020-04-17,0.19,2019-04-17,0.2,0.202
+2023-04-18,0.19,2022-04-18,0.19,2021-04-18,0.28,2020-04-18,0.18,2019-04-18,0.21,0.21000000000000002
+2023-04-19,0.18,2022-04-19,0.16,2021-04-19,0.27,2020-04-19,0.19,2019-04-19,0.21,0.202
+2023-04-20,0.21,2022-04-20,0.2,2021-04-20,0.23,2020-04-20,0.19,2019-04-20,0.21,0.20800000000000002
+2023-04-21,0.22,2022-04-21,0.12,2021-04-21,0.24,2020-04-21,0.16,2019-04-21,0.23,0.194
+2023-04-22,0.24,2022-04-22,0.14,2021-04-22,0.21,2020-04-22,0.23,2019-04-22,0.27,0.21799999999999997
+2023-04-23,0.22,2022-04-23,0.23,2021-04-23,0.21,2020-04-23,0.24,2019-04-23,0.24,0.22800000000000004
+2023-04-24,0.24,2022-04-24,0.23,2021-04-24,0.19,2020-04-24,0.24,2019-04-24,0.23,0.22599999999999998
+2023-04-25,0.26,2022-04-25,0.24,2021-04-25,0.09,2020-04-25,0.27,2019-04-25,0.25,0.22199999999999998
+2023-04-26,0.23,2022-04-26,0.25,2021-04-26,0.2,2020-04-26,0.25,2019-04-26,0.24,0.23399999999999999
+2023-04-27,0.23,2022-04-27,0.24,2021-04-27,0.26,2020-04-27,0.28,2019-04-27,0.24,0.25
+2023-04-28,0.25,2022-04-28,0.23,2021-04-28,0.27,2020-04-28,0.27,2019-04-28,0.21,0.246
+2023-04-29,0.24,2022-04-29,0.25,2021-04-29,0.27,2020-04-29,0.25,2019-04-29,0.23,0.248
+2023-04-30,0.24,2022-04-30,0.26,2021-04-30,0.26,2020-04-30,0.22,2019-04-30,0.22,0.24
+2023-05-01,0.06,2022-05-01,0.27,2021-05-01,0.27,2020-05-01,0.29,2019-05-01,0.23,0.22400000000000003
+2023-05-02,0.1,2022-05-02,0.24,2021-05-02,0.3,2020-05-02,0.28,2019-05-02,0.24,0.23199999999999998
+2023-05-03,0.1,2022-05-03,0.29,2021-05-03,0.33,2020-05-03,0.21,2019-05-03,0.24,0.23399999999999999
+2023-05-04,0.09,2022-05-04,0.25,2021-05-04,0.29,2020-05-04,0.24,2019-05-04,0.24,0.22199999999999998
+2023-05-05,0.13,2022-05-05,0.2,2021-05-05,0.26,2020-05-05,0.25,2019-05-05,0.23,0.21400000000000002
+2023-05-06,0.13,2022-05-06,0.16,2021-05-06,0.26,2020-05-06,0.29,2019-05-06,0.2,0.20800000000000002
+2023-05-07,0.21,2022-05-07,0.24,2021-05-07,0.25,2020-05-07,0.28,2019-05-07,0.21,0.238
+2023-05-08,0.18,2022-05-08,0.24,2021-05-08,0.34,2020-05-08,0.3,2019-05-08,0.22,0.256
+2023-05-09,0.21,2022-05-09,0.22,2021-05-09,0.33,2020-05-09,0.27,2019-05-09,0.21,0.248
+2023-05-10,0.22,2022-05-10,0.2,2021-05-10,0.33,2020-05-10,0.27,2019-05-10,0.23,0.25
+2023-05-11,0.22,2022-05-11,0.22,2021-05-11,0.29,2020-05-11,0.25,2019-05-11,0.22,0.24
+2023-05-12,0.24,2022-05-12,0.23,2021-05-12,0.29,2020-05-12,0.18,2019-05-12,0.25,0.238
+2023-05-13,0.26,2022-05-13,0.24,2021-05-13,0.26,2020-05-13,0.19,2019-05-13,0.27,0.244
+2023-05-14,0.26,2022-05-14,0.26,2021-05-14,0.25,2020-05-14,0.12,2019-05-14,0.17,0.21200000000000002
+2023-05-15,0.24,2022-05-15,0.27,2021-05-15,0.19,2020-05-15,0.13,2019-05-15,0.04,0.174
+2023-05-16,0.26,2022-05-16,0.25,2021-05-16,0.22,2020-05-16,0.25,2019-05-16,0.11,0.21800000000000003
+2023-05-17,0.26,2022-05-17,0.28,2021-05-17,0.23,2020-05-17,0.28,2019-05-17,0.22,0.254
+2023-05-18,0.26,2022-05-18,0.3,2021-05-18,0.31,2020-05-18,0.15,2019-05-18,0.04,0.21200000000000002
+2023-05-19,0.27,2022-05-19,0.28,2021-05-19,0.29,2020-05-19,0.22,2019-05-19,0.11,0.23400000000000004
+2023-05-20,0.24,2022-05-20,0.35,2021-05-20,0.25,2020-05-20,0.18,2019-05-20,0.21,0.246
+2023-05-21,0.24,2022-05-21,0.29,2021-05-21,0.28,2020-05-21,0.23,2019-05-21,0.16,0.24
+2023-05-22,0.25,2022-05-22,0.29,2021-05-22,0.25,2020-05-22,0.26,2019-05-22,0.23,0.256
+2023-05-23,0.23,2022-05-23,0.31,2021-05-23,0.24,2020-05-23,0.27,2019-05-23,0.24,0.258
+2023-05-24,0.23,2022-05-24,0.31,2021-05-24,0.28,2020-05-24,0.32,2019-05-24,0.24,0.276
+2023-05-25,0.23,2022-05-25,0.3,2021-05-25,0.27,2020-05-25,0.31,2019-05-25,0.2,0.262
+2023-05-26,0.23,2022-05-26,0.21,2021-05-26,0.28,2020-05-26,0.3,2019-05-26,0.17,0.238
+2023-05-27,0.23,2022-05-27,0.22,2021-05-27,0.29,2020-05-27,0.3,2019-05-27,0.17,0.242
+2023-05-28,0.16,2022-05-28,0.24,2021-05-28,0.27,2020-05-28,0.33,2019-05-28,0.24,0.248
+2023-05-29,0.24,2022-05-29,0.27,2021-05-29,0.26,2020-05-29,0.32,2019-05-29,0.24,0.266
+2023-05-30,0.24,2022-05-30,0.28,2021-05-30,0.3,2020-05-30,0.26,2019-05-30,0.23,0.262
+2023-05-31,0.24,2022-05-31,0.32,2021-05-31,0.31,2020-05-31,0.18,2019-05-31,0.24,0.258
+2023-06-01,0.25,2022-06-01,0.29,2021-06-01,0.33,2020-06-01,0.22,2019-06-01,0.27,0.272
+2023-06-02,0.26,2022-06-02,0.31,2021-06-02,0.28,2020-06-02,0.26,2019-06-02,0.25,0.272
+2023-06-03,0.27,2022-06-03,0.22,2021-06-03,0.3,2020-06-03,0.31,2019-06-03,0.25,0.27
+2023-06-04,0.28,2022-06-04,0.16,2021-06-04,0.32,2020-06-04,0.3,2019-06-04,0.27,0.266
+2023-06-05,0.26,2022-06-05,0.11,2021-06-05,0.29,2020-06-05,0.31,2019-06-05,0.3,0.254
+2023-06-06,0.16,2022-06-06,0.27,2021-06-06,0.33,2020-06-06,0.3,2019-06-06,0.33,0.278
+2023-06-07,0.25,2022-06-07,0.31,2021-06-07,0.32,2020-06-07,0.31,2019-06-07,0.3,0.29800000000000004
+2023-06-08,0.25,2022-06-08,0.32,2021-06-08,0.29,2020-06-08,0.24,2019-06-08,0.36,0.292
+2023-06-09,0.25,2022-06-09,0.34,2021-06-09,0.29,2020-06-09,0.32,2019-06-09,0.29,0.29800000000000004
+2023-06-10,0.18,2022-06-10,0.35,2021-06-10,0.26,2020-06-10,0.3,2019-06-10,0.27,0.272
+2023-06-11,0.23,2022-06-11,0.34,2021-06-11,0.29,2020-06-11,0.3,2019-06-11,0.28,0.28800000000000003
+2023-06-12,0.22,2022-06-12,0.26,2021-06-12,0.26,2020-06-12,0.29,2019-06-12,0.31,0.268
+2023-06-13,0.27,2022-06-13,0.3,2021-06-13,0.28,2020-06-13,0.24,2019-06-13,0.3,0.278
+2023-06-14,0.27,2022-06-14,0.32,2021-06-14,0.27,2020-06-14,0.26,2019-06-14,0.27,0.278
+2023-06-15,0.28,2022-06-15,0.34,2021-06-15,0.29,2020-06-15,0.27,2019-06-15,0.24,0.28400000000000003
+2023-06-16,0.31,2022-06-16,0.34,2021-06-16,0.29,2020-06-16,0.27,2019-06-16,0.25,0.292
+2023-06-17,0.29,2022-06-17,0.27,2021-06-17,0.28,2020-06-17,0.28,2019-06-17,0.27,0.278
+2023-06-18,0.33,2022-06-18,0.27,2021-06-18,0.28,2020-06-18,0.35,2019-06-18,0.31,0.308
+2023-06-19,0.31,2022-06-19,0.3,2021-06-19,0.3,2020-06-19,0.35,2019-06-19,0.28,0.30799999999999994
+2023-06-20,0.28,2022-06-20,0.32,2021-06-20,0.27,2020-06-20,0.36,2019-06-20,0.28,0.302
+2023-06-21,0.29,2022-06-21,0.33,2021-06-21,0.28,2020-06-21,0.32,2019-06-21,0.29,0.302
+2023-06-22,0.27,2022-06-22,0.26,2021-06-22,0.27,2020-06-22,0.33,2019-06-22,0.34,0.29400000000000004
+2023-06-23,0.26,2022-06-23,0.32,2021-06-23,0.28,2020-06-23,0.32,2019-06-23,0.3,0.29600000000000004
+2023-06-24,0.28,2022-06-24,0.31,2021-06-24,0.28,2020-06-24,0.33,2019-06-24,0.29,0.29800000000000004
+2023-06-25,0.27,2022-06-25,0.34,2021-06-25,0.28,2020-06-25,0.32,2019-06-25,0.34,0.31000000000000005
+2023-06-26,0.28,2022-06-26,0.32,2021-06-26,0.29,2020-06-26,0.33,2019-06-26,0.29,0.30200000000000005
+2023-06-27,0.28,2022-06-27,0.34,2021-06-27,0.3,2020-06-27,0.34,2019-06-27,0.26,0.30400000000000005
+2023-06-28,0.3,2022-06-28,0.39,2021-06-28,0.31,2020-06-28,0.32,2019-06-28,0.24,0.312
+2023-06-29,0.31,2022-06-29,0.32,2021-06-29,0.31,2020-06-29,0.3,2019-06-29,0.25,0.298
+2023-06-30,0.32,2022-06-30,0.28,2021-06-30,0.28,2020-06-30,0.35,2019-06-30,0.29,0.304
+2023-07-01,0.33,2022-07-01,0.29,2021-07-01,0.27,2020-07-01,0.3,2019-07-01,0.29,0.296
+2023-07-02,0.39,2022-07-02,0.27,2021-07-02,0.28,2020-07-02,0.31,2019-07-02,0.28,0.306
+2023-07-03,0.39,2022-07-03,0.22,2021-07-03,0.26,2020-07-03,0.3,2019-07-03,0.28,0.29
+2023-07-04,0.3,2022-07-04,0.26,2021-07-04,0.29,2020-07-04,0.29,2019-07-04,0.28,0.28400000000000003
+2023-07-05,0.29,2022-07-05,0.27,2021-07-05,0.29,2020-07-05,0.31,2019-07-05,0.29,0.29000000000000004
+2023-07-06,0.28,2022-07-06,0.28,2021-07-06,0.3,2020-07-06,0.37,2019-07-06,0.28,0.302
+2023-07-07,0.29,2022-07-07,0.29,2021-07-07,0.3,2020-07-07,0.38,2019-07-07,0.27,0.30599999999999994
+2023-07-08,0.3,2022-07-08,0.32,2021-07-08,0.28,2020-07-08,0.31,2019-07-08,0.26,0.294
+2023-07-09,0.26,2022-07-09,0.32,2021-07-09,0.3,2020-07-09,0.32,2019-07-09,0.24,0.28800000000000003
+2023-07-10,0.27,2022-07-10,0.31,2021-07-10,0.33,2020-07-10,0.31,2019-07-10,0.25,0.29400000000000004
+2023-07-11,0.3,2022-07-11,0.34,2021-07-11,0.33,2020-07-11,0.35,2019-07-11,0.3,0.32399999999999995
+2023-07-12,0.31,2022-07-12,0.29,2021-07-12,0.26,2020-07-12,0.34,2019-07-12,0.29,0.298
+2023-07-13,0.33,2022-07-13,0.25,2021-07-13,0.23,2020-07-13,0.35,2019-07-13,0.28,0.28800000000000003
+2023-07-14,0.3,2022-07-14,0.28,2021-07-14,0.23,2020-07-14,0.33,2019-07-14,0.3,0.28800000000000003
+2023-07-15,0.3,2022-07-15,0.32,2021-07-15,0.24,2020-07-15,0.28,2019-07-15,0.29,0.28600000000000003
+2023-07-16,0.3,2022-07-16,0.32,2021-07-16,0.23,2020-07-16,0.29,2019-07-16,0.27,0.282
+2023-07-17,0.33,2022-07-17,0.34,2021-07-17,0.25,2020-07-17,0.3,2019-07-17,0.24,0.292
+2023-07-18,0.31,2022-07-18,0.33,2021-07-18,0.24,2020-07-18,0.27,2019-07-18,0.24,0.27799999999999997
+2023-07-19,0.29,2022-07-19,0.35,2021-07-19,0.26,2020-07-19,0.29,2019-07-19,0.25,0.288
+2023-07-20,0.3,2022-07-20,0.33,2021-07-20,0.29,2020-07-20,0.33,2019-07-20,0.26,0.302
+2023-07-21,0.32,2022-07-21,0.34,2021-07-21,0.3,2020-07-21,0.28,2019-07-21,0.24,0.296
+2023-07-22,0.33,2022-07-22,0.37,2021-07-22,0.28,2020-07-22,0.25,2019-07-22,0.29,0.304
+2023-07-23,0.34,2022-07-23,0.31,2021-07-23,0.27,2020-07-23,0.28,2019-07-23,0.29,0.29800000000000004
+2023-07-24,0.31,2022-07-24,0.3,2021-07-24,0.3,2020-07-24,0.28,2019-07-24,0.29,0.296
+2023-07-25,0.31,2022-07-25,0.29,2021-07-25,0.3,2020-07-25,0.3,2019-07-25,0.31,0.302
+2023-07-26,0.3,2022-07-26,0.3,2021-07-26,0.19,2020-07-26,0.29,2019-07-26,0.28,0.272
+2023-07-27,0.29,2022-07-27,0.31,2021-07-27,0.24,2020-07-27,0.33,2019-07-27,0.27,0.288
+2023-07-28,0.33,2022-07-28,0.24,2021-07-28,0.28,2020-07-28,0.36,2019-07-28,0.32,0.306
+2023-07-29,0.32,2022-07-29,0.28,2021-07-29,0.29,2020-07-29,0.32,2019-07-29,0.27,0.29600000000000004
+2023-07-30,0.32,2022-07-30,0.26,2021-07-30,0.32,2020-07-30,0.32,2019-07-30,0.26,0.29600000000000004
+2023-07-31,0.28,2022-07-31,0.17,2021-07-31,0.28,2020-07-31,0.31,2019-07-31,0.26,0.26
+2023-08-01,0.27,2022-08-01,0.15,2021-08-01,0.28,2020-08-01,0.32,2019-08-01,0.24,0.252
+2023-08-02,0.27,2022-08-02,0.3,2021-08-02,0.26,2020-08-02,0.32,2019-08-02,0.27,0.28400000000000003
+2023-08-03,0.26,2022-08-03,0.33,2021-08-03,0.29,2020-08-03,0.31,2019-08-03,0.28,0.29400000000000004
+2023-08-04,0.27,2022-08-04,0.33,2021-08-04,0.28,2020-08-04,0.34,2019-08-04,0.27,0.29800000000000004
+2023-08-05,0.3,2022-08-05,0.25,2021-08-05,0.24,2020-08-05,0.28,2019-08-05,0.22,0.258
+2023-08-06,0.32,2022-08-06,0.28,2021-08-06,0.21,2020-08-06,0.24,2019-08-06,0.27,0.264
+2023-08-07,0.33,2022-08-07,0.28,2021-08-07,0.19,2020-08-07,0.26,2019-08-07,0.24,0.26
+2023-08-08,0.24,2022-08-08,0.29,2021-08-08,0.23,2020-08-08,0.26,2019-08-08,0.25,0.254
+2023-08-09,0.24,2022-08-09,0.27,2021-08-09,0.25,2020-08-09,0.29,2019-08-09,0.24,0.258
+2023-08-10,0.25,2022-08-10,0.27,2021-08-10,0.26,2020-08-10,0.3,2019-08-10,0.23,0.262
+2023-08-11,0.25,2022-08-11,0.28,2021-08-11,0.31,2020-08-11,0.33,2019-08-11,0.26,0.28600000000000003
+2023-08-12,0.28,2022-08-12,0.29,2021-08-12,0.28,2020-08-12,0.27,2019-08-12,0.27,0.278
+2023-08-13,0.29,2022-08-13,0.28,2021-08-13,0.29,2020-08-13,0.26,2019-08-13,0.26,0.276
+2023-08-14,0.23,2022-08-14,0.28,2021-08-14,0.28,2020-08-14,0.26,2019-08-14,0.27,0.264
+2023-08-15,0.3,2022-08-15,0.27,2021-08-15,0.25,2020-08-15,0.28,2019-08-15,0.27,0.274
+2023-08-16,0.29,2022-08-16,0.28,2021-08-16,0.3,2020-08-16,0.28,2019-08-16,0.3,0.29000000000000004
+2023-08-17,0.29,2022-08-17,0.21,2021-08-17,0.23,2020-08-17,0.26,2019-08-17,0.24,0.246
+2023-08-18,0.26,2022-08-18,0.35,2021-08-18,0.2,2020-08-18,0.23,2019-08-18,0.23,0.254
+2023-08-19,0.27,2022-08-19,0.3,2021-08-19,0.2,2020-08-19,0.3,2019-08-19,0.19,0.252
+2023-08-20,0.17,2022-08-20,0.27,2021-08-20,0.17,2020-08-20,0.28,2019-08-20,0.21,0.22000000000000003
+2023-08-21,0.17,2022-08-21,0.27,2021-08-21,0.2,2020-08-21,0.29,2019-08-21,0.26,0.23800000000000004
+2023-08-22,0.21,2022-08-22,0.28,2021-08-22,0.21,2020-08-22,0.25,2019-08-22,0.27,0.244
+2023-08-23,0.24,2022-08-23,0.25,2021-08-23,0.21,2020-08-23,0.23,2019-08-23,0.26,0.238
+2023-08-24,0.27,2022-08-24,0.25,2021-08-24,0.21,2020-08-24,0.24,2019-08-24,0.27,0.248
+2023-08-25,0.24,2022-08-25,0.24,2021-08-25,0.22,2020-08-25,0.21,2019-08-25,0.27,0.236
+2023-08-26,0.25,2022-08-26,0.23,2021-08-26,0.23,2020-08-26,0.25,2019-08-26,0.31,0.254
+2023-08-27,0.26,2022-08-27,0.24,2021-08-27,0.22,2020-08-27,0.26,2019-08-27,0.29,0.254
+2023-08-28,0.28,2022-08-28,0.24,2021-08-28,0.22,2020-08-28,0.23,2019-08-28,0.24,0.242
+2023-08-29,0.28,2022-08-29,0.24,2021-08-29,0.28,2020-08-29,0.25,2019-08-29,0.23,0.256
+2023-08-30,0.26,2022-08-30,0.26,2021-08-30,0.25,2020-08-30,0.23,2019-08-30,0.22,0.244
+2023-08-31,0.22,2022-08-31,0.25,2021-08-31,0.26,2020-08-31,0.22,2019-08-31,0.23,0.236
+2023-09-01,0.17,2022-09-01,0.25,2021-09-01,0.2,2020-09-01,0.23,2019-09-01,0.26,0.22200000000000003
+2023-09-02,0.08,2022-09-02,0.3,2021-09-02,0.18,2020-09-02,0.23,2019-09-02,0.26,0.21000000000000002
+2023-09-03,0.2,2022-09-03,0.29,2021-09-03,0.19,2020-09-03,0.22,2019-09-03,0.23,0.22599999999999998
+2023-09-04,0.21,2022-09-04,0.24,2021-09-04,0.21,2020-09-04,0.23,2019-09-04,0.22,0.22199999999999998
+2023-09-05,0.21,2022-09-05,0.27,2021-09-05,0.23,2020-09-05,0.26,2019-09-05,0.18,0.22999999999999998
+2023-09-06,0.24,2022-09-06,0.28,2021-09-06,0.22,2020-09-06,0.22,2019-09-06,0.23,0.238
+2023-09-07,0.21,2022-09-07,0.31,2021-09-07,0.29,2020-09-07,0.24,2019-09-07,0.19,0.248
+2023-09-08,0.21,2022-09-08,0.27,2021-09-08,0.28,2020-09-08,0.27,2019-09-08,0.19,0.244
+2023-09-09,0.22,2022-09-09,0.27,2021-09-09,0.24,2020-09-09,0.3,2019-09-09,0.22,0.25
+2023-09-10,0.22,2022-09-10,0.18,2021-09-10,0.21,2020-09-10,0.09,2019-09-10,0.18,0.176
+2023-09-11,0.24,2022-09-11,0.19,2021-09-11,0.21,2020-09-11,0.08,2019-09-11,0.19,0.182
+2023-09-12,0.2,2022-09-12,0.22,2021-09-12,0.22,2020-09-12,0.16,2019-09-12,0.21,0.202
+2023-09-13,0.2,2022-09-13,0.22,2021-09-13,0.21,2020-09-13,0.15,2019-09-13,0.21,0.198
+2023-09-14,0.21,2022-09-14,0.2,2021-09-14,0.21,2020-09-14,0.18,2019-09-14,0.21,0.202
+2023-09-15,0.19,2022-09-15,0.18,2021-09-15,0.2,2020-09-15,0.18,2019-09-15,0.2,0.19
+2023-09-16,0.18,2022-09-16,0.18,2021-09-16,0.16,2020-09-16,0.18,2019-09-16,0.15,0.16999999999999998
+2023-09-17,0.19,2022-09-17,0.17,2021-09-17,0.17,2020-09-17,0.19,2019-09-17,0.19,0.182
+2023-09-18,0.2,2022-09-18,0.11,2021-09-18,0.17,2020-09-18,0.2,2019-09-18,0.15,0.16599999999999998
+2023-09-19,0.19,2022-09-19,0.08,2021-09-19,0.16,2020-09-19,0.18,2019-09-19,0.18,0.158
+2023-09-20,0.18,2022-09-20,0.12,2021-09-20,0.2,2020-09-20,0.18,2019-09-20,0.19,0.174
+2023-09-21,0.17,2022-09-21,0.13,2021-09-21,0.17,2020-09-21,0.19,2019-09-21,0.2,0.17200000000000001
+2023-09-22,0.16,2022-09-22,0.17,2021-09-22,0.22,2020-09-22,0.19,2019-09-22,0.17,0.182
+2023-09-23,0.17,2022-09-23,0.18,2021-09-23,0.2,2020-09-23,0.24,2019-09-23,0.21,0.2
+2023-09-24,0.17,2022-09-24,0.19,2021-09-24,0.19,2020-09-24,0.22,2019-09-24,0.25,0.20400000000000001
+2023-09-25,0.16,2022-09-25,0.21,2021-09-25,0.16,2020-09-25,0.22,2019-09-25,0.18,0.186
+2023-09-26,0.16,2022-09-26,0.19,2021-09-26,0.16,2020-09-26,0.19,2019-09-26,0.23,0.186
+2023-09-27,0.19,2022-09-27,0.18,2021-09-27,0.16,2020-09-27,0.2,2019-09-27,0.16,0.178
+2023-09-28,0.19,2022-09-28,0.21,2021-09-28,0.19,2020-09-28,0.23,2019-09-28,0.15,0.194
+2023-09-29,0.18,2022-09-29,0.17,2021-09-29,0.19,2020-09-29,0.22,2019-09-29,0.17,0.186
+2023-09-30,0.11,2022-09-30,0.19,2021-09-30,0.15,2020-09-30,0.18,2019-09-30,0.15,0.15599999999999997
+2023-10-01,0.17,2022-10-01,0.18,2021-10-01,0.15,2020-10-01,0.21,2019-10-01,0.16,0.174
+2023-10-02,0.18,2022-10-02,0.18,2021-10-02,0.14,2020-10-02,0.15,2019-10-02,0.16,0.162
+2023-10-03,0.15,2022-10-03,0.18,2021-10-03,0.13,2020-10-03,0.13,2019-10-03,0.17,0.152
+2023-10-04,0.15,2022-10-04,0.16,2021-10-04,0.15,2020-10-04,0.15,2019-10-04,0.18,0.158
+2023-10-05,0.16,2022-10-05,0.15,2021-10-05,0.17,2020-10-05,0.19,2019-10-05,0.17,0.16799999999999998
+2023-10-06,0.16,2022-10-06,0.16,2021-10-06,0.15,2020-10-06,0.17,2019-10-06,0.16,0.16
+2023-10-07,0.16,2022-10-07,0.18,2021-10-07,0.14,2020-10-07,0.17,2019-10-07,0.16,0.162
+2023-10-08,0.18,2022-10-08,0.18,2021-10-08,0.14,2020-10-08,0.16,2019-10-08,0.17,0.166
+2023-10-09,0.07,2022-10-09,0.17,2021-10-09,0.14,2020-10-09,0.11,2019-10-09,0.24,0.146
+2023-10-10,0.14,2022-10-10,0.16,2021-10-10,0.15,2020-10-10,0.11,2019-10-10,0.22,0.156
+2023-10-11,0.15,2022-10-11,0.15,2021-10-11,0.23,2020-10-11,0.12,2019-10-11,0.14,0.158
+2023-10-12,0.15,2022-10-12,0.15,2021-10-12,0.2,2020-10-12,0.17,2019-10-12,0.13,0.16
+2023-10-13,0.13,2022-10-13,0.13,2021-10-13,0.1,2020-10-13,0.16,2019-10-13,0.14,0.132
+2023-10-14,0.11,2022-10-14,0.13,2021-10-14,0.16,2020-10-14,0.15,2019-10-14,0.14,0.138
+2023-10-15,0.13,2022-10-15,0.14,2021-10-15,0.12,2020-10-15,0.21,2019-10-15,0.12,0.144
+2023-10-16,0.13,2022-10-16,0.11,2021-10-16,0.12,2020-10-16,0.19,2019-10-16,0.1,0.13
+2023-10-17,0.14,2022-10-17,0.11,2021-10-17,0.1,2020-10-17,0.17,2019-10-17,0.15,0.134
+2023-10-18,0.12,2022-10-18,0.1,2021-10-18,0.11,2020-10-18,0.15,2019-10-18,0.13,0.122
+2023-10-19,0.13,2022-10-19,0.12,2021-10-19,0.09,2020-10-19,0.15,2019-10-19,0.12,0.122
+2023-10-20,0.15,2022-10-20,0.12,2021-10-20,0.04,2020-10-20,0.15,2019-10-20,0.15,0.122
+2023-10-21,0.12,2022-10-21,0.13,2021-10-21,0.05,2020-10-21,0.15,2019-10-21,0.17,0.124
+2023-10-22,0.07,2022-10-22,0.11,2021-10-22,0.08,2020-10-22,0.14,2019-10-22,0.13,0.10600000000000001
+2023-10-23,0.11,2022-10-23,0.17,2021-10-23,0.06,2020-10-23,0.19,2019-10-23,0.18,0.142
+2023-10-24,0.1,2022-10-24,0.13,2021-10-24,0,2020-10-24,0.13,2019-10-24,0.16,0.10400000000000001
+2023-10-25,0.09,2022-10-25,0.11,2021-10-25,0.09,2020-10-25,0.11,2019-10-25,0.12,0.10400000000000001
+2023-10-26,0.12,2022-10-26,0.13,2021-10-26,0.06,2020-10-26,0.15,2019-10-26,0.1,0.11199999999999999
+2023-10-27,0.1,2022-10-27,0.12,2021-10-27,0.09,2020-10-27,0.24,2019-10-27,0.29,0.168
+2023-10-28,0.16,2022-10-28,0.09,2021-10-28,0.08,2020-10-28,0.12,2019-10-28,0.12,0.11400000000000002
+2023-10-29,0.15,2022-10-29,0.11,2021-10-29,0.06,2020-10-29,0.1,2019-10-29,0.16,0.11600000000000002
+2023-10-30,0.09,2022-10-30,0.1,2021-10-30,0.08,2020-10-30,0.1,2019-10-30,0.12,0.098
+2023-10-31,0.1,2022-10-31,0.05,2021-10-31,0.08,2020-10-31,0.1,2019-10-31,0.1,0.08600000000000001
+2023-11-01,0.09,2022-11-01,0.05,2021-11-01,0.01,2020-11-01,0.1,2019-11-01,0.09,0.06799999999999999
+2023-11-02,0.09,2022-11-02,0.08,2021-11-02,0.06,2020-11-02,0.1,2019-11-02,0.09,0.08399999999999999
+2023-11-03,0.1,2022-11-03,0.11,2021-11-03,0.09,2020-11-03,0.1,2019-11-03,0.09,0.098
+2023-11-04,0.09,2022-11-04,0.07,2021-11-04,0.09,2020-11-04,0.12,2019-11-04,0.09,0.092
+2023-11-05,0.06,2022-11-05,0.02,2021-11-05,0.08,2020-11-05,0.1,2019-11-05,0.09,0.06999999999999999
+2023-11-06,0.08,2022-11-06,0.1,2021-11-06,0.04,2020-11-06,0.1,2019-11-06,0.09,0.082
+2023-11-07,0.09,2022-11-07,0.08,2021-11-07,0.09,2020-11-07,0.13,2019-11-07,0.09,0.096
+2023-11-08,0.13,2022-11-08,0.04,2021-11-08,0.07,2020-11-08,0.09,2019-11-08,0.1,0.08600000000000001
+2023-11-09,0.08,2022-11-09,0.07,2021-11-09,0.04,2020-11-09,0.13,2019-11-09,0.09,0.082
+2023-11-10,0.09,2022-11-10,0.08,2021-11-10,0.05,2020-11-10,0.11,2019-11-10,0.08,0.08199999999999999
+2023-11-11,0.09,2022-11-11,0.07,2021-11-11,0.08,2020-11-11,0.08,2019-11-11,0.08,0.08
+2023-11-12,0.08,2022-11-12,0.06,2021-11-12,0.02,2020-11-12,0.06,2019-11-12,0.08,0.06
+2023-11-13,0.07,2022-11-13,0.1,2021-11-13,0.03,2020-11-13,0.07,2019-11-13,0.07,0.068
+2023-11-14,0.08,2022-11-14,0.07,2021-11-14,0,2020-11-14,0.03,2019-11-14,0.07,0.05
+2023-11-15,0.02,2022-11-15,0.1,2021-11-15,0,2020-11-15,0.07,2019-11-15,0.08,0.054000000000000006
+2023-11-16,0.05,2022-11-16,0.08,2021-11-16,0.04,2020-11-16,0.07,2019-11-16,0.07,0.06200000000000001
+2023-11-17,0.04,2022-11-17,0.06,2021-11-17,0.04,2020-11-17,0.07,2019-11-17,0.07,0.05600000000000001
+2023-11-18,0.02,2022-11-18,0.11,2021-11-18,0.02,2020-11-18,0.03,2019-11-18,0.07,0.05
+2023-11-19,0.1,2022-11-19,0.08,2021-11-19,0.02,2020-11-19,0.06,2019-11-19,0.09,0.06999999999999999
+2023-11-20,0.1,2022-11-20,0.07,2021-11-20,0.08,2020-11-20,0.07,2019-11-20,0.16,0.096
+2023-11-21,0.05,2022-11-21,0.06,2021-11-21,0.07,2020-11-21,0.08,2019-11-21,0.06,0.064
+2023-11-22,0.06,2022-11-22,0.05,2021-11-22,0.06,2020-11-22,0.07,2019-11-22,0.07,0.062
+2023-11-23,0.12,2022-11-23,0.07,2021-11-23,0.05,2020-11-23,0.07,2019-11-23,0.07,0.076
+2023-11-24,0.13,2022-11-24,0.06,2021-11-24,0.11,2020-11-24,0.07,2019-11-24,0.07,0.088
+2023-11-25,0.08,2022-11-25,0.05,2021-11-25,0.06,2020-11-25,0.08,2019-11-25,0.13,0.08
+2023-11-26,0.07,2022-11-26,0.07,2021-11-26,0.06,2020-11-26,0.08,2019-11-26,0.04,0.064
+2023-11-27,0.07,2022-11-27,0.06,2021-11-27,0.06,2020-11-27,0.14,2019-11-27,0.04,0.074
+2023-11-28,0.07,2022-11-28,0.09,2021-11-28,0.06,2020-11-28,0.06,2019-11-28,0.02,0.06000000000000001
+2023-11-29,0.06,2022-11-29,0.07,2021-11-29,0.05,2020-11-29,0.07,2019-11-29,0.05,0.06
+2023-11-30,0.08,2022-11-30,0.06,2021-11-30,0.05,2020-11-30,0.06,2019-11-30,0.03,0.05600000000000001
+2023-12-01,0.07,2022-12-01,0.01,2021-12-01,0.06,2020-12-01,0.07,2019-12-01,0.01,0.044000000000000004
+2023-12-02,0.02,2022-12-02,0.05,2021-12-02,0.04,2020-12-02,0.07,2019-12-02,0.01,0.038000000000000006
+2023-12-03,0.06,2022-12-03,0,2021-12-03,0.03,2020-12-03,0.06,2019-12-03,0.02,0.033999999999999996
+2023-12-04,0.05,2022-12-04,0.06,2021-12-04,0.02,2020-12-04,0.06,2019-12-04,0,0.038
+2023-12-05,0.05,2022-12-05,0.06,2021-12-05,0.02,2020-12-05,0.07,2019-12-05,0.01,0.042
+2023-12-06,0.02,2022-12-06,0.04,2021-12-06,0,2020-12-06,0.06,2019-12-06,0.05,0.033999999999999996
+2023-12-07,0.04,2022-12-07,0.05,2021-12-07,0.03,2020-12-07,0.07,2019-12-07,0.06,0.05
+2023-12-08,0.08,2022-12-08,0.02,2021-12-08,0,2020-12-08,0.15,2019-12-08,0.04,0.057999999999999996
+2023-12-09,0.06,2022-12-09,0.03,2021-12-09,0.04,2020-12-09,0.06,2019-12-09,0.05,0.048
+2023-12-10,0.05,2022-12-10,0,2021-12-10,0.07,2020-12-10,0.07,2019-12-10,0.02,0.041999999999999996
+2023-12-11,0.06,2022-12-11,0.04,2021-12-11,0.05,2020-12-11,0.09,2019-12-11,0,0.048
+2023-12-12,0.05,2022-12-12,0.05,2021-12-12,0,2020-12-12,0.04,2019-12-12,0.02,0.032
+2023-12-13,0.05,2022-12-13,0.05,2021-12-13,0,2020-12-13,0.02,2019-12-13,0,0.024
+2023-12-14,0.04,2022-12-14,0.04,2021-12-14,0.04,2020-12-14,0.01,2019-12-14,0.06,0.038
+2023-12-15,0.05,2022-12-15,0.05,2021-12-15,0.04,2020-12-15,0.06,2019-12-15,0.06,0.052000000000000005
+2023-12-16,0.06,2022-12-16,0.05,2021-12-16,0.04,2020-12-16,0.04,2019-12-16,0.06,0.05
+2023-12-17,0.01,2022-12-17,0.04,2021-12-17,0.05,2020-12-17,0.03,2019-12-17,0.04,0.034
+2023-12-18,0.03,2022-12-18,0,2021-12-18,0.02,2020-12-18,0.05,2019-12-18,0.02,0.024
+2023-12-19,0.02,2022-12-19,0,2021-12-19,0,2020-12-19,0.07,2019-12-19,0.02,0.022000000000000002
+2023-12-20,0,2022-12-20,0.04,2021-12-20,0.01,2020-12-20,0.03,2019-12-20,0.04,0.024
+2023-12-21,0.03,2022-12-21,0,2021-12-21,0.01,2020-12-21,0.03,2019-12-21,0.01,0.016
+2023-12-22,0.06,2022-12-22,0,2021-12-22,0.01,2020-12-22,0.01,2019-12-22,0,0.015999999999999997
+2023-12-23,0.07,2022-12-23,0.03,2021-12-23,0.01,2020-12-23,0.04,2019-12-23,0.06,0.041999999999999996
+2023-12-24,0.04,2022-12-24,0.04,2021-12-24,0.06,2020-12-24,0.04,2019-12-24,0.05,0.046000000000000006
+2023-12-25,0.04,2022-12-25,0,2021-12-25,0,2020-12-25,0.02,2019-12-25,0.03,0.018
+2023-12-26,0.02,2022-12-26,0,2021-12-26,0.01,2020-12-26,0.01,2019-12-26,0.08,0.024
+2023-12-27,0,2022-12-27,0,2021-12-27,0.04,2020-12-27,0.05,2019-12-27,0.06,0.03
+2023-12-28,0.03,2022-12-28,0.04,2021-12-28,0.04,2020-12-28,0.05,2019-12-28,0.05,0.042
+2023-12-29,0.02,2022-12-29,0,2021-12-29,0.01,2020-12-29,0.03,2019-12-29,0.01,0.013999999999999999
+2023-12-30,0.02,2022-12-30,0,2021-12-30,0.04,2020-12-30,0.07,2019-12-30,0.06,0.038
+2023-12-31,0.01,2022-12-31,0,2021-12-31,0.04,2020-12-31,0.05,2019-12-31,0.05,0.030000000000000006
+,,,,,,,0.06,,,0.06
Index: .idea/misc.xml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+><?xml version=\"1.0\" encoding=\"UTF-8\"?>\r\n<project version=\"4\">\r\n  <component name=\"ProjectRootManager\" version=\"2\" project-jdk-name=\"Python 3.9 (venv4)\" project-jdk-type=\"Python SDK\" />\r\n  <component name=\"PythonCompatibilityInspectionAdvertiser\">\r\n    <option name=\"version\" value=\"3\" />\r\n  </component>\r\n</project>
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/.idea/misc.xml b/.idea/misc.xml
--- a/.idea/misc.xml	(revision 5cf7d78a1d94023d00661edf5517badacc064130)
+++ b/.idea/misc.xml	(date 1719610898632)
@@ -1,6 +1,9 @@
 <?xml version="1.0" encoding="UTF-8"?>
 <project version="4">
-  <component name="ProjectRootManager" version="2" project-jdk-name="Python 3.9 (venv4)" project-jdk-type="Python SDK" />
+  <component name="Black">
+    <option name="sdkName" value="Python 3.12 (Stomato)" />
+  </component>
+  <component name="ProjectRootManager" version="2" project-jdk-name="Python 3.12 (Stomato)" project-jdk-type="Python SDK" />
   <component name="PythonCompatibilityInspectionAdvertiser">
     <option name="version" value="3" />
   </component>
Index: .idea/Stomato.iml
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+><?xml version=\"1.0\" encoding=\"UTF-8\"?>\r\n<module type=\"PYTHON_MODULE\" version=\"4\">\r\n  <component name=\"NewModuleRootManager\">\r\n    <content url=\"file://$MODULE_DIR$\">\r\n      <excludeFolder url=\"file://$MODULE_DIR$/venv\" />\r\n      <excludeFolder url=\"file://$MODULE_DIR$/venv2\" />\r\n      <excludeFolder url=\"file://$MODULE_DIR$/venv3\" />\r\n      <excludeFolder url=\"file://$MODULE_DIR$/venv4\" />\r\n      <excludeFolder url=\"file://$MODULE_DIR$/venv/Jesus\" />\r\n    </content>\r\n    <orderEntry type=\"inheritedJdk\" />\r\n    <orderEntry type=\"sourceFolder\" forTests=\"false\" />\r\n  </component>\r\n  <component name=\"PackageRequirementsSettings\">\r\n    <option name=\"removeUnused\" value=\"true\" />\r\n  </component>\r\n</module>
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/.idea/Stomato.iml b/.idea/Stomato.iml
--- a/.idea/Stomato.iml	(revision 5cf7d78a1d94023d00661edf5517badacc064130)
+++ b/.idea/Stomato.iml	(date 1719610898626)
@@ -7,6 +7,7 @@
       <excludeFolder url="file://$MODULE_DIR$/venv3" />
       <excludeFolder url="file://$MODULE_DIR$/venv4" />
       <excludeFolder url="file://$MODULE_DIR$/venv/Jesus" />
+      <excludeFolder url="file://$MODULE_DIR$/.venv" />
     </content>
     <orderEntry type="inheritedJdk" />
     <orderEntry type="sourceFolder" forTests="false" />
Index: portal_data.csv
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>order,field,crop_type,crop_image,soil_moisture_num,soil_moisture_desc,si_num,si_desc,report,preview,logger_name,logger_direction\r\n90,D1,Tomatoes,https://i.imgur.com/yqTglkD.png,29.6,Optimum Moisture,,No Stress Index,https://lookerstudio.google.com/reporting/3a81b33b-a83e-4e8d-b9c5-9db3a09e1d0a,https://i.imgur.com/eyn0tht.png,DI-D1-NE,NE\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/portal_data.csv b/portal_data.csv
--- a/portal_data.csv	(revision 5cf7d78a1d94023d00661edf5517badacc064130)
+++ b/portal_data.csv	(date 1720465061892)
@@ -1,2 +1,2 @@
-order,field,crop_type,crop_image,soil_moisture_num,soil_moisture_desc,si_num,si_desc,report,preview,logger_name,logger_direction
-90,D1,Tomatoes,https://i.imgur.com/yqTglkD.png,29.6,Optimum Moisture,,No Stress Index,https://lookerstudio.google.com/reporting/3a81b33b-a83e-4e8d-b9c5-9db3a09e1d0a,https://i.imgur.com/eyn0tht.png,DI-D1-NE,NE
+order,field,crop_type,crop_image,soil_moisture_num,soil_moisture_desc,si_num,si_desc,report,preview,logger_name,logger_direction,location
+0,26,Watermelon,https://i.imgur.com/V9V5jAN.png,44.0,Optimum Moisture,,No Stress Index,https://lookerstudio.google.com/reporting/ecf036e2-3ee0-40bb-bde1-d0a79da63c3a,https://i.imgur.com/NjBMTQK.png,DF-26-WM2-NE,NE,"https://www.google.com/maps/search/?api=1&query=39.0612805,-121.7696707"
Index: KMLHandler.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import os\r\nimport xml.etree.ElementTree as ET\r\n\r\n\r\ndef find_and_write_named_to_kml(kml_file_paths, named_value, output_kml_file_path):\r\n    \"\"\"\r\n    Filters through multiple KML files to find a specific namecd element and writes them to a new KML file.\r\n\r\n    :param kml_file_paths: List of paths to the KML files.\r\n    :param named_value: The value of the named element to find.\r\n    :param output_kml_file_path: Path to the output KML file.\r\n    \"\"\"\r\n    # Define the namespace (Google Earth KML namespace)\r\n    namespace = {'kml': 'http://earth.google.com/kml/2.2'}\r\n    matching_placemarks = []\r\n\r\n    for kml_file_path in kml_file_paths:\r\n        if not os.path.exists(kml_file_path):\r\n            print(f\"File does not exist: {kml_file_path}\")\r\n            continue\r\n\r\n        # Parse the KML file\r\n        tree = ET.parse(kml_file_path)\r\n        root = tree.getroot()\r\n\r\n        # Find all Placemark elements with the desired namecd value\r\n        for placemark in root.findall('.//kml:Placemark', namespace):\r\n            name_element = placemark.find('kml:name', namespace)\r\n            if name_element is not None and name_element.text == named_value:\r\n                matching_placemarks.append(placemark)\r\n\r\n    if not matching_placemarks:\r\n        print(f\"No elements found with named value: {named_value}\")\r\n        return\r\n\r\n    # Create a new KML root element\r\n    new_kml = ET.Element('kml', xmlns='http://earth.google.com/kml/2.2')\r\n    new_document = ET.SubElement(new_kml, 'Document')\r\n\r\n    # Append matching Placemarks to the new KML document\r\n    for placemark in matching_placemarks:\r\n        new_document.append(placemark)\r\n\r\n    # Write the new KML to the output file\r\n    new_tree = ET.ElementTree(new_kml)\r\n    new_tree.write(output_kml_file_path, encoding='utf-8', xml_declaration=True)\r\n    print(f\"New KML file created at: {output_kml_file_path}\")\r\n\r\n\r\nkml_file_paths = [\"H:Shared drives//Stomato//2024//TStar Map KMLs//south 6-5-2024.kml\",\r\n                  \"H:Shared drives//Stomato//2024//TStar Map KMLs//north 6-5-2024.kml\",\r\n                  \"H:Shared drives//Stomato//2024//TStar Map KMLs//organics 6-5-2024.kml\"]\r\nnamed_value = '3239'\r\noutput_kml_file_path = f'H://Shared drives//Stomato//2024//KMLS//{named_value}.kml'\r\n\r\nfind_and_write_named_to_kml(kml_file_paths, named_value, output_kml_file_path)\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/KMLHandler.py b/KMLHandler.py
--- a/KMLHandler.py	(revision 5cf7d78a1d94023d00661edf5517badacc064130)
+++ b/KMLHandler.py	(date 1720475798226)
@@ -50,7 +50,7 @@
 kml_file_paths = ["H:Shared drives//Stomato//2024//TStar Map KMLs//south 6-5-2024.kml",
                   "H:Shared drives//Stomato//2024//TStar Map KMLs//north 6-5-2024.kml",
                   "H:Shared drives//Stomato//2024//TStar Map KMLs//organics 6-5-2024.kml"]
-named_value = '3239'
+named_value = '1226'
 output_kml_file_path = f'H://Shared drives//Stomato//2024//KMLS//{named_value}.kml'
-
+# 1320 1321 1322 1323
 find_and_write_named_to_kml(kml_file_paths, named_value, output_kml_file_path)
Index: weather forecast.csv
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>date,day,order,temp,rh,vpd,icon\r\n2023-06-25,Sun,1,85.6,43.0,2.4,https://i.imgur.com/8enBPX2.gif\r\n2023-06-26,Mon,2,88.3,43.0,2.6,https://i.imgur.com/8enBPX2.gif\r\n2023-06-27,Tue,3,87.7,39.0,2.7,https://i.imgur.com/q9T5REp.gif\r\n2023-06-28,Wed,4,91.2,41.0,2.9,https://i.imgur.com/8enBPX2.gif\r\n2023-06-29,Thu,5,96.2,36.0,3.7,https://i.imgur.com/8enBPX2.gif\r\n2023-06-30,Fri,6,103.2,29.0,5.1,https://i.imgur.com/8enBPX2.gif\r\n2023-07-01,Sat,7,105.7,24.0,5.9,https://i.imgur.com/8enBPX2.gif\r\n2023-07-02,Sun,8,106.5,23.0,6.1,https://i.imgur.com/8enBPX2.gif\r\n2023-07-03,Mon,9,105.4,24.0,5.8,https://i.imgur.com/8enBPX2.gif\r\n2023-07-04,Tue,10,108.8,22.0,6.6,https://i.imgur.com/8enBPX2.gif\r\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/weather forecast.csv b/weather forecast.csv
--- a/weather forecast.csv	(revision 5cf7d78a1d94023d00661edf5517badacc064130)
+++ b/weather forecast.csv	(date 1720474257566)
@@ -1,11 +1,11 @@
 date,day,order,temp,rh,vpd,icon
-2023-06-25,Sun,1,85.6,43.0,2.4,https://i.imgur.com/8enBPX2.gif
-2023-06-26,Mon,2,88.3,43.0,2.6,https://i.imgur.com/8enBPX2.gif
-2023-06-27,Tue,3,87.7,39.0,2.7,https://i.imgur.com/q9T5REp.gif
-2023-06-28,Wed,4,91.2,41.0,2.9,https://i.imgur.com/8enBPX2.gif
-2023-06-29,Thu,5,96.2,36.0,3.7,https://i.imgur.com/8enBPX2.gif
-2023-06-30,Fri,6,103.2,29.0,5.1,https://i.imgur.com/8enBPX2.gif
-2023-07-01,Sat,7,105.7,24.0,5.9,https://i.imgur.com/8enBPX2.gif
-2023-07-02,Sun,8,106.5,23.0,6.1,https://i.imgur.com/8enBPX2.gif
-2023-07-03,Mon,9,105.4,24.0,5.8,https://i.imgur.com/8enBPX2.gif
-2023-07-04,Tue,10,108.8,22.0,6.6,https://i.imgur.com/8enBPX2.gif
+2024-07-08,Mon,1,111.0,26.0,6.7,https://i.imgur.com/8enBPX2.gif
+2024-07-09,Tue,2,108.0,28.0,5.9,https://i.imgur.com/8enBPX2.gif
+2024-07-10,Wed,3,108.8,33.0,5.7,https://i.imgur.com/8enBPX2.gif
+2024-07-11,Thu,4,114.5,23.0,7.7,https://i.imgur.com/8enBPX2.gif
+2024-07-12,Fri,5,112.7,22.0,7.4,https://i.imgur.com/8enBPX2.gif
+2024-07-13,Sat,6,109.3,25.0,6.4,https://i.imgur.com/8enBPX2.gif
+2024-07-14,Sun,7,103.3,31.0,5.0,https://i.imgur.com/8enBPX2.gif
+2024-07-15,Mon,8,103.9,32.0,5.0,https://i.imgur.com/8enBPX2.gif
+2024-07-16,Tue,9,99.8,32.0,4.4,https://i.imgur.com/8enBPX2.gif
+2024-07-17,,10,,,,
Index: SoilAPI.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/SoilAPI.py b/SoilAPI.py
new file mode 100644
--- /dev/null	(date 1719532790221)
+++ b/SoilAPI.py	(date 1719532790221)
@@ -0,0 +1,329 @@
+import random
+
+import pandas as pd
+import requests
+import json
+import matplotlib.pyplot as plt
+from shapely import wkt
+from shapely.wkt import loads as load_wkt
+from shapely.geometry import Polygon, mapping
+import geopandas as gpd
+from matplotlib.patches import Polygon as MplPolygon
+from matplotlib.collections import PatchCollection
+import re
+import psycopg2
+
+def get_soil_texture_from_coords(latitude, longitude):
+
+    point_wkt = f"POINT({longitude} {latitude})"
+
+    # SQL query to get soil texture information
+    query = f"""
+        SELECT mu.muname, c.cokey
+        FROM mapunit AS mu
+        JOIN component AS c ON c.mukey = mu.mukey
+        JOIN chorizon AS ch ON ch.cokey = c.cokey
+        WHERE mu.mukey IN (
+            SELECT DISTINCT mukey
+            FROM SDA_Get_Mukey_from_intersection_with_WktWgs84('{point_wkt}')
+        )
+    """
+
+    # SDA request payload
+    request_payload = {
+        "format": "JSON+COLUMNNAME+METADATA",
+        "query": query
+    }
+
+    sda_url = "https://sdmdataaccess.sc.egov.usda.gov/Tabular/SDMTabularService/post.rest"
+    headers = {'Content-Type': 'application/json'}
+    response = requests.post(sda_url, json=request_payload, headers=headers)
+
+    # soil types intentionally formatted, longest to shortest length
+    # so when we check if soil_type in response_string
+    soil_types = ['Silty Clay Loam', 'Sandy Clay Loam',
+                  'Sandy Clay',  'Silt Loam', 'Clay Loam',
+                  'Silty Clay', 'Loamy Sand', 'Sandy Loam',
+                  'Sand', 'Clay', 'Loam',  'Silt']
+
+    if response.status_code == 200:
+        data = response.json()
+        if "Table" in data:
+            # the row in the json containing soil texture information
+            if data["Table"][2]:
+                texture_line = data["Table"][2][0]
+                lowercase_input = texture_line.lower()
+
+                matched_soil_type = None
+
+                # Iterate through the list of soil types and check for a match
+                for soil_type in soil_types:
+                    if soil_type.lower() in lowercase_input:
+                        matched_soil_type = soil_type
+                        break
+                return matched_soil_type
+        else:
+            print("No soil information found for the given coordinates.")
+    else:
+        print(f"Error: {response.status_code}, {response.text}")
+
+
+def get_soil_texture_from_polygon(polygon_wkt):
+    # SQL query to get soil texture information
+    query = f"""
+        SELECT mu.muname, mu.mukey, c.cokey, ch.hzname
+        FROM mapunit AS mu
+        JOIN component AS c ON c.mukey = mu.mukey
+        JOIN chorizon AS ch ON ch.cokey = c.cokey
+        WHERE mu.mukey IN (
+            SELECT DISTINCT mukey
+            FROM SDA_Get_Mukey_from_intersection_with_WktWgs84('{polygon_wkt}')
+        )
+    """
+
+    # SDA request payload
+    request_payload = {
+        "format": "JSON+COLUMNNAME+METADATA",
+        "query": query
+    }
+
+    sda_url = "https://sdmdataaccess.sc.egov.usda.gov/Tabular/SDMTabularService/post.rest"
+    headers = {'Content-Type': 'application/json'}
+    response = requests.post(sda_url, json=request_payload, headers=headers)
+
+    if response.status_code == 200:
+        data = response.json()
+        if "Table" in data:
+            soil_data = data["Table"]
+            return soil_data
+        else:
+            return "No soil information found for the given polygon."
+    else:
+        print(f"Error: {response.status_code}, {response.text}")
+        print("Request Payload:", json.dumps(request_payload, indent=2))
+        return f"Error: {response.status_code}, {response.text}"
+
+
+def visualize_soil_texture(polygon_wkt, soil_data):
+    # Extract column names and rows from soil_data
+    column_names = soil_data[0]
+    rows = soil_data[2:]
+
+    # Convert to DataFrame
+    df = pd.DataFrame(rows, columns=column_names)
+
+    # Ensure that the 'mukey' column exists
+    if 'mukey' not in df.columns or 'geom' not in df.columns:
+        print("Error: The expected columns 'mukey' and 'geom' are not present in the soil data.")
+        return
+
+    # Convert the 'mukey' column to numeric type if it's not
+    df['mukey'] = pd.to_numeric(df['mukey'], errors='coerce')
+
+    # Convert the 'geom' column to geometries
+    df['geometry'] = df['geom'].apply(load_wkt)
+
+    # Create a GeoDataFrame
+    gdf = gpd.GeoDataFrame(df, geometry='geometry')
+
+    # Create a plot
+    fig, ax = plt.subplots(1, 1, figsize=(10, 10))
+
+    # Plot the polygons
+    gdf.plot(column='mukey', cmap='Set1', legend=True, ax=ax)
+
+    # Plot settings
+    plt.title('Soil Types within Polygon')
+    plt.xlabel('Longitude')
+    plt.ylabel('Latitude')
+    plt.show()
+
+
+def get_soil_texture_from_polygon2(polygon_wkt):
+    # SQL query to get soil texture information, including spatial data
+    query = f"""
+        SELECT mu.muname, mu.mukey, c.cokey, ch.hzname, mu.geom
+        FROM mapunit AS mu
+        JOIN component AS c ON c.mukey = mu.mukey
+        JOIN chorizon AS ch ON ch.cokey = c.cokey
+        WHERE mu.mukey IN (
+            SELECT DISTINCT mukey
+            FROM SDA_Get_Mukey_from_intersection_with_WktWgs84('{polygon_wkt}')
+        )
+    """
+
+    # SDA request payload
+    request_payload = {
+        "format": "JSON+COLUMNNAME+METADATA",
+        "query": query
+    }
+
+    sda_url = "https://sdmdataaccess.sc.egov.usda.gov/Tabular/SDMTabularService/post.rest"
+    headers = {'Content-Type': 'application/json'}
+    response = requests.post(sda_url, json=request_payload, headers=headers)
+
+    if response.status_code == 200:
+        data = response.json()
+        if "Table" in data:
+            soil_data = data["Table"]
+            return soil_data
+        else:
+            return "No soil information found for the given polygon."
+    else:
+        print(f"Error: {response.status_code}, {response.text}")
+        print("Request Payload:", json.dumps(request_payload, indent=2))
+        return f"Error: {response.status_code}, {response.text}"
+
+
+def get_soil_data(polygon_wkt):
+    """
+    Get soil data for the given polygon WKT.
+
+    :param polygon_wkt: WKT string of the polygon
+    :return: List of dictionaries, each containing 'muname', 'cokey', 'hzname', and 'geom'
+    """
+    # Step 1: Get Map Unit Polygon Keys
+    query1 = f"""
+    SELECT DISTINCT mupolygonkey
+    FROM SDA_Get_Mupolygonkey_from_intersection_with_WktWgs84('{polygon_wkt}')
+    """
+    request_payload1 = {
+        "format": "JSON+COLUMNNAME+METADATA",
+        "query": query1
+    }
+    sda_url = "https://sdmdataaccess.sc.egov.usda.gov/Tabular/SDMTabularService/post.rest"
+    headers = {'Content-Type': 'application/json'}
+
+    response1 = requests.post(sda_url, json=request_payload1, headers=headers)
+
+    if response1.status_code != 200:
+        print(f"Error: {response1.status_code}, {response1.text}")
+        return f"Error: {response1.status_code}, {response1.text}"
+
+    data1 = response1.json()
+    if "Table" not in data1 or not data1["Table"]:
+        return []
+
+    mupolygonkeys = [row[0] for row in data1["Table"][2:]]
+
+    if not mupolygonkeys:
+        return []
+
+    # Step 2: Get Map Unit Geometries and Soil Data
+    query2 = f"""
+    SELECT mu.muname, c.cokey, ch.hzname, mp.geom
+    FROM mupolygon AS mp
+    JOIN mapunit AS mu ON mu.mukey = mp.mukey
+    JOIN component AS c ON c.mukey = mu.mukey
+    JOIN chorizon AS ch ON ch.cokey = c.cokey
+    WHERE mp.mupolygonkey IN ({','.join(map(str, mupolygonkeys))})
+    """
+    request_payload2 = {
+        "format": "JSON+COLUMNNAME+METADATA",
+        "query": query2
+    }
+
+    response2 = requests.post(sda_url, json=request_payload2, headers=headers)
+
+    if response2.status_code != 200:
+        print(f"Error: {response2.status_code}, {response2.text}")
+        return f"Error: {response2.status_code}, {response2.text}"
+
+    data2 = response2.json()
+    if "Table" not in data2 or not data2["Table"]:
+        return []
+
+    rows = data2["Table"][2:]
+
+    # Format the data
+    soil_data = []
+    for row in rows:
+        soil_data.append({
+            'muname': row[0],
+            'cokey': row[1],
+            'hzname': row[2],
+            'geom': row[3]  # Assuming 'geom' is directly in WKT format
+        })
+
+    return soil_data
+
+
+
+def graph_soil_polygon(polygon_wkt, soil_data):
+    """
+    Graph the polygon with different soil types in different colors.
+
+    :param polygon_wkt: WKT string of the polygon
+    :param soil_data: List of dictionaries, each containing 'muname', 'cokey', 'hzname', and 'geom'
+    """
+    # Create a GeoDataFrame from the polygon
+    polygon = gpd.GeoDataFrame(geometry=[wkt.loads(polygon_wkt)], crs="EPSG:4326")
+
+    # Create a GeoDataFrame from the soil data
+    soil_gdf = gpd.GeoDataFrame(soil_data, geometry=[wkt.loads(soil['geom']) for soil in soil_data], crs="EPSG:4326")
+
+    # Generate a color map for soil types
+    unique_soils = soil_gdf['muname'].unique()
+    color_map = {soil: f'#{random.randint(0, 0xFFFFFF):06x}' for soil in unique_soils}
+
+    # Plot the data
+    fig, ax = plt.subplots(figsize=(12, 8))
+
+    # Plot the polygon boundary
+    polygon.boundary.plot(ax=ax, color='black', linewidth=2)
+
+    # Plot each soil type
+    for soil in unique_soils:
+        soil_subset = soil_gdf[soil_gdf['muname'] == soil]
+        soil_subset.plot(ax=ax, color=color_map[soil], alpha=0.5, label=soil)
+
+    # Customize the plot
+    ax.set_title('Soil Types Within Polygon')
+    ax.legend(title='Soil Types', loc='center left', bbox_to_anchor=(1, 0.5))
+    ax.axis('off')
+
+    plt.tight_layout()
+    plt.show()
+
+
+# Example polygon
+polygon_wkt_cain33 = "POLYGON((-121.8310477 39.0194877, -121.8310596 39.0148568, -121.8262794 39.0194330, -121.8262943 39.0148207, -121.8310477 39.0194877))"
+polygon_wkt_ladd = "POLYGON((-121.6302038 37.9318445, -121.6281822 37.9406082, -121.6253142 37.9405416, -121.6274177 37.9318445))"
+sample_poly = "POLYGON((-121.420000 36.850000, -121.410000 36.850000, -121.410000 36.860000, -121.420000 36.860000, -121.420000 36.850000))"
+
+# Convert the WKT to a shapely geometry
+# polygon_geom = wkt.loads(polygon_wkt_cain33)
+#
+# # Create a GeoDataFrame
+# gdf = gpd.GeoDataFrame([{'geometry': polygon_geom}], crs="EPSG:4326")
+#
+# # Define the output file path
+# output_file_path = r"C:\Users\odolan\Downloads\cain.shp"
+#
+# # Export to shapefile
+# gdf.to_file(output_file_path, driver='ESRI Shapefile')
+
+# Get soil data
+# soil_data = get_soil_texture_from_polygon2(sample_poly)
+# print(soil_data)
+# # Visualize the soil data
+# if isinstance(soil_data, list):
+#     visualize_soil_texture(sample_poly, soil_data)
+# else:
+#     print(soil_data)
+#
+# Example usage:
+# polygon_wkt = "POLYGON((-121.6292038 37.9328445, -121.6291822 37.9396082, -121.6263142 37.9395416, -121.6264177 37.9328445, -121.6292038 37.9328445))"
+# soil_data = get_soil_data(polygon_wkt)
+
+# Now you can use the soil_data with the graph_soil_polygon function
+# graph_soil_polygon(polygon_wkt, soil_data)
+#
+latitude = 39.0613077
+longitude = -122.0138332
+l2 = 39.05923
+
+l3 = -122.01279
+#
+soil_info = get_soil_texture_from_coords(latitude, longitude)
+print(soil_info)
